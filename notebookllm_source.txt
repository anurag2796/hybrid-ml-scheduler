

======= FILE: RELEASE_NOTES_v2.0.0.md =======

# Release Notes v2.0.0 - Major Update

**Date:** November 27, 2025
**Version:** 2.0.0

## ğŸš€ Major Release Highlights

This release marks a significant milestone in the Hybrid ML Scheduler project, transitioning from a prototype to a production-ready system. It introduces a fully refactored backend architecture, enhanced security, persistent storage, and a comprehensive observability suite.

### ğŸ—ï¸ Architectural Overhaul
- **Modular Backend:** Refactored monolithic code into a layered architecture (API, Services, Repositories, Models).
- **Async Database:** Integrated PostgreSQL with `asyncpg` and SQLAlchemy for high-performance, non-blocking data persistence.
- **Redis Caching:** Implemented Redis caching layer for high-frequency data access (training data, scheduler stats).
- **Pydantic Configuration:** Centralized configuration management using Pydantic Settings with environment variable support.

### ğŸ”’ Security & Reliability
- **Rate Limiting:** Added token-bucket rate limiting middleware backed by Redis.
- **Security Headers:** Implemented OWASP-recommended security headers (HSTS, CSP, etc.).
- **Input Validation:** Added comprehensive input validation and sanitization (SQLi, XSS prevention).
- **Robust Error Handling:** Standardized error responses and global exception handling.

### ğŸ“Š Enhanced Observability & Dashboard
- **Structured Logging:** Implemented JSON structured logging with correlation IDs for request tracing.
- **Real-time Metrics:** Exposed Prometheus metrics for monitoring system health and performance.
- **Advanced Visualizations:** Added new "Enhanced Analytics" and "Historical Analytics" views to the dashboard.
- **Interactive Charts:** Included Heatmaps, Win/Loss Matrices, and Correlation Matrices for deep performance analysis.

### ğŸ§  Simulation & ML Improvements
- **Continuous Simulation:** Upgraded simulation engine to run continuously with real-time task generation.
- **Online Retraining:** Implemented automated model retraining using a sliding window of historical data.
- **Data Persistence:** All simulation data and scheduler decisions are now persisted to the database for long-term analysis.

## ğŸ› ï¸ Technical Details

### Dependencies Added
- `fastapi`, `uvicorn[standard]`
- `sqlalchemy[asyncio]`, `asyncpg`
- `redis`
- `pydantic-settings`
- `loguru`
- `prometheus-client`

### Database Schema
- **Tables:** `tasks`, `scheduler_results`, `training_data`, `simulation_state`
- **Indexes:** Optimized indexes for time-series queries and frequent lookups.

## ğŸ“ Upgrade Instructions
1. **Environment Setup:**
   ```bash
   cp .env.example .env
   # Update .env with your database and redis credentials
   ```
2. **Install Dependencies:**
   ```bash
   pip install -r requirements.txt
   ```
3. **Initialize Database:**
   ```bash
   python scripts/init_db.py
   ```
4. **Run Server:**
   ```bash
   python src/dashboard_server_v2.py
   ```

---



======= FILE: presentation_slides.md =======

# Hybrid ML Scheduler: Optimizing Heterogeneous Computing
## A Reinforcement Learning Approach to Task Scheduling

---

# Slide 1: Title Slide

**Title:** Hybrid ML Scheduler
**Subtitle:** Optimizing Heterogeneous GPU/CPU Clusters using Reinforcement Learning
**Presenter:** [Your Name/Team Name]
**Context:** Advanced Parallel Computing Project

**Key Visual:** A split image showing a Brain (AI) connected to a Server Rack.

---

# Slide 2: The Problem - "The Kitchen Metaphor"

**The Challenge:**
Modern computers have different types of processors:
1.  **CPUs (Prep Cooks):** Slow, cheap, good for simple logic.
2.  **GPUs (Master Chefs):** Fast, expensive, power-hungry. (50x faster for math).

**The Dilemma:**
*   Sending a small task to a GPU is wasteful (Transfer time > Execution time).
*   Sending a huge task to a CPU causes bottlenecks.
*   **Goal:** Assign the *right* task to the *right* processor automatically.

---

# Slide 3: The Solution - AI-Driven Scheduling

**Introducing the Hybrid ML Scheduler:**
An intelligent system that learns to manage the cluster without human rules.

**Core Innovation:**
Instead of static rules ("If size > 100MB..."), we use **Reinforcement Learning** (DQN). The scheduler:
1.  **Observes** the task (Size, Complexity).
2.  **Acts** (Assigns to CPU or GPU).
3.  **Learns** from the result (Reward = Speed - Cost).

---

# Slide 4: System Architecture

**The Stack:**
*   **Frontend:** React Dashboard (Real-time visualization).
*   **Backend:** FastAPI (Task generation & Simulation).
*   **Model:** PyTorch (DQN) + Scikit-Learn (Random Forest).

**Data Flow:**
`Workload Gen` -> `Simulator` -> `Scheduler (The Brain)` -> `Execution` -> `Feedback Loop`

*(Visual Idea: Use the Mermaid diagram from the project wiki showing the loop)*

---

# Slide 5: The Contenders (Scheduling Strategies)

We compare 6 strategies in a "Race":

1.  **Round Robin:** Alternates A -> B -> A -> B. (Fair but dumb).
2.  **Random:** Total chaos. (Baseline for "worst case").
3.  **Greedy:** "If it has math, send to GPU." (Fails on small data-heavy tasks).
4.  **Hybrid ML:** Random Forest. Trained on past data.
5.  **RL Agent (Our Hero):** Learns live. Adapts to changes.
6.  **Oracle:** The theoretical limit. Brute-forces every option to find the truth.

---

# Slide 6: Deep Dive - The RL Agent (DQN)

**How it works (Deep Q-Network):**
*   **State:** [Task Size, Compute Intensity, Memory Required]
*   **Action:** Choose [CPU, GPU 0, GPU 1, GPU 2, GPU 3]
*   **Reward Function:** $- (Time + 0.5 \times Energy + 0.5 \times Cost)$

**Key Tech:**
*   **Experience Replay:** Remembers past mistakes to avoid repeating them.
*   **Target Networks:** Stabilizes learning.

---

# Slide 7: The "Oracle" - Knowing the Truth

**What is the Oracle?**
A scheduler that "cheats" by running every task 11 times (0% GPU, 10% GPU ... 100% GPU) and picking the best result.

**Why do we need it?**
*   It gives us a **"Speed of Light"** baseline.
*   If our RL Agent gets within 5% of the Oracle, we have solved the problem.
*   It generates "Labelled Data" for the Random Forest model.

---

# Slide 8: The Dashboard - Real-Time Visibility

**Features:**
*   **Live Race:** Watch schedulers compete in real-time.
*   **Radar Chart:** Compare Cost vs. Time vs. Energy.
*   **Utilization:** See if the cluster is overloaded.

*(Include Screenshot of the Dark Mode Dashboard)*

---

# Slide 9: Results - RL vs The World

**Performance Metrics:**
*   **RL Agent vs Random:** **5x Faster**.
*   **RL Agent vs Hybrid ML:** **20% Faster** (Adapts better to new patterns).
*   **RL Agent vs Oracle:** **~95% Efficiency** (Almost perfect!).

**Key Insight:**
The RL agent learned to **keep small tasks on the CPU**. It realized the "walking time" to the GPU wasn't worth it for small jobs.

---

# Slide 10: Conclusion & Future Work

**Summary:**
We successfully built a self-optimizing scheduler that saves time and money by understanding the physics of the hardware.

**Key Takeaways:**
1.  Heterogeneous scheduling is non-trivial.
2.  RL agents can learn complex physics rules (Amdahl's Law) without being explicitly programmed.
3.  Real-time visualization is crucial for trusting AI decisions.

**Future Work:**
*   Multi-Node Clustering (Kubernetes integration).
*   Transformer-based Agents (Attention mechanisms).

---


======= FILE: pytest.ini =======

[pytest]
pythonpath = .
testpaths = tests
addopts = -v


======= FILE: requirements.txt =======

# Core dependencies
numpy>=1.24.0
pandas>=2.0.0
matplotlib>=3.7.0
scikit-learn>=1.3.0
xgboost>=2.0.0
pyyaml>=6.0

# PyTorch with MPS support (for Mac)
torch>=2.0.0

# Utilities
tqdm>=4.65.0
pydantic>=2.0.0
pydantic-settings>=2.0.0
loguru>=0.7.0
joblib>=1.3.0
scipy>=1.11.0
seaborn>=0.12.0

# Web Framework
fastapi>=0.100.0
uvicorn[standard]>=0.20.0
websockets>=11.0.0
python-multipart>=0.0.6

# Database
sqlalchemy[asyncio]>=2.0.0
asyncpg>=0.29.0
psycopg2-binary>=2.9.9
alembic>=1.12.0

# Caching
redis>=5.0.0
hiredis>=2.2.3

# Messaging
kafka-python>=2.0.2

# Security & Auth
python-jose[cryptography]>=3.3.0
passlib[bcrypt]>=1.7.4
python-dotenv>=1.0.0

# Monitoring
prometheus-client>=0.19.0
opentelemetry-api>=1.20.0
opentelemetry-sdk>=1.20.0
opentelemetry-instrumentation-fastapi>=0.41b0

# Testing
pytest>=7.4.0
pytest-asyncio>=0.21.0
httpx>=0.25.0
locust>=2.17.0

# Development
black>=23.0.0
ruff>=0.1.0


======= FILE: config_heavy.yaml =======

# Configuration for Heavy Workload Simulation

hardware:
  device: "mps"  # Metal Performance Shaders for M4 Max
  use_cpu_fallback: true
  num_virtual_gpus: 4
  
profiling:
  matrix_sizes: [256, 512, 1024, 2048]
  iterations: 5
  save_profiles: true
  profile_output: "data/profiles/hardware_profile.json"

workload_generation:
  num_tasks: 1000
  simulation_tasks: 100
  rl_training_tasks: 1000 # Increased for better learning
  evaluation_tasks: 20000 # 20k tasks for Very Heavy workload
  task_size_range: [100, 5000] # Slightly smaller max size to keep runtime reasonable but still heavy
  compute_intensity_range: [0.6, 1.0] # Biased towards heavy compute
  memory_range: [100, 10000] # Wide memory range
  arrival_rate: 200  # Increased arrival rate for pressure
  seed: 42

ml_models:
  model_type: "random_forest"
  random_forest:
    n_estimators: 100
    max_depth: 15
    min_samples_split: 5
    n_jobs: -1
  
  train_test_split: 0.8
  validation_split: 0.1
  
scheduling:
  batch_size: 100 # Larger batches
  prediction_threshold: 0.8
  load_balance_weight: 0.3
  
evaluation:
  baseline_strategies: ["round_robin", "random", "greedy"]
  metrics: ["makespan", "throughput", "load_balance", "utilization"]
  
output:
  results_dir: "data/results/"
  plots_dir: "data/results/plots/"
  model_dir: "models/"


======= FILE: debug_ws.py =======

import asyncio
import websockets
import json

async def listen():
    uri = "ws://localhost:8000/ws"
    async with websockets.connect(uri) as websocket:
        print(f"Connected to {uri}")
        while True:
            msg = await websocket.recv()
            data = json.loads(msg)
            if data['type'] == 'simulation_update':
                print("\n--- Simulation Update ---")
                print(f"Keys in latest_results: {list(data['latest_results'].keys())}")
                print(f"Hybrid ML Result: {data['latest_results'].get('hybrid_ml')}")
                print(f"Utilization: {data['utilization']}")
                break

asyncio.run(listen())


======= FILE: project_guide.md =======

# Hybrid Offline-Online ML Scheduler for Parallel Computing on M4 Max

## Complete Project Setup & Execution Guide

### Project Overview

This project implements a **Hybrid Offline-Online ML Scheduler** for dynamic resource allocation in parallel computing systems. It's specifically optimized for MacBook M4 Max with 36GB RAM.

**Key Components:**
- Phase 1: Hardware Profiling (benchmark CPU/GPU performance)
- Phase 2: Workload Data Generation (create synthetic training data)
- Phase 3: Offline ML Training (train performance prediction models)
- Phase 4: Online Scheduling (real-time task scheduling)
- Phase 5: Evaluation & Analysis (compare against baselines)

---

## Installation & Setup (15 minutes)

### Step 1: Create Project Structure

```bash
# Create project directory
mkdir scheduler-project
cd scheduler-project

# Create subdirectories
mkdir -p src notebooks data/{workload_traces,profiles,results} models logs
```

### Step 2: Create Python Virtual Environment

```bash
# Using pyenv (you have Python 3.12.7)
pyenv local 3.12.7

# Create virtual environment
python -m venv venv

# Activate virtual environment
source venv/bin/activate

# Upgrade pip
pip install --upgrade pip setuptools wheel
```

### Step 3: Install Dependencies

```bash
# Copy the requirements.txt content and install
pip install -r requirements.txt
```

**Note for M4 Max:** PyTorch will automatically use MPS (Metal Performance Shaders) backend. Verify with:

```python
import torch
print(f"MPS available: {torch.backends.mps.is_available()}")
print(f"MPS built: {torch.backends.mps.is_built()}")
```

### Step 4: Copy Configuration Files

Create `config.yaml` in project root with the provided configuration.

---

## Week-by-Week Implementation

### WEEK 1: Profiling & Data Generation

#### Day 1-2: Hardware Profiling

**File: `src/profiler.py`** (100+ lines)

```python
from src.profiler import HardwareProfiler

# Initialize profiler
profiler = HardwareProfiler(device_type="mps")

# Run benchmarks on different matrix sizes
profile = profiler.profile_range(sizes=[256, 512, 1024, 2048])

# Save profile for later use
profiler.save_profile("data/profiles/hardware_profile.json")

print(f"CPU/GPU Ratio: {profile['cpu_gpu_ratio']:.2f}")
print(f"GPU should handle: {profile['gpu_fraction']:.1%} of work")
```

**Expected Output:**
```
Hardware Profiler initialized on device: mps
Starting hardware profiling with sizes: [256, 512, 1024, 2048]
Profiling size 256x256...
CPU benchmark 256x256: 0.0023s
GPU benchmark 256x256: 0.0051s
...
Performance Model: CPU/GPU Ratio = 0.45
GPU should receive 31.0% of work
```

**Key Metrics:**
- CPU vs GPU execution time for different problem sizes
- CPU/GPU performance ratio (determines optimal work splitting)
- Hardware profile saved for reproducibility

---

#### Day 3-5: Workload Generation

**File: `src/workload_generator.py`** (150+ lines)

```python
from src.workload_generator import WorkloadGenerator

# Generate synthetic workload
wg = WorkloadGenerator(seed=42)

tasks = wg.generate_workload(
    num_tasks=10000,
    task_size_range=(100, 5000),
    compute_intensity_range=(0.1, 1.0),
    memory_range=(10, 500),
    arrival_rate=100.0
)

# Save for training
wg.save_workload("data/workload_traces/training_workload.csv")

# View statistics
stats = wg.get_statistics()
print(f"Generated {stats['num_tasks']} tasks")
print(f"Avg task size: {stats['avg_size']:.0f}")
print(f"Avg compute intensity: {stats['avg_intensity']:.2f}")
```

**Generated Features:**
- `task_id`: Unique identifier
- `size`: Problem size (compute amount)
- `compute_intensity`: 0-1, higher = GPU-friendly
- `memory_required`: MB needed
- `arrival_time`: When task arrives
- `duration_estimate`: Expected execution time

---

### WEEK 2: Offline ML Training

#### Day 6-8: ML Model Training

**File: `src/ml_models.py`** (220+ lines)

```python
from src.ml_models import RandomForestPredictor, XGBoostPredictor
from src.workload_generator import WorkloadGenerator

# Load training workload
wg = WorkloadGenerator.load("data/workload_traces/training_workload.csv")

# Option 1: Use Random Forest (recommended for M4 Max)
model = RandomForestPredictor(n_estimators=100, max_depth=15)

# Option 2: Use XGBoost (more accurate but slower)
# model = XGBoostPredictor(n_estimators=100, max_depth=7)

# Prepare features
df = wg.to_dataframe()
features = ['task_size', 'compute_intensity', 'memory_required']
X = df[features]
y = df['duration_estimate']

# Train model
results = model.fit(X, y, test_size=0.2)
print(f"Train RÂ²: {results['train_r2']:.4f}")
print(f"Test RÂ²: {results['test_r2']:.4f}")

# Get feature importances
importances = model.feature_importance()
print(f"Feature importances: {importances}")

# Save model for online use
model.save("models/scheduler_model.pkl")
```

**Expected Output:**
```
Training Random Forest model...
Train R2: 0.8234, Test R2: 0.8120
CV R2: 0.8100 (+/- 0.0150)
Feature importances:
  - task_size: 0.4521
  - compute_intensity: 0.3842
  - memory_required: 0.1637
Model saved to models/scheduler_model.pkl
```

#### Day 9-10: Offline Trainer Pipeline

**File: `src/offline_trainer.py`** (180+ lines)

```python
from src.offline_trainer import OfflineTrainer
from src.workload_generator import WorkloadGenerator

# Initialize trainer
trainer = OfflineTrainer(
    model_type="random_forest",
    n_estimators=100,
    max_depth=15
)

# Load workload
wg = WorkloadGenerator.load("data/workload_traces/training_workload.csv")

# Run complete pipeline
results = trainer.run_full_pipeline(
    wg,
    model_output_path="models/scheduler_model.pkl"
)

print(f"Training complete!")
print(f"Feature importances: {results['feature_importances']}")
```

---

### WEEK 3: Online Scheduling & Simulation

#### Day 11-13: Online Scheduler Implementation

**File: `src/online_scheduler.py`** (200+ lines)

```python
from src.online_scheduler import OnlineScheduler
from src.ml_models import RandomForestPredictor

# Load trained model
model = RandomForestPredictor.load("models/scheduler_model.pkl")

# Create online scheduler with 4 virtual GPUs
scheduler = OnlineScheduler(model=model, num_gpus=4)

# Generate test tasks
from src.workload_generator import WorkloadGenerator
wg = WorkloadGenerator(seed=99)
test_tasks = wg.generate_workload(num_tasks=1000)

# Submit tasks
for task in test_tasks:
    scheduler.submit_task(task)

# Schedule all tasks
decisions = scheduler.process_queue()

print(f"Scheduled {len(decisions)} tasks")

# Check utilization
util = scheduler.get_utilization()
print(f"Average GPU utilization: {util['average_utilization']:.1%}")

for gpu_util in util.items():
    if gpu_util[0] != 'average_utilization':
        print(f"  {gpu_util[0]}: {gpu_util[1]['utilization']:.1%}")
```

#### Day 14: Multi-GPU Simulation

**File: `src/simulator.py`** (120+ lines)

```python
from src.simulator import VirtualMultiGPU
from src.workload_generator import WorkloadGenerator

# Create simulator
simulator = VirtualMultiGPU(num_gpus=4, memory_per_gpu=8000)

# Generate test workload
wg = WorkloadGenerator(seed=123)
test_tasks = wg.generate_workload(num_tasks=500)

# Evaluate baseline schedulers
baselines = simulator.evaluate_baseline_schedulers(test_tasks)

print("Baseline Performance Comparison:")
for strategy, metrics in baselines.items():
    print(f"\n{strategy}:")
    print(f"  Makespan: {metrics['makespan']:.2f}s")
    print(f"  Avg time: {metrics['avg_time']:.4f}s")
    print(f"  Max time: {metrics['max_time']:.4f}s")
```

**Expected Baselines:**
- `round_robin`: Fixed 50-50 split
- `random`: Random allocation
- `greedy`: Based on compute intensity

---

### WEEK 4: Evaluation & Analysis

#### Day 15-18: Complete Pipeline Execution

**File: `main.py`** (200+ lines)

```bash
# Run complete project
python main.py
```

This executes all 5 phases:

1. **Phase 1**: Hardware profiling â†’ `data/profiles/hardware_profile.json`
2. **Phase 2**: Workload generation â†’ `data/workload_traces/workload_train.csv`
3. **Phase 3**: ML training â†’ `models/scheduler_model.pkl` + feature importances
4. **Phase 4**: Online scheduling â†’ 1000+ task schedules
5. **Phase 5**: Evaluation â†’ comparison with baselines

#### Day 19: Performance Analysis & Visualization

```python
import pandas as pd
import matplotlib.pyplot as plt

# Load scheduling results
results = pd.read_csv("data/results/scheduling_results.csv")

# Analyze predictions
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# GPU fraction distribution
axes[0, 0].hist(results['gpu_fraction'], bins=50)
axes[0, 0].set_title('GPU Fraction Distribution')
axes[0, 0].set_xlabel('GPU Fraction')

# Task size vs GPU allocation
axes[0, 1].scatter(results['task_id'], results['gpu_fraction'], alpha=0.5)
axes[0, 1].set_title('GPU Allocation Over Time')
axes[0, 1].set_xlabel('Task ID')

# Comparison metrics
strategies = ['round_robin', 'random', 'greedy', 'our_scheduler']
makespans = [5.2, 4.8, 4.5, 4.1]  # Example values
axes[1, 0].bar(strategies, makespans)
axes[1, 0].set_title('Makespan Comparison')
axes[1, 0].set_ylabel('Total Time (s)')

# Speedup analysis
speedups = [1.0, 1.08, 1.15, 1.27]  # vs CPU baseline
axes[1, 1].bar(strategies, speedups)
axes[1, 1].set_title('Speedup vs CPU Baseline')
axes[1, 1].set_ylabel('Speedup Factor')

plt.tight_layout()
plt.savefig('data/results/performance_analysis.png', dpi=300)
plt.show()
```

---

## Expected Performance Metrics

### Phase 1: Hardware Profiling
```
CPU/GPU Ratio: 0.4-0.6 (GPU is 1.7-2.5x faster)
GPU work fraction: 30-45% (CPU-bound overall on M4)
Memory bandwidth: 400+ GB/s
```

### Phase 3: ML Model Accuracy
```
Random Forest:
  - Train RÂ²: 0.80-0.85
  - Test RÂ²: 0.78-0.82
  - MAE: 0.05-0.15 (for 0-1 GPU fraction predictions)
```

### Phase 5: Scheduling Performance
```
Speedup vs Round-Robin: 1.2-1.4x
Speedup vs Greedy: 1.05-1.15x
Load Balance: 0.90-0.95
```

---

## Troubleshooting on M4 Max

### Issue: MPS Device Not Available
```python
# Check MPS support
import torch
print(torch.backends.mps.is_available())  # Should be True
print(torch.backends.mps.is_built())      # Should be True

# If False, reinstall PyTorch:
pip install --upgrade torch torchvision torchaudio
```

### Issue: Slow GPU Performance
- Expected on M4 Max: GPU is integrated, not discrete
- MPS has ~60% speedup vs CPU on matrix operations
- This is sufficient for the project!

### Issue: Memory Issues
- Your 36GB RAM is more than enough
- If OOM: reduce `num_tasks` in config or batch size

---

## Project Deliverables

### Code Files
- `src/profiler.py`: Hardware profiling
- `src/workload_generator.py`: Synthetic workload generation
- `src/ml_models.py`: ML prediction models
- `src/offline_trainer.py`: Training pipeline
- `src/online_scheduler.py`: Online scheduling
- `src/simulator.py`: Baseline simulation
- `main.py`: Complete pipeline
- `config.yaml`: Configuration

### Data Files
- `data/profiles/hardware_profile.json`: Performance characteristics
- `data/workload_traces/training_workload.csv`: Training data
- `data/results/scheduling_results.csv`: Scheduling decisions
- `models/scheduler_model.pkl`: Trained model

### Results & Analysis
- Performance comparison charts
- Model accuracy plots
- Speedup analysis
- Load balance metrics
- Feature importance ranking

### Documentation
- `README.md`: Project overview
- `METHODOLOGY.md`: Technical approach
- `RESULTS.md`: Experimental findings
- Jupyter notebooks with analysis

---

## Next Steps & Extensions

### Easy Extensions (1-2 weeks)
1. Add energy consumption optimization
2. Test with different ML models (SVM, Neural Networks)
3. Add multiple workload types (convolution, sorting, sparse operations)
4. Implement online model updating

### Medium Extensions (2-4 weeks)
1. Add real GPU profiling (if you get GPU access)
2. Implement hierarchical scheduling
3. Add task dependencies and DAG scheduling
4. Distributed scheduler across machines

### Advanced Extensions (4-8 weeks)
1. Implement reinforcement learning scheduler (Q-learning)
2. Add dynamic feature extraction
3. Multi-objective optimization (time + energy)
4. Publish paper on approach!

---

## Estimated Timeline

| Phase | Difficulty | Time | Status |
|-------|-----------|------|--------|
| Setup | Easy | 1 day | âœ“ |
| Profiling | Easy | 2 days | |
| Workload Gen | Easy | 2 days | |
| ML Training | Medium | 3 days | |
| Online Scheduler | Medium | 3 days | |
| Evaluation | Medium | 3 days | |
| Analysis & Report | Medium | 3 days | |
| **Total** | **Medium** | **~4 weeks** | |

---

## Questions & Support

Refer to:
- Comments in code for detailed explanations
- Config file for tuning parameters
- Research papers in `docs/` folder
- GitHub issues for similar projects

Good luck with your project!


======= FILE: config.yaml =======

# Configuration for Hybrid Offline-Online Scheduler

hardware:
  device: "mps"  # Metal Performance Shaders for M4 Max
  use_cpu_fallback: true
  num_virtual_gpus: 4
  
profiling:
  matrix_sizes: [256, 512, 1024, 2048]
  iterations: 5
  save_profiles: true
  profile_output: "data/profiles/hardware_profile.json"

workload_generation:
  num_tasks: 1000
  simulation_tasks: 100
  rl_training_tasks: 500
  evaluation_tasks: 500
  task_size_range: [100, 10000]
  compute_intensity_range: [0.0, 1.0]
  memory_range: [10, 5000]
  arrival_rate: 100  # tasks per second
  seed: 42

ml_models:
  model_type: "random_forest"  # or "xgboost"
  random_forest:
    n_estimators: 100
    max_depth: 15
    min_samples_split: 5
    n_jobs: -1
  xgboost:
    n_estimators: 100
    max_depth: 7
    learning_rate: 0.1
    subsample: 0.8
  
  train_test_split: 0.8
  validation_split: 0.1
  
scheduling:
  batch_size: 50
  prediction_threshold: 0.8
  load_balance_weight: 0.3
  
evaluation:
  baseline_strategies: ["round_robin", "random", "greedy"]
  metrics: ["makespan", "throughput", "load_balance", "utilization"]
  
output:
  results_dir: "data/results/"
  plots_dir: "data/results/plots/"
  model_dir: "models/"


======= FILE: run_live_dashboard.sh =======

#!/bin/bash

# Kill any existing processes
lsof -ti:8000 | xargs kill -9 2>/dev/null
lsof -ti:5173 | xargs kill -9 2>/dev/null

# Start Backend (Python FastAPI)
echo "Starting Python Backend (Mock Mode if Kafka missing)..."
python3 -m uvicorn src.dashboard_server:app --host 0.0.0.0 --port 8000 &
BACKEND_PID=$!

# Wait for backend to start
sleep 2

# Start Frontend
echo "Starting Frontend Dashboard..."
cd dashboard
npm run dev &
FRONTEND_PID=$!

# Trap Ctrl+C to kill all
trap "kill $BACKEND_PID $FRONTEND_PID; exit" SIGINT

echo "Dashboard is live at http://localhost:5173"
echo "Press Ctrl+C to stop."

wait


======= FILE: README.md =======

# Hybrid ML Scheduler

> A real-time scheduler comparison system that optimizes heterogeneous GPU/CPU task scheduling using machine learning and reinforcement learning techniques.

---

### ğŸ“š **Documentation Center**
**For deep technical details, theory, and the "Textbook" guide:**
*   [**ğŸ“– Project Wiki (v10.0 Ultimate Edition)**](PROJECT_WIKI_DOCUMENTATION.md) - *1000+ lines covering Theory, Code, and FAQs.*
*   [**ğŸ“„ PDF Manual**](PROJECT_WIKI_DOCUMENTATION.pdf) - *The printable version of the wiki.*

---

## ğŸš€ Overview

The Hybrid ML Scheduler is a simulation framework for comparing different task scheduling strategies in heterogeneous computing environments. It features:

- **6 Scheduling Strategies**: Round Robin, Random, Greedy, Hybrid ML, RL Agent, and Oracle (optimal baseline)
- **Live Dashboard**: Real-time WebSocket-based visualization of scheduler performance
- **Online Learning**: Continuous model retraining based on accumulated execution data
- **Comprehensive Metrics**: Time, energy consumption, and cost tracking for each scheduler

### ğŸ³ The Concept: A "Kitchen" Metaphor
To understand the complexity, imagine a restaurant kitchen:
*   **4 Master Chefs (GPUs):** Fast but expensive ($$$).
*   **1 Prep Cook (CPU):** Slow but cheap ($).
*   **The Challenge:** Moving ingredients to a Chef takes time. Simple tasks (chopping an onion) are faster with the Prep Cook because you save the walk. Complex tasks (SoufflÃ©) need the Chef.
*   **Our Solution:** An **RL Agent** that learns to be the perfect Kitchen Manager, assigning tasks based on "Cooking Time" vs "Walking Time".

## ğŸ“Š Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Web Dashboard (React)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Global View â”‚  â”‚ Scheduler    â”‚  â”‚  Historical  â”‚      â”‚
â”‚  â”‚  Comparison  â”‚  â”‚ Details      â”‚  â”‚  Analysis    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ WebSocket (ws://localhost:8000/ws)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             FastAPI Backend (dashboard_server.py)            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚        Continuous Simulation Engine                   â”‚   â”‚
â”‚  â”‚  â€¢ Task Generation (WorkloadGenerator)                â”‚   â”‚
â”‚  â”‚  â€¢ Parallel Scheduler Execution                       â”‚   â”‚
â”‚  â”‚  â€¢ Metrics Collection & Broadcasting                  â”‚   â”‚
â”‚  â”‚  â€¢ Model Retraining (every 50 tasks)                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
34 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                    â”‚                    â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Hybrid ML  â”‚     â”‚  RL Agent   â”‚    â”‚  Simple Rules  â”‚
    â”‚  (RF Model)  â”‚     â”‚  (DQN)      â”‚    â”‚  (RR/Random)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **Task Generation**: `WorkloadGenerator` creates tasks with random size, compute intensity, and memory requirements
2. **Parallel Execution**: Each task runs through all 6 schedulers simultaneously
3. **Oracle Computation**: Brute-force grid search finds optimal GPU fraction (ground truth)
4. **Metrics Calculation**: Energy (50W GPU, 30W CPU) and cost ($0.15/kWh) computed
5. **Persistence**: Oracle results saved to `data/long_term_history.csv`
6. **Retraining**: Every 50 tasks, the Hybrid ML model retrains on accumulated data
7. **Broadcasting**: Results sent to all WebSocket clients in real-time

## ğŸ› ï¸ Installation

### Prerequisites

- **Python 3.10+**
- **Node.js 16+** (for dashboard)
- **pip** and **npm**

### Setup

1. **Clone the repository**
```bash
git clone <repository-url>
cd hybrid_ml_scheduler
```

2. **Install Python dependencies**
```bash
pip install -r requirements.txt
```

3. **Install frontend dependencies**
```bash
cd dashboard
npm install
cd ..
```

## ğŸƒ Usage

### Quick Start: Live Dashboard

The easiest way to see the system in action is to launch the live dashboard:

```bash
./run_live_dashboard.sh
```

This script:
1. Starts the FastAPI backend on `http://localhost:8000`
2. Launches the React frontend on `http://localhost:5173`
3. Begins the continuous simulation

Open **http://localhost:5173** in your browser to view the dashboard.

### Dashboard Views

#### 1. **Global Comparison**
- Performance race bar chart comparing all schedulers
- Workload distribution scatter plot
- Average metrics summary

#### 2. **Scheduler Details** (click any scheduler button)
- Virtual cluster load over time
- Radar chart comparing against Oracle baseline
- GPU/CPU resource split

#### 3. **Historical Analysis**
- Long-term performance trends
- Model retraining history
- Data collection statistics

### Running Experiments Manually

For offline experiments without the dashboard (Scientific Mode):

```bash
python scripts/run_heavy_simulation.py
```

This runs a batch simulation of 10,000 tasks and outputs:
- **CDF Plots:** `data/results/plots/latency_cdf.png`
- **Cost Analysis:** `data/results/plots/cost_comparison.png`
- **Detailed Logs:** `data/results/heavy_simulation_report.csv`

## ğŸ“ API Documentation

### WebSocket Protocol

**Endpoint**: `ws://localhost:8000/ws`

**Message Format**:
```json
{
  "type": "simulation_update",
  "task": {
    "id": 123,
    "size": 960,
    "intensity": 0.75,
    "memory": 81
  },
  "latest_results": {
    "hybrid_ml": {"time": 0.45, "energy": 22.5, "cost": 0.0009},
    "oracle": {"time": 0.42, "energy": 21.0, "cost": 0.0008},
    ...
  },
  "comparison": [
    {"name": "hybrid_ml", "avg_time": 0.48},
    ...
  ],
  "utilization": {
    "average_utilization": 0.65,
    "gpu_0": {"utilization": 0.72},
    ...
  }
}
```

### REST Endpoints

#### `GET /`
Health check endpoint
```json
{"status": "online", "message": "Hybrid ML Scheduler Dashboard API is running"}
```

#### `GET /api/full_history`
Retrieve complete historical training data
```json
[
  {"task_id": 0, "size": 960, "compute_intensity": 0.26, ...},
  ...
]
```

#### `DELETE /api/history`
Clear all historical data

#### `POST /api/pause`
Pause the simulation

#### `POST /api/resume`
Resume a paused simulation

## ğŸ§ª Testing

Run the test suite:

```bash
# Backend tests
pytest tests/
```

## ğŸ“‚ Project Structure

```
hybrid_ml_scheduler/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ simulation_engine.py    # Core simulation loop
â”‚   â”œâ”€â”€ dashboard_server.py      # FastAPI WebSocket server
â”‚   â”œâ”€â”€ workload_generator.py    # Task generation
â”‚   â”œâ”€â”€ simulator.py             # GPU/CPU execution model
â”‚   â”œâ”€â”€ offline_trainer.py       # ML model training
â”‚   â”œâ”€â”€ online_scheduler.py      # Hybrid ML scheduler
â”‚   â”œâ”€â”€ rl_scheduler.py          # RL agent
â”‚   â”œâ”€â”€ dqn_scheduler.py         # Deep Q-Network implementation
â”‚   â”œâ”€â”€ ml_models.py             # RandomForest predictor
â”‚   â””â”€â”€ profiler.py              # Performance profiling
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ App.jsx              # Main React component
â”‚       â””â”€â”€ index.css            # Cyberpunk theme styles
â”œâ”€â”€ data/
â”‚   â””â”€â”€ long_term_history.csv    # Training data persistence
â”œâ”€â”€ tests/                       # Unit tests
â”œâ”€â”€ main.py                      # Entry point for offline experiments
â”œâ”€â”€ run_live_dashboard.sh        # Dashboard launcher
â”œâ”€â”€ config.yaml                  # System configuration
â””â”€â”€ requirements.txt             # Python dependencies
```

## ğŸ¯ Schedulers Explained

| Scheduler | Strategy | GPU Fraction Selection |
|-----------|----------|------------------------|
| **Round Robin** | Alternating | 0.5 if task_id % 2 == 0, else 0.0 |
| **Random** | Stochastic | Uniform random [0, 1] |
| **Greedy** | Heuristic | Uses task.compute_intensity |
| **Hybrid ML** | ML-Based (RandomForest) | Predicts speedup using Size + Intensity |
| **RL Agent** | Reinforcement Learning (DQN) | Learns Q-Values for discrete GPU allocations |
| **Oracle** | Optimal | Grid search over 11 fractions [0, 0.1, ..., 1.0] |

## ğŸ”§ Configuration

Edit `config.yaml` to customize:

```yaml
simulation:
  num_gpus: 4
  retrain_interval: 50
  
workload:
  size_range: [500, 1500]
  intensity_range: [0.1, 0.9]
  memory_range: [50, 150]
```

## ğŸ“ˆ Performance Metrics

The system tracks three key metrics:

1. **Execution Time** (seconds): Wall-clock time to complete the task
2. **Energy** (Joules): Power model: GPU=50W, CPU=30W
3. **Cost** (dollars): Based on $0.15 per kWh electricity rate

## ğŸš§ Development

### Adding a New Scheduler

1. Create a new scheduler class in `src/`
2. Implement the scheduling logic
3. Add to `simulation_engine.py`:
```python
self.simulators['my_scheduler'] = VirtualMultiGPU(self.num_gpus)
```
4. Update the `_run_all_schedulers` method
5. Update frontend `SCHEDULERS` constant in `App.jsx`

### Modifying the Power Model

Edit `_calculate_metrics` in `simulation_engine.py`:
```python
# Power Model: GPU=50W, CPU=30W
power = (gpu_frac * GPU_POWER) + ((1.0 - gpu_frac) * CPU_POWER)
```

## ğŸ“œ License

MIT License

---
**See `PROJECT_WIKI_DOCUMENTATION.md` for the full technical encyclopedia.**


======= FILE: check_metrics.py =======


import asyncio
from sqlalchemy import create_engine, text
from backend.core.config import settings

def check_metrics():
    # Use sync driver for this script
    db_url = settings.database_url.replace("+asyncpg", "") # ensure sync if needed, but config.database_url is sync
    try:
        engine = create_engine(db_url)
        with engine.connect() as conn:

            schedulers = ['hybrid_ml', 'rl_agent', 'greedy', 'round_robin', 'random']
            
            print(f"{'Scheduler':<15} | {'Avg Time':<10} | {'Avg Energy':<10} | {'Avg Cost':<10} | {'Count':<10}")
            print("-" * 65)
            
            for scheduler in schedulers:
                query = text(f"""
                SELECT AVG(actual_time), AVG(energy_consumption), AVG(execution_cost), COUNT(*)
                FROM (
                    SELECT actual_time, energy_consumption, execution_cost
                    FROM scheduler_results
                    WHERE scheduler_name = '{scheduler}'
                    ORDER BY id DESC
                    LIMIT 20000
                ) as recent
                """)
                result = conn.execute(query).fetchone()
                print(f"{scheduler:<15} | {result[0]:<10.4f} | {result[1]:<10.4f} | {result[2]:<10.4f} | {result[3]:<10}")
    except Exception as e:
        print(f"Error: {e}")

if __name__ == "__main__":
    check_metrics()


======= FILE: test_pg_connection.py =======

"""Simple test to check PostgreSQL connection."""
import psycopg2

try:
    conn = psycopg2.connect(
        host="localhost",
        port=5432,
        user="postgres",
        password="Coloreal@1",
        database="postgres"
    )
    print("âœ… Successfully connected to PostgreSQL!")
    cursor = conn.cursor()
    cursor.execute("SELECT version();")
    version = cursor.fetchone()
    print(f"PostgreSQL version: {version[0]}")
    cursor.close()
    conn.close()
except Exception as e:
    print(f"âŒ Failed to connect: {e}")
    print("\nPlease check:")
    print("1. PostgreSQL is running")
    print("2. Credentials are correct (user: postgres, password: Coloreal@1)")
    print("3. PostgreSQL is listening on localhost:5432")


======= FILE: generate_report.py =======


import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import matplotlib.image as mpimg
from pathlib import Path
import datetime

def create_text_page(pdf, title, content, fontsize=10):
    fig = plt.figure(figsize=(8.5, 11))
    plt.axis('off')
    
    # Title
    plt.text(0.5, 0.95, title, ha='center', va='top', fontsize=16, weight='bold')
    
    # Content
    y_pos = 0.85
    for line in content.split('\n'):
        # Simple word wrap
        words = line.split()
        current_line = ""
        for word in words:
            if len(current_line + " " + word) > 80:
                plt.text(0.1, y_pos, current_line, ha='left', va='top', fontsize=fontsize, fontfamily='monospace')
                y_pos -= 0.02
                current_line = word
            else:
                current_line += " " + word if current_line else word
        
        if current_line:
            plt.text(0.1, y_pos, current_line, ha='left', va='top', fontsize=fontsize, fontfamily='monospace')
            y_pos -= 0.02
        
        y_pos -= 0.01 # Paragraph spacing
        
        if y_pos < 0.05:
            pdf.savefig(fig)
            plt.close()
            fig = plt.figure(figsize=(8.5, 11))
            plt.axis('off')
            y_pos = 0.95

    pdf.savefig(fig)
    plt.close()

def add_image_page(pdf, image_path, title):
    if not Path(image_path).exists():
        print(f"Warning: {image_path} not found")
        return

    fig = plt.figure(figsize=(8.5, 11))
    plt.axis('off')
    plt.text(0.5, 0.95, title, ha='center', va='top', fontsize=14, weight='bold')
    
    img = mpimg.imread(image_path)
    # Maintain aspect ratio, fit to page
    plt.imshow(img)
    
    # Adjust layout to fit image
    plt.tight_layout(rect=[0.05, 0.05, 0.95, 0.90])
    
    pdf.savefig(fig)
    plt.close()

def main():
    output_path = "data/results/Project_Report.pdf"
    plots_dir = Path("data/results/plots")
    
    with PdfPages(output_path) as pdf:
        # Page 1: Title & Summary
        title = "Hybrid ML Scheduler Project Report"
        summary = """
Date: {}

1. EXECUTIVE SUMMARY
--------------------
This project implements a Hybrid Offline-Online Scheduler for parallel computing tasks.
It compares a static Offline Trainer (Random Forest) against an adaptive Reinforcement Learning (DQN) agent.

Experiment: "Distribution Shift & Realistic Physics"
- We introduced realistic data transfer costs and intensity-based speedups.
- We trained models on GPU-favorable tasks (High Intensity).
- We evaluated on CPU-favorable tasks (Low Intensity, High Transfer Cost).

Results:
- Offline Trainer: FAILED (Makespan: 122.92s). It failed to adapt and blindly used GPU.
- RL Agent: SUCCESS (Makespan: 21.12s). It adapted online and switched to CPU.
- Improvement: RL Agent was ~6x faster than the Offline Trainer.

2. CODE FLOW
------------
The execution pipeline follows these steps (in main.py):

1. Initialization:
   - Load configuration from config.yaml.
   - Setup logging.

2. Phase 1: Hardware Profiling (src/profiler.py)
   - Benchmarks CPU vs GPU matrix multiplication.
   - Establishes baseline performance ratios.

3. Phase 2: Data Generation (src/workload_generator.py)
   - Generates synthetic training data.
   - In this experiment: Biased towards High Compute Intensity (GPU optimal).

4. Phase 3: Offline Training (src/offline_trainer.py)
   - Trains a Random Forest model on the generated data.
   - Learns to predict "GPU" for everything (due to bias).

5. Phase 4.5: RL Training (src/dqn_scheduler.py)
   - Pre-trains DQN agent on the same biased data.
   - Agent starts with "Always GPU" policy.

6. Phase 5: Evaluation (src/simulator.py)
   - Generates TEST data: Biased towards Low Intensity + High Memory (CPU optimal).
   - Runs the Virtual Simulator with Transfer Costs.
   - Compares strategies.
   - RL Agent enables online learning (epsilon=0.1) to adapt during this phase.

""".format(datetime.datetime.now().strftime("%Y-%m-%d %H:%M"))
        
        create_text_page(pdf, title, summary)
        
        # Page 2: Results Table
        results = """
3. EXPERIMENTAL RESULTS
-----------------------
Workload: 500 Tasks (Low Intensity, High Memory)

Strategy          | Makespan (s) | Avg Time (s) | Status
------------------|--------------|--------------|-------
Offline Optimal   | 12.97        | 0.03         | Benchmark
Round Robin       | 56.87        | 0.11         | Baseline
Random            | 67.14        | 0.13         | Baseline
Greedy            | 18.30        | 0.04         | Baseline
Hybrid ML (RF)    | 122.92       | 0.25         | FAILED
RL Agent (DQN)    | 21.12        | 0.04         | SUCCESS

Key Insight:
The RL Agent successfully unlearned its training bias and adapted to the new environment, whereas the Offline Trainer remained static and performed poorly.
"""
        create_text_page(pdf, "Detailed Results", results)
        
        # Page 3+: Plots
        add_image_page(pdf, plots_dir / "makespan_comparison.png", "Makespan Comparison")
        add_image_page(pdf, plots_dir / "avg_duration_comparison.png", "Average Task Duration")
        add_image_page(pdf, plots_dir / "speedup_comparison.png", "Speedup vs Baseline")
        add_image_page(pdf, plots_dir / "compute_vs_memory.png", "Workload Characteristics")
        
    print(f"Report generated at {output_path}")

if __name__ == "__main__":
    main()


======= FILE: speaker_notes.md =======


# ğŸ¤ Project Presentation: Hybrid ML Scheduler
**Speaker Notes & Walkthrough Guide**

---

## ğŸ•’ Time Estimate: ~10 Minutes
**Goal:** Explain how we optimize task scheduling using AI, in simple English.

---

## 1. Introduction (1 Minute)
**Objective:** Hook the audience.

*   "Hello everyone. Today I am presenting my project: **Hybrid ML Scheduler**."
*   "In modern computing, we have many tasks (like math calculations) and many resources (CPUs and GPUs). The big problem is: **Where should we put each task?**"
*   "If we put a small task on a big GPU, we waste energy. If we put a big task on a slow CPU, it takes forever. We need a smart way to decide."
*   "My solution uses two types of AI:
    1.  **Hybrid ML (Random Forest):** A teacher that learns from perfect examples.
    2.  **RL Agent (Deep Q-Network):** A student that learns by trying and failing."

---

## 2. How We Create Tasks (The Workload Generator) (2 Minutes)
**Objective:** Show that the simulation is realistic.
**File:** `src/workload_generator.py`

*   "First, let's look at how we simulate work. Please open `src/workload_generator.py`."
*   "Go to **Line 132**. Here we create a `Task` object. Every task has a Size and 'Intensity' (how much math it needs)."
*   "Look at **Lines 100-102**:
    ```python
    100:                 # GPU-bound peak
    101:                 compute_intensity = np.random.normal(0.8, 0.1)
    102:             compute_intensity = np.clip(compute_intensity, 0.05, 1.0)
    ```
    *   **Simple Explanation:** We create random tasks. Some are very heavy (Intensity 1.0), which should go to GPU. Some are light (Intensity 0.1), which should stay on CPU."
*   "Look at **Line 126**:
    ```python
    126:                 base_duration = (size / 1000) ** 1.5
    ```
    *   **Simple Explanation:** Bigger tasks take longer to finish. We simulate this math here."

---

## 3. The Reinforcement Learning (RL) Agent (2 Minutes)
**Objective:** Explain the "Student" AI.
**Concept:** It learns by trial and error.
**File:** `src/dqn_scheduler.py`

*   "Now, the star of the show: The RL Agent. This comes from 'Deep Q-Learning'."
*   "Open `src/dqn_scheduler.py`."
*   "Look at **Section 135-152** (The `get_action` function):
    ```python
    135:     def get_action(self, task: Task) -> Dict:
    ...
    140:         state = self._get_state_vector(task)
    143:         if random.random() < self.epsilon:
    144:             action = random.randrange(self.action_dim)
    ```
    *   **Line 140:** The agent looks at the task (its Size and Intensity). This is like looking at a math problem."
    *   **Line 143:** Sometimes it guesses randomly (Exploration). This helps it discover new tricks."
*   "If it doesn't guess, it uses its Brain (**Line 148**):
    ```python
    148:                 q_values = self.policy_net(state_tensor)
    ```
    *   **Simple Explanation:** The Neural Network predicts: 'If I give 50% of this task to the GPU, how good will the reward be?'"
*   **The Special Trick:** "We use **Fractional Actions** (**Line 152**). Instead of just 'GPU vs CPU', the agent can say 'Give 40% to GPU'. This is very precise!"

---

## 4. The Hybrid ML (The Teacher) (2 Minutes)
**Objective:** Explain the supervised model.
**File:** `src/offline_trainer.py`

*   "The RL agent is slow to start, so we have a 'Teacher' called Hybrid ML."
*   "Open `src/offline_trainer.py`."
*   "It uses a **Random Forest**, which is a classic Machine Learning model."
*   "Look at **Line 119**:
    ```python
    119:         results = self.model.fit(X, y, test_size=test_size)
    ```
    *   **Simple Explanation:** We run thousands of experiments first. We find the 'Perfect Answer' (Oracle). Then we show these answers to the Hybrid ML model. It memorizes the patterns."
*   "We actually used this Hybrid ML to **pre-train** the RL agent, so the Student starts smart!"

---

## 5. How We Calculate Efficiency (1 Minute)
**Objective:** Explain the score.
**File:** `src/simulation_engine.py`

*   "How do we know who is winning? Please go to `src/simulation_engine.py`, **Line 199**."
*   "Here is the `_calculate_metrics` function."
    ```python
    203:         time = result['actual_time']
    204:         gpu_frac = result['gpu_fraction']
    206:         # Power Model: GPU=50W, CPU=30W
    207:         power = (gpu_frac * 50.0) + ((1.0 - gpu_frac) * 30.0)
    208:         energy_joules = power * time
    ```
*   **Simple Explanation:**
    *   **Time:** How long the task took. Faster is better.
    *   **Energy:** We assume a GPU uses 50 Watts and CPU uses 30 Watts.
    *   **Calculation:** `Energy = Power * Time`.
    *   **Goal:** We want to minimize BOTH Time and Energy."

---

## 6. The Dashboard & Results (2 Minutes)
**Objective:** Show the live demo.
**Action:** Switch to the Browser / Dashboard UI.

*   "Let me show you the live system."
*   "On the screen, you see the **Live Task Stream**. Each moving bar is a task."
*   "**Colors:**
    *   **Blue bars:** CPU work.
    *   **Green bars:** GPU work."
*   "**The Charts:**
    *   You can see the **Average Task Duration** chart.
    *   **Hybrid ML (Green Line)** and **RL Agent (Purple Line)** are very low. This means they are FAST."
    *   **Round Robin (Orange Line)** is high. It is slow and dumb."
*   "**Conclusion:** My RL Agent (Student) learned to be almost as fast as the Hybrid ML (Teacher), but it is often more energy-efficient because it thinks about the long term!"

---

## Summary for Q&A
*   **Workload Generator:** create random math problems.
*   **Hybrid ML:** copies the best answers.
*   **RL Agent:** learns by doing.
*   **Efficiency:** Speed + Low Power.


======= FILE: run_dashboard.py =======

import json
import time
import threading
import random
from loguru import logger
from kafka import KafkaProducer
from kafka.errors import NoBrokersAvailable

from src.workload_generator import WorkloadGenerator
from src.dqn_scheduler import DQNScheduler
from main import load_config

# Kafka Configuration
KAFKA_BOOTSTRAP_SERVERS = ['localhost:9092']
KAFKA_TOPIC = 'scheduler-events'

def get_kafka_producer():
    """Retry logic for connecting to Kafka"""
    producer = None
    for i in range(10):
        try:
            producer = KafkaProducer(
                bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
                value_serializer=lambda x: json.dumps(x).encode('utf-8')
            )
            logger.info("Connected to Kafka!")
            return producer
        except NoBrokersAvailable:
            logger.warning(f"Kafka not available, retrying in 5s... ({i+1}/10)")
            time.sleep(5)
    return None

def run_simulation_loop():
    """Runs the simulation loop"""
    logger.info("Starting simulation loop...")
    
    # Connect to Kafka
    producer = get_kafka_producer()
    if not producer:
        logger.error("Could not connect to Kafka. Exiting.")
        return

    def kafka_callback(data):
        """Callback to publish to Kafka"""
        try:
            producer.send(KAFKA_TOPIC, value=data)
        except Exception as e:
            logger.error(f"Failed to send to Kafka: {e}")

    config = load_config()
    
    logger.info("Initializing RL Scheduler...")
    rl_scheduler = DQNScheduler(
        num_gpus=config['hardware']['num_virtual_gpus'],
        monitor_callback=kafka_callback
    )
    
    # Generate infinite stream of tasks
    wg = WorkloadGenerator(seed=int(time.time()))
    
    logger.info("Starting Live Scheduling...")
    
    task_id = 0
    while True:
        # Generate a few tasks
        new_tasks = wg.generate_workload(num_tasks=1, arrival_rate=0.5)
        for task in new_tasks:
            task.task_id = task_id
            task_id += 1
            
            # Simulate arrival delay
            time.sleep(random.uniform(0.5, 2.0)) 
            
            # Schedule
            rl_scheduler.randomize_resources() # Simulate dynamic environment
            rl_scheduler.schedule_task(task)
            
            # Also send resource update
            kafka_callback({
                'type': 'resources',
                'data': rl_scheduler.get_utilization()
            })

if __name__ == "__main__":
    run_simulation_loop()


======= FILE: test_config.py =======

from backend.core.config import settings

print("Database URL:", settings.database_url)
print("Async Database URL:", settings.async_database_url)


======= FILE: docker-compose.yml =======

version: '3'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1


======= FILE: PROJECT_WIKI_DOCUMENTATION.md =======

# Hybrid ML Scheduler for Parallel Computing: The Definitive Guide (v10.0)

**Version:** 10.0.0 (The Ultimate Edition)
**Date:** December 10, 2025
**Maintainers:** Hybrid Scheduler Research Team
**Repository:** `hybrid_ml_scheduler`

---

# ğŸ“– Table of Contents

1.  **A Beginner's Guide to AI Scheduling (The "Kitchen" Metaphor)**
2.  **Executive Summary & Achievements**
3.  **Theoretical Framework**
4.  **System Architecture & Data Flow**
5.  **Front-End Architecture (The Dashboard)**
6.  **Simulation Environment Specifications**
7.  **Technology Stack Deep Dive (The Tools We Use)**
8.  **Deep Dive: Hybrid ML (Random Forest)**
9.  **Deep Dive: The RL Agent (DQN)**
10. **The Oracle Justification**
11. **Visual Graph Gallery (Detailed Analysis)**
12. **Database & Data Model Reference**
13. **API Documentation**
14. **The 100-Question FAQ**

---

# 1. A Beginner's Guide to AI Scheduling (Start Here)

## 1.1 The Metaphor: The Busy Kitchen
Imagine a restaurant kitchen with **4 Master Chefs (GPUs)** and **1 Prep Cook (CPU)**.

*   **The Orders (Tasks):** Customers send in orders. Some are simple (chop onions), some are complex (SoufflÃ©).
*   **The Problem:** The Master Chefs are fast but expensive (salary). The Prep Cook is slow but cheap.
*   **Transfer Cost:** Giving an ingredient to a Master Chef takes time (walking across the kitchen). If the task is just "chop one onion", walking it to the Chef takes longer than just doing it yourself.
*   **The Scheduler:** You are the manager. You must decide for *every single order*: "Do I give this to a Chef or the Prep Cook?"

## 1.2 Why is this "AI"?
A traditional manager follows a rule: "All meat goes to Chefs." (This is a **Heuristic**).
But what if the Chefs are busy? What if the meat needs 1 second of cooking but 10 minutes of prep? The rule fails.
*   **Machine Learning (Hybrid ML):** You hire a manager who has studied 10,000 past orders. He looks at a new order and says, "Based on history, this takes 40% less time with a Chef."
*   **Reinforcement Learning (RL Agent):** You hire a TRAINEE manager. He knows nothing. He tries giving soup to a Chef. It spills. He gets yelled at (Negative Reward). Next time, he gives soup to the Prep Cook. He *learns by doing*.

---

# 2. Executive Summary & Achievements

## 2.1 Mission Statement
The **Hybrid ML Scheduler** project aims to solve the "Heterogeneous Scheduling Bottleneck" in High-Performance Computing (HPC). Modern clusters are no longer homogeneous; they contain CPUs, GPUs, TPUs, and FPGAs. Deciding *which* task runs on *which* hardware is critical.

Traditional schedulers use static heuristics (e.g., "GPU for math"). These heuristics fail when data transfer costs are high (PCIe bottleneck) or when resources are scarce. Our mission was to build a self-monitoring, self-optimizing scheduler that uses Artificial Intelligence to make real-time decisions based on task physics and cluster state.

## 2.2 Key Research Achievements
1.  **Solved the Cost/Performance Trade-off**: Demonstrated that maximizing performance (using GPUs) incurs a 5x financial cost penalty, a critical insight for cloud cost management.
2.  **Outperformed Standard Baselines**: The Reinforcement Learning (RL) agent consistently beats Greedy and Round-Robin schedulers by **15-20%** in total makespan for mixed workloads.
3.  **Autonomous Learning**: The agent learns "hidden rules" of the cluster (e.g., "Don't use GPU for tasks < 200MB") without human programming.
4.  **Real-Time Observability**: Built a full-stack dashboard proving millisecond-level visibility into cluster decisions, costs, and energy usage.

---

# 3. Theoretical Framework

## 3.1 Heterogeneous Computing & Amdahl's Law
This project is grounded in **Amdahl's Law**, which states that the speedup of a program is limited by its sequential part.
$$ Speedup(s) = \frac{1}{(1-p) + \frac{p}{s}} $$
Where $p$ is the parallel fraction and $s$ is the speedup of the parallel part.
*   **Relevance:** Our scheduler identifies tasks with high $p$ (High Compute Intensity) and assigns them to GPUs where $s \approx 4x$. Tasks with low $p$ are kept on CPUs to avoid the overhead of data transfer, which effectively increases the sequential part $(1-p)$.

## 3.2 The Scheduling Problem (NP-Hard)
Task scheduling is a variation of the **Bin Packing Problem**, which is NP-Hard. Finding the *perfect* schedule requires iterating through all $N!$ permutations.
*   **Our Approach:** Instead of solving for the perfect schedule (too slow), we approximate the optimal policy $\pi(s)$ using Deep Q-Learning.

---

# 4. System Architecture

## 4.1 Overall Pipeline Diagram

```mermaid
graph TD
    subgraph "Phase 1: Profiling"
        H(Hardware Profiler) -->|Benchmark| P[Hardware Profile JSON]
    end

    subgraph "Phase 2 & 3: Training"
        WG[Workload Gen] -->|Training Data| ML[Random Forest Trainer]
        WG -->|Pre-train Data| RL[DQN Trainer]
    end

    subgraph "Phase 4: Runtime"
        Gen[Generator Stream] -->|Task| Sch{Scheduler Decision}
        Sch -->|Hybrid Prediction| S1[Simulate ML]
        Sch -->|DQN Policy| S2[Simulate RL]
        
        S1 -->|Results| DB[(PostgreSQL)]
        S2 -->|Reward| RL
    end
    
    DB -->|Sync| API[FastAPI Backend]
    API -->|WebSocket| UI[React Dashboard]
```

## 4.2 Data Flow Lifecycle
1.  **Task Ingestion:** A `Task` object is created with `size`, `compute_intensity`, and `memory_required`. API: `POST /api/tasks` (Internal).
2.  **Feature Vectorization:** The scheduler converts the Task object into a normalized tensor: `[Size/Max, Intensity, Mem/Max]`.
3.  **Inference:**
    *   **Hybrid ML:** Queries Scikit-Learn model `model.predict(features)`. Returns `gpu_fraction`.
    *   **RL Agent:** Queries PyTorch model `q_net(state)`. Returns `argmax(Q_values)`.
4.  **Execution Simulation:** The `VirtualMultiGPU` class calculates the physical time: `Time = Work / (Speed * Speedup)`.
5.  **Metrics Calculation:** Energy ($J$) and Cost ($$) are computed.
6.  **Persistence:** Results are saved to `scheduler_results` table.
7.  **Broadcast:** The result is pushed to the Frontend via `manager.broadcast()`.

---

# 5. Front-End Architecture (The Dashboard)

The **Hybrid Scheduler Dashboard** is a modern, responsive Single Page Application (SPA) built to visualize the "Brain" of the cluster in real-time.

## 5.1 Tech Stack
*   **Core Framework:** React 18 (Vite Build System)
*   **Styling:** Tailwind CSS (Utility-first styling for speed)
*   **Visualization:** Recharts (D3-based React charts)
*   **Layout:** React Grid Layout (Draggable, resizable widgets)
*   **Icons:** Lucide React (Clean, vector-based icons)
*   **Communication:** Native WebSockets (Real-time bi-directional stream)

## 5.2 Key Components (`dashboard/src/App.jsx`)

### The State Machine
The top-level `App` component manages the global state of the simulation:
```javascript
const [data, setData] = useState([]); // Time-series load data
const [comparisonData, setComparisonData] = useState([]); // Bar chart race
const [activeView, setActiveView] = useState('global'); // Navigation state
const [isConnected, setIsConnected] = useState(false); // Socket health
```

### WebSocket Integration
We use a robust WebSocket hook that ensures reconnection on failure:
```javascript
useEffect(() => {
  const connect = () => {
    const ws = new WebSocket('ws://localhost:8000/ws');
    ws.onmessage = (event) => {
      const message = JSON.parse(event.data);
      if (message.type === 'simulation_update') {
         // Batch updates to React State
         setData(prev => [...prev, transform(message)].slice(-50));
      }
    };
  };
  connect();
}, []);
```
*   **Optimization:** We slice the data array to the last 50 points to prevent memory leaks in long-running simulations.

### Visualization Widgets (The "Eyes" of the Operator)

We have designed 8 specialized widgets to visualize different aspects of the scheduling problem.

#### 1. **Global Comparison Bar Chart (The "Race")**
*   **Visual:** A horizontal bar chart race.
*   **Purpose:** The primary leaderboard. It shows the **Average Execution Time per Task** for each scheduler.
*   **Interpretation:** Shorter bars are better. The "Oracle" bar sets the impossible standard. The "RL Agent" should be chasing the Oracle. If "Round Robin" is beating "RL Agent", something is wrong.

#### 2. **Real-Time Utilization Area Chart (The "Pulse")**
*   **Visual:** A flowing, stacked area chart.
*   **Purpose:** Shows the instantaneous load on the cluster (Combined CPU + GPU usage) over the last 60 seconds.
*   **Interpretation:**
    *   **Flat Line (0%):** System is idle/broken.
    *   **Flat Line (100%):** System is saturated (good for throughput, bad for latency).
    *   **Wavy:** Healthy variation in workload.

#### 3. **Radar Chart (The "Spider")**
*   **Visual:** A pentagonal or triangular radar plot.
*   **Purpose:** Multi-dimensional comparison of the *currently selected* scheduler against the "Oracle" baseline.
*   **Axes:**
    *   **Time:** Lower is better.
    *   **Energy:** Lower is better.
    *   **Cost:** Lower is better.
*   **Interpretation:** You want your scheduler's shape to be *inside* or *perfectly overlapping* the Oracle's shape. If it spikes out on "Cost", your agent is spending too much money.

#### 4. **Historical Performance Table (The "Ledger")**
*   **Visual:** A sortable data grid.
*   **Purpose:** Displays raw numerical data for the last N tasks.
*   **Columns:** Task ID, Assigned Scheduler, Time, Energy, Cost.
*   **Interpretation:** useful for auditing specific "bad decisions" where an agent might have spiked execution time.

#### 5. **Task Distribution Histogram (The "Workload")**
*   **Visual:** A bar chart showing the frequency of task sizes.
*   **Purpose:** Verifies that the workload generator is creating a diverse mix of tasks (Small, Medium, Large).
*   **Interpretation:** Should look like a "Long Tail" (many small tasks, few huge ones).

#### 6. **Win/Loss Matrix (The "Scoreboard")**
*   **Visual:** A heatmap grid.
*   **Purpose:** Shows how often Scheduler A beats Scheduler B.
*   **Interpretation:** Green cells mean "Row wins against Column". A dominant RL agent should be green across its entire row.

#### 7. **Correlation Matrix (The "Insights")**
*   **Visual:** A color-coded heatmap of statistics.
*   **Purpose:** Reveals relationships between variables (e.g., "Does `Task Size` correlate with `Execution Time`?").
*   **Interpretation:**
    *   **+1.0 (Red):** Perfect positive correlation.
    *   **-1.0 (Blue):** Perfect negative correlation.
    *   **0.0 (Grey):** No relationship.

#### 8. **Live Log Stream (The "Console")**
*   **Visual:** A scrolling terminal-like window.
*   **Purpose:** Shows raw text logs from the backend (e.g., "Task 105 assigned to GPU 3").
*   **Interpretation:** Essential for catching specific logic errors or connection drops.

## 5.3 UX Design Decisions
*   **Glassmorphism:** We used semi-transparent backgrounds (`bg-black/20 backdrop-blur`) to create a futuristic "Sci-Fi" aesthetic suitable for an AI project.
*   **Neon Accents:** Critical metrics use high-contrast neon colors (Cyan for ML, Purple for RL) to pop against the dark mode theme.
*   **Interactive Playback:** The "Pause/Resume" button sends an API call to the backend to physically stop the generic task stream, allowing users to inspect a specific moment in time.

---

# 6. Simulation Environment Specifications

The `VirtualMultiGPU` class (`src/simulator.py`) is the physics engine of our universe.

## 6.1 Virtual Hardware
We simulate a single high-end compute node:

| Component | Specs | Simulation Behavior |
| :--- | :--- | :--- |
| **Host CPU** | 1 Node (Multi-core) | Base execution speed ($1.0x$). Handles all OS overhead. |
| **Virtual GPUs** | 4 Units | Accelerators ($1x - 4x$ speedup). |
| **Memory** | 8000 MB per GPU | Tasks exceeding this fail or paging logic kicks in. |
| **Interconnect** | PCIe Gen 3 (16GB/s) | Limits data transfer speed. Modeled as `TransferTime = Size / Bandwidth`. |

## 6.2 Cost & Power Models (The "Physics")

### Financial Cost ($)
Based on approximate AWS Spot Instance pricing (e.g., `p3.8xlarge` vs `c5.large`):
*   **GPU Rate:** **$0.0001 per second**. This is 5x more expensive than CPU to penalize indiscriminate usage.
*   **CPU Rate:** **$0.00002 per second**.

### Power Consumption (Watts)
Based on TDP (Thermal Design Power) profiles:
*   **GPU Power:** **50 Watts** (Active Load).
*   **CPU Power:** **30 Watts** (Active Load).
*   **Formula:** `Energy (J) = Power (W) * Time (s)`

---

# 7. Technology Stack Deep Dive (The Tools We Use)

To build this "Brain", we used specific tools. Here is what each one does and why we picked it.

## 7.1 PyTorch (`torch`)
*   **What it is:** A library for building Neural Networks.
*   **Role here:** Powering the **RL Agent (DQN)**.
*   **Key Concept: The Tensor:** A Tensor is just a matrix (list of numbers) that can live on the GPU. When we say "State", we convert the task info `[size, intensity]` into a Tensor `[0.5, 0.9]`.
*   **Key Concept: Autograd:** PyTorch automatically calculates "gradients" (errors). When the RL Agent makes a mistake, PyTorch calculates exactly how much to adjust each neuron to fix it.

## 7.2 Scikit-Learn (`sklearn`)
*   **What it is:** The industry standard for classical Machine Learning (Regression, Clustering).
*   **Role here:** Powering the **Hybrid ML (Random Forest)** and preprocessing data.
*   **Key Concept: Scaler:** `StandardScaler` squishes all our data (e.g., Size 100MB vs 10GB) into a standard range (-1 to 1). This helps the math work better.

## 7.3 Pandas (`pd`)
*   **What it is:** Excel for Python.
*   **Role here:** Handling the "History". When we save simulation results, we store them in a DataFrame. This fits perfectly into Scikit-Learn.

## 7.4 FastAPI
*   **What it is:** A modern web server for Python.
*   **Role here:** It's the "Mouth" of our project. It speaks to the React Dashboard. It uses `async` (asynchronous code) to handle hundreds of requests per second without crashing.

---

# 8. Deep Dive: Hybrid ML (Random Forest)

## 8.1 The Core Concept: Decision Trees
The "Hybrid" scheduler uses a **Random Forest**. To understand a Forest, you must understand a **Tree**.
A Decision Tree asks a sequence of Yes/No questions to make a prediction.
*   *Question 1:* "Is the task Size > 500MB?"
    *   *Yes:* *Question 2:* "Is Compute Intensity > 0.8?"
        *   *Yes:* **Prediction:** "Run on GPU."
        *   *No:* **Prediction:** "Run on CPU."

## 8.2 Why a "Forest"?
One tree is often wrong (it "overfits" or memorizes the data).
A **Random Forest** creates 100 different trees. Each tree gets a random subset of the data.
*   Tree 1 says: "GPU"
*   Tree 2 says: "CPU"
*   Tree 3 says: "GPU"
*   ...
*   **The Vote:** The Forest averages all 100 answers. If 70 say GPU, the confidence is 0.7. This "Wisdom of the Crowd" is mathematically proven to be more accurate than any single expert.

## 8.3 In Our Code (`src/ml_models.py`)
We use `RandomForestRegressor`.
*   **Input:** `[Task Size, Compute Intensity, Memory Requirement]`
*   **Output:** `Optimal GPU Fraction` (A number 0.0 to 1.0).
*   **Why we chose it:** It requires very little "tuning". It just works.

### Code Snippet: Training Logic
```python
def fit(self, X, y):
    # 1. Normalize data so "Size" doesn't dominate "Intensity"
    X_scaled = self.scaler.fit_transform(X)
    
    # 2. Train the forest with 100 trees
    self.model = RandomForestRegressor(n_estimators=100)
    self.model.fit(X_scaled, y)
```

---

# 9. Deep Dive: The RL Agent (DQN)

## 9.1 The Core Concept: Q-Learning
Reinforcement Learning is about **mapping States to Actions to maximize Reward**.
We use **Q-Learning**. "Q" stands for "Quality".
The agent tries to learn a function $Q(S, A)$ which tells it:
> "If I am in State $S$ (e.g., Large Task) and I take Action $A$ (e.g., Use GPU), what is the Total Future Reward I will get?"

## 9.2 The "Deep" in Deep Q-Network (DQN)
In simple Q-learning, we use a table (Excel sheet).
*   Row = State, Column = Action.
*   Problem: Our state (Task Size) is continuous. We can have 100MB, 100.1MB, 100.01MB... The table would be infinite!
*   **Solution:** We replace the table with a **Neural Network**.
    *   **Input:** The State (3 numbers).
    *   **Hidden Layers:** 256 Neurons (The "Brain" processing patterns).
    *   **Output:** 5 numbers (The Q-Value for each Action: CPU, GPU0, GPU1, GPU2, GPU3).

## 9.3 The "tricks" we used to make it work
Training a DQN is unstable. We added two key technologies:

### A. Experience Replay (The "Memory")
If the agent learns from tasks one-by-one, it might forget the past.
*   **Analogy:** If you only study math for a week, you forget history.
*   **Solution:** We save every decision `(State, Action, Reward, NextState)` into a buffer of 50,000 items. When training, we pick a **random batch** of 64 past experiences. This ensures the agent remembers both recent successes and old failures.

### B. Target Networks (The "Teacher")
The agent is "chasing its own tail". It tries to update its guess based on... its own guess!
*   **Solution:** We create TWO copies of the brain.
    *   **Policy Net (The Student):** Updates every step.
    *   **Target Net (The Teacher):** Updates only every 50 steps.
    *   The Student tries to match the Teacher's estimates. This stabilizes learning.

## 9.4 The Code Flow (`src/dqn_scheduler.py`)
1.  **Observe:** `get_state_vector(task)` -> `[0.1, 0.8, 0.2]`
2.  **Act:** `epsilon_greedy()` -> Flip a coin.
    *   *Heads (Exploit):* Ask Neural Net for best action.
    *   *Tails (Explore):* Pick random action (Try something new!).
3.  **Learn:**
    *   Calculate Loss: `(Predicted Q - True Reward)^2`.
    *   Backpropagate: Adjust weights using PyTorch `optimizer.step()`.

### Code Snippet: Neural Network
```python
class DQN(nn.Module):
    def __init__(self):
        # 256 Neurons in the hidden layer
        self.fc1 = nn.Linear(3, 256) 
        self.fc2 = nn.Linear(256, 256)
        # Output: 5 Actions (CPU + 4 GPUs)
        self.head = nn.Linear(256, 5)
        
    def forward(self, x):
        x = F.relu(self.fc1(x)) # Activation Function
        x = F.relu(self.fc2(x))
        return self.head(x)
```

---

# 10. The Oracle Justification

## Why Compare to an Oracle?
An **Oracle** is a theoretical scheduler that knows the future (or tries every possibility).
1.  **Upper Bound:** It establishes the "Speed of Light". We can't beat the Oracle. If our RL agent gets within 5% of the Oracle, we know we are doing great.
2.  **Training Data:** The Hybrid ML (Supervised) model *needs* labeled data. The Oracle provides the labels: "For this task, the BEST action was X".
3.  **Sanity Check:** If the Oracle also performs poorly, it proves the workload itself is impossible, not our scheduler.

---

# 11. Visual Graph Gallery (Detailed Analysis)

We generated extensive plots to validate our hypotheses. Here is what they mean.

## 11.1 Makespan Comparison
![Makespan Comparison](/Users/anurag/codebase/Parallel%20Computing/hybrid_ml_scheduler/data/results/plots/makespan_comparison.png)
*   **What leads to this result?** The RL agent (Red bar) is lowest. Why? Because it learned that small tasks hurt GPU performance (due to transfer time). By correctly filtering small tasks to the CPU, it kept the GPUs free for the monster tasks that *really* needed them.

## 11.2 Cost Comparison
![Cost Comparison](/Users/anurag/codebase/Parallel%20Computing/hybrid_ml_scheduler/data/results/plots/cost_comparison.png)
*   **The "Price of Speed":** Notice RL is expensive. The CPU Only bar is cheap. This teaches us that **Time = Money**. If you want the job done fast (RL), you must pay the "GPU Tax" ($0.0001/sec).

## 11.3 Latency Distribution (CDF)
![Latency CDF](/Users/anurag/codebase/Parallel%20Computing/hybrid_ml_scheduler/data/results/plots/latency_cdf.png)
*   **Reading the Curve:** The X-axis is "Time to finish". The Y-axis is "% of tasks finished".
*   **The RL Advantage:** The RL curve shoots up vertically. This means almost *all* tasks finished instantly. The other curves slope gently, meaning many tasks got stuck in queues.

## 11.4 Workload Characterization
![Task Size Distribution](/Users/anurag/codebase/Parallel%20Computing/hybrid_ml_scheduler/data/results/plots/task_size_dist.png)
*   **The Long Tail:** This shape is called a "Log-Normal" or "Pareto" distribution. It is very common in computer systems (files sizes, network packets). Our scheduler is designed specifically to handle this "imbalance".

---

# 12. Database & Data Model Reference

## 12.1 Entity Relationship Diagram (ERD)
```text
[Tasks] 1 ---- * [SchedulerResults]
   |                 |
   |-- task_id       |-- id (PK)
   |-- size          |-- task_id (FK)
   |-- intensity     |-- scheduler_name (e.g., 'rl_agent')
   |-- memory        |-- actual_time
                     |-- energy_consumption
                     |-- execution_cost
```

## 12.2 Key Pydantic Schemas (`backend/models/schemas.py`)
These enforce data validity for the API.
*   **`TaskCreate`**:
    *   `task_id` (int)
    *   `size` (float)
    *   `compute_intensity` (float, 0.0-1.0)
    *   `memory_required` (float)
*   **`SchedulerResultCreate`**:
    *   `gpu_fraction` (float, 0.0-1.0)
    *   `actual_time` (float, >0)

---

# 13. API Documentation

## 13.1 REST Endpoints
*   **`GET /api/status`**
    *   Returns: `{"is_running": true, "tasks_processed": 500, ...}`
    *   Use: Heartbeat check for dashboard.
*   **`POST /api/pause`**
    *   Action: Suspends the simulation loop inside `ContinuousSimulation`.
*   **`POST /api/resume`**
    *   Action: Resumes the loop.
*   **`GET /api/history/comparative`**
    *   Returns: List of detailed results for charts.
    *   Note: Can be heavy; supports `limit` query param.

## 13.2 WebSocket Protocol (`/ws`)
*   **Update Frequency:** ~2Hz (Every 500ms).
*   **Message Format:**
    ```json
    {
      "type": "simulation_update",
      "task": { "id": 101, "intensity": 0.9 },
      "comparison": [ {"name": "rl_agent", "avg_time": 1.2}, ... ],
      "latest_results": { ... }
    }
    ```

---

# 14. The 100-Question FAQ

1.  **What is this project?** A hybrid scheduler using both ML and RL to optimize parallel task execution.
2.  **Why Hybrid?** Because we verify if RL (Unsupervised) can beat Random Forest (Supervised).
3.  **What hardware is required?** None. It runs on a simulator (`VirtualMultiGPU`).
4.  **Can it run on real hardware?** Yes, by swapping the `simulate()` method for a `subprocess.run()` call.
5.  **Why Python?** For its rich ML ecosystem.
6.  **What version of Python?** Tested on 3.10+.
7.  **What libraries are used?** PyTorch, Scikit-Learn, Pandas, FastAPI, React.
8.  **What is a "Task" here?** A synthetic object with Size (MB) and Intensity (0-1).
9.  **How many GPUs are simulated?** 4 by default.
10. **Can I change the GPU count?** Yes, in `config.yaml`.
11. **What is Amdahl's Law?** The formula limiting parallel speedup.
12. **Why 4x max speedup?** It's a conservative estimate for typical CUDA kernels vs Optimized C++.
13. **Why is GPU cost higher?** In cloud (AWS/GCP), GPUs cost significantly more per hour.
14. **Does it handle memory paging?** Simplified; tasks exceeding memory just "fail" or go to CPU in the logic.
15. **What is "Compute Intensity"?** Ratio of FLOPs to Bytes I/O.
16. **Why use Random Forest?** It's robust to outliers and requires little tuning.
17. **Why not XGBoost?** XGBoost was tested but RF was faster to train in-loop.
18. **Why DQN?** It handles discrete action spaces (GPU selection) well.
19. **Why not PPO?** PPO is more complex to implement for this simple discrete space.
20. **What is Epsilon-Greedy?** A strategy to balance exploration (trying new things) and exploitation (doing what works).
21. **What is the state space?** Task Size, Intensity, Memory.
22. **What is the action space?** CPU, GPU 0, GPU 1, GPU 2, GPU 3.
23. **What is the reward function?** Negative combination of Time and Energy.
24. **Why negative reward?** RL maximizes score; we want to minimize time.
25. **Does it model Network Latency?** Implicitly via "Transfer Time".
26. **What is PCIe bandwidth?** 16 GB/s in our sim.
27. **Why 16 GB/s?** Represents PCIe Gen 3 x16.
28. **Does it support multi-node?** No, currently single-node multi-GPU.
29. **How is the Dashboard built?** React with Vite.
30. **How does the Dashboard get data?** WebSockets.
31. **What is the update frequency?** ~500ms.
32. **Why use SQLite?** Zero config setup for local dev.
33. **Can I use Postgres?** Yes, the ORM supports it.
34. **What is the `reporting.py` for?** Generating the PDF summary.
35. **How is the PDF generated?** `xhtml2pdf` library.
36. **What is `pipeline.py`?** The orchestrator function.
37. **What is `profiler.py`?** Benchmarks the host machine (if running locally).
38. **Does `profiler.py` run on M1/M2 Macs?** Yes, supports MPS backend.
39. **What is "Makespan"?** The total time to finish a batch of tasks.
40. **What is "Throughput"?** Tasks per second.
41. **Why is Random scheduler bad?** It ignores task characteristics.
42. **Why is Greedy scheduler good?** It follows the basic physics rule (Math -> GPU).
43. **When does Greedy fail?** When a task is Math-heavy but Data-huge (Transfer cost > Speedup).
44. **Does the RL agent capture that failure?** Yes, it learns to avoid huge transfers.
45. **How long does RL training take?** 1000 tasks effectively "warms up" the agent.
46. **What is "Experience Replay"?** Storing past actions to retrain on them later.
47. **Why use Experience Replay?** Breaks correlation between consecutive tasks.
48. **What is "Target Network"?** A stable copy of the neural net to calculate loss against.
49. **Why "Virtual"?** Because running 10,000 actual matrix multiplications would take hours and cost electricity.
50. **How accurate is the sim?** It's a first-order approximation (correct trends, synthesized magnitudes).
51. **Can I add more metrics?** Yes, edit `_calculate_metrics` in `simulator.py`.
52. **What is the cost unit?** User-defined abstract dollars ($).
53. **What is `workload_generator.py`?** Creates the synthetic task stream.
54. **Is the workload random?** Yes, but seeded for reproducibility.
55. **Does it support dependency graphs (DAGs)?** Code has support, but currently generating independent tasks.
56. **What is "Arrival Rate"?** How many tasks enter the queue per second.
57. **What happens if queue is full?** Latency increases (Queuing Theory).
58. **Does RL verify "Energy"?** Yes, it's half the reward function.
59. **Why prioritize Energy?** Green computing and thermal limits.
60. **What if I don't care about Energy?** Set `energy_weight=0.0` in `dqn_scheduler.py`.
61. **What happens if I set `energy_weight=1.0`?** Agent will likely pick CPU (Low power) always.
62. **Is there a Dockerfile?** Yes available (implicit in structure).
63. **How to deploy?** `docker compose up`.
64. **Can I run this on Windows?** Yes, Python is cross-platform.
65. **Can I run this on Linux?** Yes, preferred environment.
66. **What is `backend/services/`?** Business logic layer for the API.
67. **What is `backend/routers/`?** API endpoints definitions.
68. **What is Pydantic?** Data validation library.
69. **Why Async?** To handle high concurrency of tasks.
70. **What is Loguru?** A better logging library than standard `logging`.
71. **How to debug?** Check `logs/app.log`.
72. **How to visualize weights?** Use TensorBoard (optional integration).
73. **Can I change the neural net architecture?** Yes, edit `DQN` class.
74. **What is the "Oracle" strategy?** Brute force search of all GPU fractions (0.0 to 1.0).
75. **Why is Oracle slow?** It runs `simulate()` 11 times per task.
76. **Do we use Oracle in production?** No, only for training data generation.
77. **What is "Offline Training"?** Phase 3, where we train models before running live.
78. **What is "Online Training"?** RL Agent learning while running.
79. **Does RL degrade performance initially?** Yes, due to exploration.
80. **Can we pre-train RL?** Yes, using `pretrain()` method with heuristic data.
81. **What is the input vector size?** 3 (Size, Intensity, Memory).
82. **What is the hidden layer size?** 256.
83. **Why 256?** Empirical sweet spot for this complexity.
84. **What is Batch Normalization?** Stabilizes learning by normalizing inputs.
85. **Does the agent overfit?** Unlikely with the dynamic workload generation.
86. **How to save the model?** `scheduler.save_model()`.
87. **How to load?** `scheduler.load_model()`.
88. **What format is the model?** PyTorch `.pth`.
89. **What format is the Random Forest?** Joblib `.pkl`.
90. **Can I use my own workload trace?** Yes, replace CSV in `data/workload_traces/`.
91. **What is the "Heavy" simulation?** A dedicated stress test with 10k tasks.
92. **Why 10k tasks?** To reach statistical significance.
93. **What is the "CDF" plot?** Cumulative Distribution Function.
94. **Why overlap happens in plots?** Fixed by rotating labels.
95. **What is the future of this project?** Multi-node support and Transformer agents.
96. **Can it schedule microservices?** Concepts apply, but tailored for HPC kernels.
97. **Is it open source?** Yes (Internal Research License).
98. **Who built this?** The Hybrid Scheduler Team.
99. **How to contribute?** Open a PR on the repo.
100. **Is this the best scheduler ever?** It is definitely the most "self-aware" one we've built!

---
**End of Project Wiki**


======= FILE: PROJECT_DOCUMENTATION.md =======

# Hybrid ML Scheduler - Project Documentation

**Version:** 2.0.0
**Date:** November 27, 2025

---

## 1. Executive Summary

The **Hybrid ML Scheduler** is an advanced simulation and scheduling system designed to optimize task allocation in heterogeneous computing environments (CPU + GPU). It leverages **Machine Learning (Random Forest)** and **Reinforcement Learning (DQN)** to make intelligent scheduling decisions that balance execution time, energy consumption, and cost.

The system features a continuous simulation engine, a robust backend API, persistent storage, and a real-time interactive dashboard for visualization and analysis.

---

## 2. Installation & Setup

### Prerequisites
*   **Python 3.9+**
*   **Node.js 16+**
*   **PostgreSQL** (Optional, for full persistence)
*   **Redis** (Optional, for caching)

### Backend Setup
1.  **Create Virtual Environment**:
    ```bash
    python -m venv .venv
    source .venv/bin/activate  # On Windows: .venv\Scripts\activate
    ```
2.  **Install Dependencies**:
    ```bash
    pip install -r requirements.txt
    ```
3.  **Run the Server**:
    ```bash
    uvicorn src.dashboard_server:app --reload
    ```
    The API will be available at `http://localhost:8000`.

### Frontend Setup
1.  **Navigate to Dashboard**:
    ```bash
    cd dashboard
    ```
2.  **Install Dependencies**:
    ```bash
    npm install
    ```
3.  **Start Development Server**:
    ```bash
    npm run dev
    ```
    The dashboard will be available at `http://localhost:5173`.

---

## 3. System Architecture (High-Level Design)

The system follows a modern, modular architecture composed of four main layers:

1.  **Presentation Layer (Frontend):** A React-based dashboard for real-time monitoring and control.
2.  **Application Layer (Backend):** A FastAPI server handling API requests, WebSocket streaming, and business logic.
3.  **Simulation Layer (Engine):** A Python-based engine that generates workloads and executes scheduling strategies.
4.  **Data Layer (Persistence):** PostgreSQL for long-term storage and Redis for high-speed caching.

```mermaid
graph TD
    subgraph "Frontend (React)"
        Dash[Dashboard UI]
    end

    subgraph "Backend (FastAPI)"
        API[API Endpoints]
        WS[WebSocket Manager]
        Sim[Simulation Engine]
        Gen[Workload Generator]
        Schedulers[Schedulers]
    end

    subgraph "Data Layer"
        DB[(PostgreSQL)]
        File[History CSV]
    end

    Gen -->|Tasks| Sim
    Sim -->|State| Schedulers
    Schedulers -->|Decisions| Sim
    Sim -->|Real-time Updates| WS
    WS -->|JSON Stream| Dash
    Sim -->|Persist Results| File
    Sim -->|Persist Results| DB
    Dash -->|Control (Start/Stop)| API
    API -->|Commands| Sim
```

### Data Flow Overview
1.  **Workload Generation:** The Simulation Engine generates synthetic tasks with varying characteristics (size, compute intensity, memory).
2.  **Scheduling:** Tasks are processed by multiple schedulers in parallel (Round Robin, Random, Greedy, Hybrid ML, RL Agent, Oracle).
3.  **Execution Simulation:** A Virtual Cluster model estimates execution time and energy based on the scheduling decision.
4.  **Data Persistence:** Results are saved to PostgreSQL via the Backend API. Training data is buffered and stored.
5.  **Model Retraining:** The Offline Trainer periodically retrains the ML model using the latest historical data from the database.
6.  **Visualization:** The Backend broadcasts real-time updates via WebSockets to the Frontend Dashboard.

---

## 4. Deep Dive: Core Logic & Algorithms

This section explains the internal mechanics of the system, allowing you to understand the "how" and "why" without reading the code.

### 4.1. Workload Generation (The "Tasks")
The system generates a continuous stream of synthetic tasks that mimic real-world parallel computing jobs.

*   **Generation Process:** Tasks arrive according to a **Poisson Process** (exponentially distributed inter-arrival times), creating a realistic, bursty workload.
*   **Task Attributes:**
    *   **Size ($N$):** The magnitude of the problem (100 - 5000 units).
    *   **Compute Intensity ($I$):** A value between 0.0 and 1.0 indicating how much the task benefits from parallelization.
        *   $I \approx 1.0$: Highly parallelizable (Matrix Multiplication, Deep Learning).
        *   $I \approx 0.0$: Serial (I/O bound, recursive logic).
    *   **Memory Required ($M$):** RAM usage (10 - 500 MB).
*   **Duration Model:** The estimated base duration is calculated as:
    $$ T_{base} \propto \frac{N^{1.5}}{I + 0.5} $$
    *This means larger tasks take super-linearly longer, but high compute intensity reduces time (assuming parallel hardware).*

### 4.2. The Schedulers (The "Competitors")
The system runs six scheduling strategies in parallel for every task to compare their performance.

#### 1. Round Robin (Baseline)
*   **Logic:** Alternates blindly between resources.
*   **Behavior:** Task $i$ goes to GPU, Task $i+1$ goes to CPU.
*   **Pros/Cons:** Simple but inefficient; sends GPU-hostile tasks to GPU and vice versa.

#### 2. Random (Baseline)
*   **Logic:** Assigns a random fraction of the task to the GPU ($0.0$ to $1.0$).
*   **Pros/Cons:** Acts as a stochastic baseline to prove that other methods are learning.

#### 3. Greedy (Heuristic)
*   **Logic:** Uses the task's **Compute Intensity** directly as the GPU fraction.
*   **Formula:** $Fraction_{GPU} = Intensity$
*   **Rationale:** High intensity tasks *should* go to GPU. This is a strong heuristic baseline.

#### 4. Hybrid ML (The "Brain")
*   **Type:** Supervised Learning (Random Forest Regressor via Scikit-Learn).
    *   **Model:** `RandomForestRegressor(n_estimators=100, max_depth=15)`.
    *   **Preprocessing:** Features are scaled using `StandardScaler`.
*   **Input Features:**
    *   `size`: Raw task size.
    *   `compute_intensity`: Parallelizability factor (0.0 - 1.0).
    *   `memory_required`: RAM usage in MB.
    *   `memory_per_size`: Density metric ($Memory / (Size + 1)$).
    *   `compute_to_memory`: Ratio metric ($Intensity / (Memory + 1)$).
*   **Decision Logic:**
    1.  **Predict:** The model predicts the optimal **GPU Fraction** ($0.0 - 1.0$).
    2.  **Select GPU:** The scheduler calculates a "Cost" for each available GPU to find the best placement:
        $$ Cost = (1 - w_E) \times \frac{T_{est}}{10.0} + w_E \times \frac{E_{est}}{500.0} $$
        *Where $w_E$ is the Energy Weight (default 0.5), normalizing time to ~10s and energy to ~500J.*
*   **Training Loop:**
    *   Retrains every 50 tasks (sliding window).
    *   Uses "Oracle" decisions (retrospective optimal choices) as the ground truth labels.

#### 5. RL Agent (Deep Q-Network)
*   **Architecture:** Dueling DQN (Deep Q-Network).
    *   **Value Stream:** Estimates state value $V(s)$.
    *   **Advantage Stream:** Estimates action advantage $A(s, a)$.
    *   **Aggregation:** $Q(s, a) = V(s) + (A(s, a) - \text{mean}(A))$.
    *   **Hidden Layers:** 2x Fully Connected (256 units, ReLU activation).
*   **State Space (Normalized):**
    *   $\text{Size} / 10000.0$
    *   $\text{Compute Intensity}$ (Raw 0-1)
    *   $\text{Memory} / 5000.0$
*   **Action Space:** Discrete options $[ \text{CPU}, \text{GPU}_0, \text{GPU}_1, \dots, \text{GPU}_N ]$.
*   **Reward Function:**
    *   The agent aims to maximize specific rewards defined as negative cost:
    *   $$ R = - \left[ (1 - w) \times T_{exec} + w \times \frac{E_{joules}}{100.0} \right] $$
*   **Hyperparameters:**
    *   `Gamma` (Discount Factor): 0.99
    *   `Epsilon` (Exploration): Starts at 1.0, decays to 0.01 (Factor: 0.9999).
    *   `Replay Buffer`: 50,000 transitions.
    *   `Batch Size`: 128.
    *   `Target Update`: Every 100 steps.

#### 6. Oracle (The "Ground Truth")
*   **Logic:** A theoretical solver that "cheats" by trying every possible split (0% to 100% in 5% steps).
*   **Purpose:** It finds the absolute mathematical minimum execution time for a task.
*   **Usage:**
    *   Acts as the **Label** for the Hybrid ML model (Supervised Learning).
    *   Serves as the **Performance Ceiling** (100% Efficiency) for comparison.

### 4.3. Simulation Physics
How do we calculate "Time" and "Energy"?

*   **Execution Time:**
    *   **CPU Time:** Base duration.
    *   **GPU Time:** $\frac{Base Duration}{Speedup} + TransferTime$
    *   **Speedup:** $1.0 + (3.0 \times Intensity)$ (Max 4x speedup for high intensity).
    *   **Transfer Time:** $\frac{Memory}{Bandwidth}$ (Simulating PCIe bottlenecks).
*   **Energy Consumption:**
    *   **GPU Power:** 50W (Active).
    *   **CPU Power:** 30W (Active).
    *   $Energy (Joules) = Power \times Time$.

---

## 5. System Flow & Architecture

### 5.1. The "Loop"
1.  **Generate:** A new task is born (`WorkloadGenerator`).
2.  **Broadcast:** The task is sent to all 6 schedulers simultaneously.
3.  **Decide:** Each scheduler makes its move (Predict, Randomize, or Calculate).
4.  **Simulate:** The `VirtualMultiGPU` calculates the *result* (Time, Energy) for each decision.
5.  **Persist:** Results are saved to PostgreSQL.
6.  **Learn:**
    *   **Hybrid ML:** If 50 tasks have passed, fetch history -> Retrain Random Forest.
    *   **RL Agent:** Store transition -> Update Q-Network weights.
7.  **Visualize:** Send JSON packet via WebSocket to the Dashboard.

### 5.2. Component Details

#### Frontend Dashboard (`/dashboard`)
*   **Tech:** React, Vite, TailwindCSS, Recharts.
*   **Key Views:**
    *   **Performance Race:** A live bar chart where shorter bars = faster schedulers.
    *   **Enhanced Analytics:**
        *   **Heatmap:** Shows which tasks (Size vs. Intensity) perform best on GPU.
        *   **Win/Loss Matrix:** How often Scheduler A beats Scheduler B.
        *   **State Management**: Uses React `useState` and `useEffect` for real-time data updates.

![Dashboard Mockup](docs/images/dashboard_mockup.png)

#### Backend Server (`/backend`)
*   **Tech:** FastAPI, Uvicorn, SQLAlchemy (Async), Pydantic.
*   **Role:** The central nervous system. It orchestrates the simulation, manages the DB connection, and serves the API.
*   **Security:** Implements Rate Limiting (Redis) and Input Validation to protect the system.

#### Data Layer
*   **PostgreSQL:** Stores the "Truth". Every single task execution is logged here.
*   **Redis:** The "Short-term Memory". Caches high-speed data like current stats to prevent DB overload.

## 6. Technical Reference

### 6.1. Database Schema (PostgreSQL)
The system uses a relational schema optimized for time-series performance.

#### `tasks` Table
Stores metadata for every generated task.
| Column | Type | Description |
|--------|------|-------------|
| `task_id` | Integer (PK) | Unique identifier for the task. |
| `size` | Float | Problem size ($N$). |
| `compute_intensity` | Float | Parallelizability factor ($0.0 - 1.0$). |
| `memory_required` | Float | RAM required in MB. |
| `arrival_time` | Float | Simulation timestamp of arrival. |
| `dependencies` | JSONB | List of parent task IDs. |

#### `scheduler_results` Table
Logs the outcome of every scheduling decision.
| Column | Type | Description |
|--------|------|-------------|
| `id` | Integer (PK) | Unique result ID. |
| `task_id` | Integer (FK) | Reference to the task. |
| `scheduler_name` | String | Name of the strategy (e.g., 'hybrid_ml'). |
| `gpu_fraction` | Float | Allocated GPU portion ($0.0 - 1.0$). |
| `actual_time` | Float | Execution time in seconds. |
| `energy_consumption` | Float | Energy used in Joules. |
| `execution_cost` | Float | Cost in USD. |

#### `training_data` Table
Historical data used to train the Hybrid ML model.
| Column | Type | Description |
|--------|------|-------------|
| `size` | Float | Task size. |
| `compute_intensity` | Float | Task intensity. |
| `optimal_gpu_fraction` | Float | **Label:** The best fraction found by Oracle. |
| `optimal_time` | Float | The execution time achieved by Oracle. |

### 6.2. API Specification (FastAPI)

#### Simulation Control
*   `POST /api/simulation/start`: Begin the continuous simulation.
*   `POST /api/simulation/stop`: Gracefully stop the engine.
*   `POST /api/simulation/pause`: Temporarily halt task generation.
*   `GET /api/simulation/status`: Get current stats (tasks processed, running state).

#### Data & Metrics
*   `GET /api/full_history`: Retrieve the latest 1000 training records (used for charts).
*   `GET /api/metrics`: Expose Prometheus-formatted metrics for scraping.
*   `GET /api/health`: Check connectivity to PostgreSQL and Redis.

#### WebSocket (`/ws`)
*   **Protocol:** JSON over WebSocket.
*   **Events:**
    *   `simulation_update`: Real-time packet with current task, scheduler results, and cluster utilization.
    *   `notification`: System alerts (e.g., "Model Retrained").

### 6.3. Configuration Management
The system is configured via environment variables (using `pydantic-settings`).

#### Key Variables (`.env`)
| Variable | Default | Description |
|----------|---------|-------------|
| `ENVIRONMENT` | development | App environment (dev/prod). |
| `POSTGRES_HOST` | localhost | Database host. |
| `POSTGRES_DB` | hybrid_scheduler_db | Database name. |
| `REDIS_HOST` | localhost | Redis cache host. |
| `NUM_GPUS` | 4 | Number of virtual GPUs to simulate. |
| `RETRAIN_INTERVAL` | 50 | Tasks between model updates. |

---

## 7. Directory Structure
```
hybrid_ml_scheduler/
â”œâ”€â”€ backend/                # FastAPI Application
â”‚   â”œâ”€â”€ api/                # Routes and Controllers
â”‚   â”œâ”€â”€ core/               # Config and DB setup
â”‚   â”œâ”€â”€ middleware/         # Rate Limit, Security
â”‚   â”œâ”€â”€ models/             # SQLAlchemy & Pydantic models
â”‚   â””â”€â”€ services/           # Business Logic (Data, Cache)
â”œâ”€â”€ dashboard/              # React Frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/     # Reusable UI widgets (Charts)
â”‚   â”‚   â””â”€â”€ App.jsx         # Main Dashboard View
â”œâ”€â”€ src/                    # Simulation Engine
â”‚   â”œâ”€â”€ simulation_engine.py # Main Loop
â”‚   â”œâ”€â”€ online_scheduler.py  # Hybrid ML Logic
â”‚   â”œâ”€â”€ dqn_scheduler.py     # RL Logic
â”‚   â””â”€â”€ workload_generator.py # Task Factory
â”œâ”€â”€ scripts/                # Utility Scripts (Init DB, Verify)
â””â”€â”€ tests/                  # Unit and Integration Tests
```

---



======= FILE: main.py =======

"""
Main Entry Point - Complete project execution pipeline
"""

import yaml
from loguru import logger

from src.pipeline import (
    run_profiling_phase,
    run_data_generation_phase,
    run_offline_training_phase,
    run_online_scheduling_phase,
    run_rl_training_phase,
    run_evaluation_phase
)


def load_config(config_path: str = "config.yaml") -> dict:
    """Load configuration"""
    with open(config_path) as f:
        return yaml.safe_load(f)


def main():
    """Main execution pipeline"""
    # Setup logging
    logger.add("logs/scheduler.log", rotation="500 MB")
    
    logger.info("="*80)
    logger.info("HYBRID OFFLINE-ONLINE ML SCHEDULER FOR PARALLEL COMPUTING")
    logger.info("="*80)
    
    # Load configuration
    config = load_config()
    logger.info(f"Configuration loaded")
    
    # Phase 1: Profiling
    profiler, profile = run_profiling_phase(config)
    
    # Phase 2: Data Generation
    wg, tasks = run_data_generation_phase(config)
    
    # Phase 3: Offline Training
    trainer, train_results = run_offline_training_phase(config, wg)
    
    # Phase 4: Online Scheduling
    scheduler, decisions = run_online_scheduling_phase(config, trainer)
    
    # Phase 4.5: RL Training
    rl_scheduler = run_rl_training_phase(config)
    
    # Phase 5: Evaluation
    baselines, results_df = run_evaluation_phase(config, scheduler, decisions, rl_scheduler)
    
    logger.info("\n" + "="*80)
    logger.info("PROJECT COMPLETE")
    logger.info("="*80)
    logger.info("Check data/results/ for all output files")


if __name__ == "__main__":
    main()


======= FILE: implementation_plan.md =======

# Cyberpunk Theme Implementation Plan

**Objective:** Apply a "Cyberpunk" aesthetic to the dashboard using dark mode, neon colors, and glassmorphism.

## Current Status
- Basic React setup.
- Tailwind CSS classes present but library not installed.

## Action Items

### 1. CSS Configuration
- Install dependencies: `npm install tailwindcss postcss autoprefixer`.
- Initialize configuration files.
- Update `tailwind.config.js` with the neon color palette.

### 2. Design Implementation
- **Colors:** Set background to deep black/grey. Define neon cyan, purple, and pink accents.
- **Effects:** Implement glassmorphism (semi-transparent panels with blur).
- **Typography:** Import and apply 'Orbitron' font.

### 3. Code Updates
- Update `index.css` with new variables and font imports.
- Refactor `App.jsx` to use new container classes.
- Update chart components to use neon colors matching the theme.

## Verification
- Start dev server (`npm run dev`).
- Visually verify color application and layout rendering.


======= FILE: tests/test_simulator.py =======


import pytest
from src.simulator import VirtualMultiGPU
from src.workload_generator import Task

class TestVirtualMultiGPU:
    
    @pytest.fixture
    def simulator(self):
        return VirtualMultiGPU(num_gpus=2)
        
    @pytest.fixture
    def sample_task(self):
        return Task(
            task_id=1,
            size=1000,
            compute_intensity=0.5,
            memory_required=100,
            arrival_time=0.0,
            duration_estimate=1.0
        )

    def test_initialization(self, simulator):
        assert simulator.num_gpus == 2
        assert simulator.memory_per_gpu == 8000
        assert simulator.task_timeline == []
        assert simulator.execution_log == []

    def test_simulate_task_execution_cpu_only(self, simulator, sample_task):
        # 0% GPU -> 100% CPU
        result = simulator.simulate_task_execution(sample_task, gpu_fraction=0.0)
        
        assert result['task_id'] == 1
        assert result['gpu_fraction'] == 0.0
        assert result['actual_time'] == sample_task.duration_estimate
        assert result['speedup'] == 1.0

    def test_simulate_task_execution_gpu_only(self, simulator, sample_task):
        # 100% GPU
        # Speedup = 1 + 3 * 0.5 = 2.5
        # GPU time = 1.0 / 2.5 = 0.4
        # Transfer time = 100 / 16000 = 0.00625
        # Total = 0.4 + 0.00625 = 0.40625
        
        result = simulator.simulate_task_execution(sample_task, gpu_fraction=1.0)
        
        expected_speedup = 1.0 + (3.0 * sample_task.compute_intensity)
        expected_gpu_time = sample_task.duration_estimate / expected_speedup
        expected_transfer = sample_task.memory_required / 16000.0
        expected_total = expected_gpu_time + expected_transfer
        
        assert result['gpu_fraction'] == 1.0
        assert result['actual_time'] == pytest.approx(expected_total)

    def test_simulate_task_execution_hybrid(self, simulator, sample_task):
        # 50% GPU, 50% CPU
        gpu_frac = 0.5
        result = simulator.simulate_task_execution(sample_task, gpu_fraction=gpu_frac)
        
        expected_speedup = 1.0 + (3.0 * sample_task.compute_intensity)
        expected_gpu_time = sample_task.duration_estimate / expected_speedup
        expected_transfer = sample_task.memory_required / 16000.0
        
        gpu_part = (expected_gpu_time + expected_transfer) * gpu_frac
        cpu_part = sample_task.duration_estimate * (1.0 - gpu_frac)
        expected_total = gpu_part + cpu_part
        
        assert result['actual_time'] == pytest.approx(expected_total)

    def test_evaluate_baseline_schedulers(self, simulator, sample_task):
        workload = [sample_task, sample_task] # 2 tasks
        baselines = simulator.evaluate_baseline_schedulers(workload)
        
        assert 'round_robin' in baselines
        assert 'random' in baselines
        assert 'greedy' in baselines
        assert 'offline_optimal' in baselines
        
        for strategy in baselines:
            assert 'makespan' in baselines[strategy]
            assert 'avg_time' in baselines[strategy]
            assert 'max_time' in baselines[strategy]


======= FILE: tests/test_scheduler_dependencies.py =======


import unittest
from unittest.mock import MagicMock
from src.online_scheduler import OnlineScheduler
from src.workload_generator import Task

class TestSchedulerDependencies(unittest.TestCase):
    def test_topological_scheduling(self):
        """Test that tasks are scheduled in topological order"""
        # Create a mock model that always predicts 0.5
        mock_model = MagicMock()
        mock_model.predict.return_value = [0.5]
        
        scheduler = OnlineScheduler(model=mock_model, num_gpus=1)
        
        # Create a diamond dependency graph:
        #   0
        #  / \
        # 1   2
        #  \ /
        #   3
        
        t0 = Task(0, 100, 0.5, 10, 0, 1.0, dependencies=[])
        t1 = Task(1, 100, 0.5, 10, 0, 1.0, dependencies=[0])
        t2 = Task(2, 100, 0.5, 10, 0, 1.0, dependencies=[0])
        t3 = Task(3, 100, 0.5, 10, 0, 1.0, dependencies=[1, 2])
        
        # Submit in reverse order to test queue handling
        scheduler.submit_task(t3)
        scheduler.submit_task(t2)
        scheduler.submit_task(t1)
        scheduler.submit_task(t0)
        
        decisions = scheduler.process_queue()
        
        scheduled_ids = [d['task_id'] for d in decisions]
        
        # Verify order
        self.assertEqual(len(scheduled_ids), 4)
        self.assertEqual(scheduled_ids[0], 0, "Task 0 must be first")
        self.assertIn(1, scheduled_ids[1:3], "Task 1 must be after 0 and before 3")
        self.assertIn(2, scheduled_ids[1:3], "Task 2 must be after 0 and before 3")
        self.assertEqual(scheduled_ids[3], 3, "Task 3 must be last")

if __name__ == '__main__':
    unittest.main()


======= FILE: tests/test_generator_efficiency.py =======


import unittest
import numpy as np
from src.workload_generator import WorkloadGenerator, Task

class TestWorkloadGenerator(unittest.TestCase):
    def test_stream_generation(self):
        """Test that the stream generator yields tasks"""
        wg = WorkloadGenerator(seed=42)
        stream = wg.generate_workload_stream(num_tasks=10)
        
        tasks = []
        for task in stream:
            self.assertIsInstance(task, Task)
            tasks.append(task)
            
        self.assertEqual(len(tasks), 10)
        
    def test_backward_compatibility(self):
        """Test that generate_workload still works and produces same results"""
        # Original implementation logic check (conceptually)
        # We can't easily check against the OLD code since we overwrote it, 
        # but we can check consistency.
        
        wg1 = WorkloadGenerator(seed=123)
        tasks1 = wg1.generate_workload(num_tasks=100)
        
        wg2 = WorkloadGenerator(seed=123)
        tasks2 = list(wg2.generate_workload_stream(num_tasks=100))
        
        self.assertEqual(len(tasks1), len(tasks2))
        for t1, t2 in zip(tasks1, tasks2):
            self.assertEqual(t1.task_id, t2.task_id)
            self.assertEqual(t1.size, t2.size)
            self.assertAlmostEqual(t1.arrival_time, t2.arrival_time)

    def test_large_scale_simulation(self):
        """Test that we can generate a large number of tasks without error"""
        wg = WorkloadGenerator(seed=42)
        # Generate 100,000 tasks but only consume a few to verify it starts
        stream = wg.generate_workload_stream(num_tasks=100000)
        
        count = 0
        for _ in stream:
            count += 1
            if count >= 100:
                break
        
        self.assertEqual(count, 100)

if __name__ == '__main__':
    unittest.main()


======= FILE: tests/test_dependencies.py =======


import unittest
import os
from src.workload_generator import WorkloadGenerator, Task

class TestTaskDependencies(unittest.TestCase):
    def test_dependency_generation(self):
        """Test that tasks are generated with dependencies"""
        wg = WorkloadGenerator(seed=42)
        tasks = wg.generate_workload(
            num_tasks=100,
            dependency_prob=0.5, # High probability for testing
            max_dependencies=3
        )
        
        # Check that some tasks have dependencies
        tasks_with_deps = [t for t in tasks if t.dependencies]
        self.assertGreater(len(tasks_with_deps), 0, "No dependencies generated")
        
        # Verify dependency validity
        for task in tasks:
            for dep_id in task.dependencies:
                # Dependency ID must be less than Task ID (no cycles, only backward refs)
                self.assertLess(dep_id, task.task_id, f"Task {task.task_id} depends on future task {dep_id}")
                self.assertGreaterEqual(dep_id, 0, "Dependency ID cannot be negative")

    def test_serialization(self):
        """Test that dependencies are saved and loaded correctly"""
        wg = WorkloadGenerator(seed=123)
        tasks = wg.generate_workload(num_tasks=50, dependency_prob=0.8)
        
        filename = "test_deps.csv"
        wg.save_workload(filename)
        
        try:
            wg_loaded = WorkloadGenerator.load_workload(filename)
            
            self.assertEqual(len(wg.tasks), len(wg_loaded.tasks))
            
            for t1, t2 in zip(wg.tasks, wg_loaded.tasks):
                self.assertEqual(t1.task_id, t2.task_id)
                self.assertEqual(t1.dependencies, t2.dependencies)
                
        finally:
            if os.path.exists(filename):
                os.remove(filename)

if __name__ == '__main__':
    unittest.main()


======= FILE: tests/test_energy_optimization.py =======


import unittest
from unittest.mock import MagicMock
from src.online_scheduler import OnlineScheduler, ResourceState
from src.workload_generator import Task

class TestEnergyOptimization(unittest.TestCase):
    def test_energy_cost_calculation(self):
        """Test that scheduler selects GPU based on energy-aware cost"""
        mock_model = MagicMock()
        mock_model.predict.return_value = [0.8] # GPU-heavy task
        
        # Create scheduler with high energy weight (prefer efficiency)
        scheduler = OnlineScheduler(model=mock_model, num_gpus=2, energy_weight=0.9)
        
        # Manually set GPU states
        # GPU 0: Low load (0.1) -> Fast execution -> Lower Energy
        # GPU 1: High load (0.9) -> Slow execution -> Higher Energy
        scheduler.gpu_states[0].current_load = 0.1
        scheduler.gpu_states[1].current_load = 0.9
        
        task = Task(0, 1000, 0.8, 100, 0, 1.0)
        
        # With identical hardware, lower load = faster = less energy
        # So it should pick GPU 0
        best_gpu = scheduler._find_best_gpu(task, gpu_fraction=0.8)
        self.assertEqual(best_gpu, 0)
        
    def test_weight_impact(self):
        """Test that energy weight parameter is stored correctly"""
        mock_model = MagicMock()
        s1 = OnlineScheduler(mock_model, energy_weight=0.1)
        s2 = OnlineScheduler(mock_model, energy_weight=0.9)
        
        self.assertEqual(s1.energy_weight, 0.1)
        self.assertEqual(s2.energy_weight, 0.9)

if __name__ == '__main__':
    unittest.main()


======= FILE: tests/test_offline_trainer.py =======


import pytest
from unittest.mock import MagicMock, patch
import pandas as pd
import numpy as np
from src.offline_trainer import OfflineTrainer
from src.workload_generator import Task, WorkloadGenerator

class TestOfflineTrainer:
    
    @pytest.fixture
    def trainer(self):
        return OfflineTrainer(model_type="random_forest", n_estimators=10)
        
    @pytest.fixture
    def mock_workload_generator(self):
        wg = MagicMock(spec=WorkloadGenerator)
        tasks = [
            Task(task_id=1, size=100, compute_intensity=0.1, memory_required=10, arrival_time=0.0, duration_estimate=1.0),
            Task(task_id=2, size=200, compute_intensity=0.9, memory_required=20, arrival_time=0.0, duration_estimate=2.0)
        ]
        wg.tasks = tasks
        return wg

    def test_initialization(self, trainer):
        assert trainer.model_type == "random_forest"
        assert trainer.model_kwargs == {'n_estimators': 10}
        assert trainer.model is None

    @patch('src.offline_trainer.VirtualMultiGPU')
    def test_prepare_data(self, mock_vmgpu_cls, trainer, mock_workload_generator):
        # Mock simulator to return a fixed time
        mock_sim = mock_vmgpu_cls.return_value
        mock_sim.simulate_task_execution.return_value = {'actual_time': 1.0}
        
        df = trainer.prepare_data(mock_workload_generator)
        
        assert isinstance(df, pd.DataFrame)
        assert len(df) == 2
        assert 'size' in df.columns
        assert 'optimal_gpu_fraction' in df.columns
        assert 'optimal_total_time' in df.columns

    @patch('src.offline_trainer.RandomForestPredictor')
    def test_create_model_rf(self, mock_rf, trainer):
        model = trainer.create_model()
        assert model == mock_rf.return_value
        assert trainer.model is not None

    def test_create_model_invalid(self):
        trainer = OfflineTrainer(model_type="invalid")
        with pytest.raises(ValueError):
            trainer.create_model()

    def test_train(self, trainer):
        # Mock model
        trainer.model = MagicMock()
        trainer.model.fit.return_value = {'score': 0.9}
        trainer.model.feature_importance.return_value = {'feature1': 0.5}
        
        X = pd.DataFrame({'f1': [1, 2], 'f2': [3, 4]})
        y = pd.Series([0, 1])
        
        results = trainer.train(X, y)
        
        assert results['score'] == 0.9
        trainer.model.fit.assert_called_once()

    @patch('src.offline_trainer.OfflineTrainer.prepare_data')
    @patch('src.offline_trainer.OfflineTrainer.train')
    @patch('src.offline_trainer.OfflineTrainer.save_model')
    @patch('src.offline_trainer.OfflineTrainer.create_model')
    def test_run_full_pipeline(self, mock_create, mock_save, mock_train, mock_prep, trainer, mock_workload_generator):
        # Setup mocks
        mock_prep.return_value = pd.DataFrame({
            'size': [100], 'compute_intensity': [0.5], 'memory_required': [10],
            'memory_per_size': [0.1], 'compute_to_memory': [0.05],
            'optimal_gpu_fraction': [0.5]
        })
        mock_train.return_value = {'score': 0.9}
        trainer.model = MagicMock()
        trainer.model.feature_importance.return_value = {}
        
        results = trainer.run_full_pipeline(mock_workload_generator, "dummy_path")
        
        assert results['training_results']['score'] == 0.9
        mock_prep.assert_called_once()
        mock_create.assert_called_once()
        mock_train.assert_called_once()
        mock_save.assert_called_once_with("dummy_path")


======= FILE: tests/test_rl_scheduler.py =======


import unittest
from src.rl_scheduler import QLearningScheduler
from src.workload_generator import Task

class TestRLScheduler(unittest.TestCase):
    def test_q_learning_update(self):
        """Test that Q-table updates after scheduling"""
        scheduler = QLearningScheduler(num_gpus=2, learning_rate=0.5)
        task = Task(0, 1000, 0.5, 100, 0, 1.0)
        
        # Initial state
        state = scheduler._get_state(task)
        initial_q = scheduler._get_q_value(state, 0) # Action 0 (CPU)
        self.assertEqual(initial_q, 0.0)
        
        # Force action 0 (CPU) by mocking choice? 
        # Easier: Just call schedule_task and check if ANY Q-value changed
        decision = scheduler.schedule_task(task)
        action = decision['action']
        
        # Check Q-value updated
        new_q = scheduler._get_q_value(state, action)
        self.assertNotEqual(new_q, 0.0, "Q-value should be updated")
        self.assertLess(new_q, 0.0, "Reward is negative cost, so Q should be negative")

    def test_epsilon_decay(self):
        """Test that epsilon decays over time"""
        scheduler = QLearningScheduler(epsilon=1.0, epsilon_decay=0.9)
        task = Task(0, 1000, 0.5, 100, 0, 1.0)
        
        scheduler.schedule_task(task)
        self.assertEqual(scheduler.epsilon, 0.9)
        
        scheduler.schedule_task(task)
        self.assertAlmostEqual(scheduler.epsilon, 0.81)

if __name__ == '__main__':
    unittest.main()


======= FILE: tests/test_database_integration.py =======

"""
Integration test for database persistence in simulation engine.
"""
import pytest
import anyio
from backend.services import SimulationDataService
from backend.core.database import init_db


@pytest.mark.anyio
async def test_training_data_save_and_retrieve():
    """Test saving and retrieving training data."""
    
    # Sample training data
    test_data = [
        {
            'size': 100.0,
            'compute_intensity': 0.7,
            'memory_required': 1000.0,
            'optimal_gpu_fraction': 0.8,
            'optimal_time': 1.5
        },
        {
            'size': 200.0,
            'compute_intensity': 0.3,
            'memory_required': 500.0,
            'optimal_gpu_fraction': 0.3,
            'optimal_time': 2.1
        }
    ]
    
    # Save data
    count = await SimulationDataService.save_training_data_batch(test_data)
    assert count == 2, "Should save 2 records"
    
    # Retrieve data  
    retrieved = await SimulationDataService.get_latest_training_data(limit=10)
    assert len(retrieved) >= 2, "Should retrieve at least 2 records"
    
    # Verify data integrity
    last_two = retrieved[:2]
    assert last_two[0]['size'] in [100.0, 200.0]
    assert last_two[0]['compute_intensity'] in[0.3, 0.7]


@pytest.mark.anyio
async def test_empty_batch_handling():
    """Test handling of empty batches."""
    count = await SimulationDataService.save_training_data_batch([])
    assert count == 0, "Should handle empty batch gracefully"


if __name__ == "__main__":
    # Run test
    asyncio.run(test_training_data_save_and_retrieve())
    asyncio.run(test_empty_batch_handling())
    print("âœ… All integration tests passed!")


======= FILE: tests/test_profiler.py =======


import pytest
from unittest.mock import MagicMock, patch
import numpy as np
from src.profiler import HardwareProfiler

class TestHardwareProfiler:
    
    @pytest.fixture
    def profiler(self):
        # Use CPU to avoid needing actual GPU for tests
        return HardwareProfiler(device_type="cpu")

    def test_initialization(self, profiler):
        assert profiler.device_type == "cpu"
        assert profiler.cpu_times == []
        assert profiler.gpu_times == []

    @patch('src.profiler.time.perf_counter')
    @patch('src.profiler.np.matmul')
    def test_benchmark_cpu(self, mock_matmul, mock_perf_counter, profiler):
        # Mock time to return 0 then 0.1 (diff 0.1)
        mock_perf_counter.side_effect = [0.0, 0.1, 0.0, 0.1, 0.0, 0.1]
        
        avg_time = profiler.benchmark_cpu(matrix_size=100, iterations=3)
        
        assert avg_time == pytest.approx(0.1)
        assert mock_matmul.call_count == 3

    def test_benchmark_gpu_no_gpu(self, profiler):
        # Initialized with CPU, so should return None
        result = profiler.benchmark_gpu(matrix_size=100)
        assert result is None

    @patch('src.profiler.HardwareProfiler.benchmark_cpu')
    @patch('src.profiler.HardwareProfiler.benchmark_gpu')
    def test_profile_range(self, mock_gpu, mock_cpu, profiler):
        mock_cpu.return_value = 1.0
        mock_gpu.return_value = 0.5
        
        sizes = [100, 200]
        model = profiler.profile_range(sizes)
        
        assert len(profiler.problem_sizes) == 2
        assert len(profiler.cpu_times) == 2
        assert len(profiler.gpu_times) == 2
        assert model['cpu_gpu_ratio'] == pytest.approx(2.0) # 1.0 / 0.5 = 2.0

    def test_get_performance_model_empty(self, profiler):
        model = profiler.get_performance_model()
        assert model['cpu_only'] is True

    def test_estimate_energy(self):
        # GPU: 50W * 2s = 100J
        assert HardwareProfiler.estimate_energy(2.0, is_gpu=True) == 100.0
        # CPU: 30W * 2s = 60J
        assert HardwareProfiler.estimate_energy(2.0, is_gpu=False) == 60.0


======= FILE: tests/test_rl_convergence.py =======

import unittest
import numpy as np
import os
import sys
import shutil
import tempfile

# Add project root to path
sys.path.append(os.getcwd())

from src.workload_generator import WorkloadGenerator
from src.dqn_scheduler import DQNScheduler
from src.simulator import VirtualMultiGPU

class TestRLConvergence(unittest.TestCase):
    
    def setUp(self):
        self.wg = WorkloadGenerator(seed=42)
        # Fast learning for tests
        self.scheduler = DQNScheduler(
            num_gpus=4,
            epsilon_start=1.0,
            epsilon_end=0.05,
            epsilon_decay=0.99, # Fast decay
            batch_size=32
        )
        self.simulator = VirtualMultiGPU(num_gpus=4)
        
    def test_convergence(self):
        """Test that the agent converges to near-zero regret within 600 tasks"""
        tasks = self.wg.generate_workload(num_tasks=600)
        regrets = []
        
        for task in tasks:
            # Oracle
            best_time = float('inf')
            for frac in np.linspace(0, 1, 11):
                res = self.simulator.simulate_task_execution(task, frac)
                if res['actual_time'] < best_time:
                    best_time = res['actual_time']
            
            # RL
            action_dict = self.scheduler.get_action(task)
            rl_res = self.simulator.simulate_task_execution(task, action_dict['gpu_fraction'])
            
            # Reward Calculation (Simple version)
            # Cost = Time (ignoring energy for simple test check, or assuming energy correlated)
            # To be precise, let's match the scheduler's internal logic 
            # But here we just check if it matches Oracle Time
            
            # Provide feedback
            metrics = {'time': rl_res['actual_time'], 'energy': 0.0} # Dummy energy, focus on time
            self.scheduler.observe(task, action_dict['action'], metrics)
            
            regret = rl_res['actual_time'] - best_time
            regrets.append(regret)
            
        # Check last 50 tasks
        final_regret = np.mean(regrets[-50:])
        print(f"\nFinal Average Regret (Last 50): {final_regret:.4f}")
        
        # It should be very close to 0
        self.assertLess(final_regret, 0.1, "RL Agent failed to converge (Regret > 0.1)")
        
    def test_replay_buffer_growth(self):
        """Test that replay buffer grows as we observe"""
        initial_size = len(self.scheduler.memory)
        tasks = self.wg.generate_workload(num_tasks=10)
        
        for task in tasks:
            action_dict = self.scheduler.get_action(task)
            self.scheduler.observe(task, action_dict['action'], {'time': 1.0, 'energy': 1.0})
            
        final_size = len(self.scheduler.memory)
        self.assertEqual(final_size, initial_size + 10)
        
    def test_epsilon_decay(self):
        """Test that epsilon decays after observations"""
        initial_eps = self.scheduler.epsilon
        task = self.wg.generate_workload(num_tasks=1)[0]
        
        action_dict = self.scheduler.get_action(task)
        self.scheduler.observe(task, action_dict['action'], {'time': 1.0, 'energy': 1.0})
        
        self.assertLess(self.scheduler.epsilon, initial_eps)

if __name__ == '__main__':
    unittest.main()


======= FILE: tests/test_pipeline.py =======


import pytest
from unittest.mock import MagicMock, patch
from src.pipeline import (
    run_profiling_phase,
    run_data_generation_phase,
    run_offline_training_phase,
    run_online_scheduling_phase,
    run_rl_training_phase,
    run_evaluation_phase
)

class TestPipeline:
    
    @pytest.fixture
    def mock_config(self):
        return {
            'hardware': {'device': 'cpu', 'num_virtual_gpus': 2},
            'profiling': {'matrix_sizes': [10], 'save_profiles': False, 'profile_output': 'dummy'},
            'workload_generation': {
                'seed': 42, 'num_tasks': 10, 'arrival_rate': 1.0,
                'task_size_range': [10, 20], 'compute_intensity_range': [0.1, 0.2], 'memory_range': [10, 20],
                'simulation_tasks': 5, 'rl_training_tasks': 5, 'evaluation_tasks': 5
            },
            'output': {'plots_dir': 'dummy_plots', 'results_dir': 'dummy_results', 'model_dir': 'dummy_models'},
            'ml_models': {'model_type': 'random_forest', 'random_forest': {'n_estimators': 2}}
        }

    @patch('src.pipeline.HardwareProfiler')
    def test_run_profiling_phase(self, mock_profiler_cls, mock_config):
        mock_profiler = mock_profiler_cls.return_value
        mock_profiler.profile_range.return_value = {}
        
        profiler, profile = run_profiling_phase(mock_config)
        
        assert profiler == mock_profiler
        assert profile == {}
        mock_profiler.profile_range.assert_called_once()

    @patch('src.pipeline.WorkloadGenerator')
    @patch('src.pipeline.plot_workload_characteristics')
    def test_run_data_generation_phase(self, mock_plot, mock_wg_cls, mock_config):
        mock_wg = mock_wg_cls.return_value
        mock_wg.generate_workload.return_value = []
        mock_wg.get_statistics.return_value = {}
        
        wg, tasks = run_data_generation_phase(mock_config)
        
        assert wg == mock_wg
        assert tasks == []
        mock_wg.generate_workload.assert_called_once()

    @patch('src.pipeline.OfflineTrainer')
    def test_run_offline_training_phase(self, mock_trainer_cls, mock_config):
        mock_trainer = mock_trainer_cls.return_value
        mock_trainer.run_full_pipeline.return_value = {'feature_importances': {}}
        mock_wg = MagicMock()
        
        trainer, results = run_offline_training_phase(mock_config, mock_wg)
        
        assert trainer == mock_trainer
        mock_trainer.run_full_pipeline.assert_called_once()

    @patch('src.pipeline.OnlineScheduler')
    @patch('src.pipeline.WorkloadGenerator')
    def test_run_online_scheduling_phase(self, mock_wg_cls, mock_scheduler_cls, mock_config):
        mock_trainer = MagicMock()
        mock_scheduler = mock_scheduler_cls.return_value
        mock_scheduler.process_queue.return_value = []
        mock_scheduler.get_utilization.return_value = {}
        
        scheduler, decisions = run_online_scheduling_phase(mock_config, mock_trainer)
        
        assert scheduler == mock_scheduler
        assert decisions == []
        mock_scheduler.process_queue.assert_called_once()

    @patch('src.pipeline.DQNScheduler')
    @patch('src.pipeline.WorkloadGenerator')
    def test_run_rl_training_phase(self, mock_wg_cls, mock_rl_cls, mock_config):
        mock_rl = mock_rl_cls.return_value
        mock_rl.epsilon = 0.1
        
        rl_scheduler = run_rl_training_phase(mock_config)
        
        assert rl_scheduler == mock_rl
        mock_rl.pretrain.assert_called_once()
        mock_rl.save_model.assert_called_once()

    @patch('src.pipeline.VirtualMultiGPU')
    @patch('src.pipeline.plot_comparison')
    def test_run_evaluation_phase(self, mock_plot, mock_sim_cls, mock_config):
        mock_sim = mock_sim_cls.return_value
        mock_sim.evaluate_baseline_schedulers.return_value = {}
        mock_sim.simulate_task_execution.return_value = {'actual_time': 1.0}
        
        mock_scheduler = MagicMock()
        mock_scheduler.model.predict.return_value = [0.5]
        
        mock_rl = MagicMock()
        mock_rl.schedule_task.return_value = {'estimated_time': 1.0}
        
        decisions = []
        
        baselines, results_df = run_evaluation_phase(mock_config, mock_scheduler, decisions, mock_rl)
        
        assert isinstance(baselines, dict)
        mock_sim.evaluate_baseline_schedulers.assert_called_once()


======= FILE: tests/test_online_scheduler.py =======


import pytest
from unittest.mock import MagicMock, patch
import pandas as pd
from src.online_scheduler import OnlineScheduler
from src.workload_generator import Task

class TestOnlineScheduler:
    
    @pytest.fixture
    def mock_model(self):
        model = MagicMock()
        model.predict.return_value = [0.5] # Always predict 0.5 split
        return model
        
    @pytest.fixture
    def scheduler(self, mock_model):
        return OnlineScheduler(model=mock_model, num_gpus=2)
        
    @pytest.fixture
    def sample_task(self):
        return Task(task_id=1, size=100, compute_intensity=0.5, memory_required=10, arrival_time=0.0, duration_estimate=1.0)

    def test_initialization(self, scheduler):
        assert scheduler.num_gpus == 2
        assert len(scheduler.gpu_states) == 2
        assert scheduler.task_queue == []

    def test_submit_task(self, scheduler, sample_task):
        scheduler.submit_task(sample_task)
        assert len(scheduler.task_queue) == 1
        assert scheduler.task_queue[0] == sample_task

    def test_predict_placement(self, scheduler, sample_task):
        placement = scheduler._predict_placement(sample_task)
        assert placement['gpu_fraction'] == 0.5
        assert placement['cpu_fraction'] == 0.5
        scheduler.model.predict.assert_called_once()

    @patch('src.online_scheduler.HardwareProfiler.estimate_energy')
    def test_find_best_gpu(self, mock_energy, scheduler, sample_task):
        mock_energy.return_value = 10.0
        
        # Make GPU 0 busy, GPU 1 free
        scheduler.gpu_states[0].current_load = 0.9
        scheduler.gpu_states[1].current_load = 0.0
        
        best_gpu = scheduler._find_best_gpu(sample_task, gpu_fraction=0.5)
        
        # Should pick GPU 1 because it has lower load -> lower time -> lower cost
        assert best_gpu == 1

    @patch('src.online_scheduler.OnlineScheduler._find_best_gpu')
    @patch('src.online_scheduler.OnlineScheduler._predict_placement')
    def test_schedule_task(self, mock_predict, mock_find, scheduler, sample_task):
        mock_predict.return_value = {'gpu_fraction': 0.5, 'cpu_fraction': 0.5}
        mock_find.return_value = 0
        
        decision = scheduler.schedule_task(sample_task)
        
        assert decision['task_id'] == 1
        assert decision['gpu_id'] == 0
        assert decision['gpu_fraction'] == 0.5
        
        # Check resource update
        assert scheduler.gpu_states[0].tasks_running == 1
        assert scheduler.gpu_states[0].available_memory < 8000

    def test_process_queue_no_deps(self, scheduler):
        t1 = Task(task_id=1, size=10, compute_intensity=0.1, memory_required=1, arrival_time=0.0, duration_estimate=1.0)
        t2 = Task(task_id=2, size=10, compute_intensity=0.1, memory_required=1, arrival_time=0.0, duration_estimate=1.0)
        
        scheduler.submit_task(t1)
        scheduler.submit_task(t2)
        
        decisions = scheduler.process_queue()
        
        assert len(decisions) == 2
        assert len(scheduler.task_queue) == 0

    def test_process_queue_with_deps(self, scheduler):
        t1 = Task(task_id=1, size=10, compute_intensity=0.1, memory_required=1, arrival_time=0.0, duration_estimate=1.0)
        t2 = Task(task_id=2, size=10, compute_intensity=0.1, memory_required=1, arrival_time=0.0, duration_estimate=1.0)
        t2.dependencies = [1] # t2 depends on t1
        
        scheduler.submit_task(t2) # Submit t2 first to test ordering logic
        scheduler.submit_task(t1)
        
        decisions = scheduler.process_queue()
        
        assert len(decisions) == 2
        # t1 should be scheduled before t2
        assert decisions[0]['task_id'] == 1
        assert decisions[1]['task_id'] == 2

    def test_get_utilization(self, scheduler):
        util = scheduler.get_utilization()
        assert 'gpu_0' in util
        assert 'gpu_1' in util
        assert 'average_utilization' in util

    def test_reset_state(self, scheduler, sample_task):
        scheduler.schedule_task(sample_task)
        assert len(scheduler.scheduled_tasks) == 1
        
        scheduler.reset_state()
        
        assert len(scheduler.scheduled_tasks) == 0
        assert scheduler.gpu_states[0].tasks_running == 0
        assert scheduler.gpu_states[0].current_load == 0.0


======= FILE: backend/__init__.py =======

"""Backend package initialization."""


======= FILE: backend/__version__.py =======

"""
Version information for the Hybrid ML Scheduler.
"""

__version__ = "2.0.0"  # Major.Phase.Patch

# Version History:
# 1.0.0 - Initial release
# 1.1.0 - Phase 1: Foundation & Infrastructure (Database, Config, Redis setup)
# 1.2.0 - Phase 2: Data Layer Migration (Repositories, Services, Database integration)
# 1.3.0 - Phase 3: API Architecture Refactoring (Modular routes, Health checks)
# 1.4.0 - Phase 4: Performance Optimization (Redis caching, Prometheus metrics)
# 1.5.0 - Phase 5: Observability & Monitoring (Structured logging, Tracing, Error tracking)
# 1.6.0 - Phase 6: Security & Polish (Rate limiting, Security headers, Input validation)


======= FILE: backend/middleware/security.py =======

"""
Security headers middleware.

Implements security best practices:
- HSTS (HTTP Strict Transport Security)
- X-Content-Type-Options
- X-Frame-Options
- X-XSS-Protection
- Content-Security-Policy
- Referrer-Policy
"""
from fastapi import Request
from starlette.middleware.base import BaseHTTPMiddleware


class SecurityHeadersMiddleware(BaseHTTPMiddleware):
    """
    Middleware to add security headers to all responses.
    
    Implements OWASP recommended security headers.
    """
    
    async def dispatch(self, request: Request, call_next):
        response = await call_next(request)
        
        # HTTP Strict Transport Security (HSTS)
        # Enforces HTTPS for 1 year
        response.headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains"
        
        # Prevent MIME type sniffing
        response.headers["X-Content-Type-Options"] = "nosniff"
        
        # Prevent clickjacking attacks
        response.headers["X-Frame-Options"] = "DENY"
        
        # XSS Protection (legacy browsers)
        response.headers["X-XSS-Protection"] = "1; mode=block"
        
        # Content Security Policy
        # Restrictive policy - adjust based on your needs
        response.headers["Content-Security-Policy"] = (
            "default-src 'self'; "
            "script-src 'self' 'unsafe-inline' 'unsafe-eval'; "
            "style-src 'self' 'unsafe-inline'; "
            "img-src 'self' data: https:; "
            "font-src 'self' data:; "
            "connect-src 'self' ws: wss:; "
            "frame-ancestors 'none';"
        )
        
        # Referrer Policy
        # Don't send referrer to external sites
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
        
        # Permissions Policy (formerly Feature-Policy)
        # Disable potentially dangerous browser features
        response.headers["Permissions-Policy"] = (
            "geolocation=(), "
            "microphone=(), "
            "camera=(), "
            "payment=()"
        )
        
        # Remove server header (don't advertise server technology)
        if "Server" in response.headers:
            del response.headers["Server"]
        
        return response


======= FILE: backend/middleware/__init__.py =======

"""Middleware package."""


======= FILE: backend/middleware/observability.py =======

"""
FastAPI middleware for observability.

Provides:
- Request correlation IDs
- Request/response logging
- Performance tracking
- Error tracking
"""
import time
from typing import Callable
from fastapi import Request, Response
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.types import ASGIApp
from loguru import logger

from backend.services.logging_service import set_correlation_id, clear_correlation_id, log_performance
from backend.services.performance_service import (
    http_requests_total,
    http_request_duration_seconds
)


class ObservabilityMiddleware(BaseHTTPMiddleware):
    """
    Middleware for request observability.
    
    Tracks:
    - Correlation IDs
    - Request/response logging
    - Performance metrics
    - Error tracking
    """
    
    async def dispatch(self, request: Request, call_next: Callable) -> Response:
        # Generate or extract correlation ID
        correlation_id = request.headers.get("X-Correlation-ID")
        correlation_id = set_correlation_id(correlation_id)
        
        # Start timer
        start_time = time.time()
        
        # Log incoming request
        logger.info(
            f"â†’ {request.method} {request.url.path}",
            method=request.method,
            path=request.url.path,
            client_ip=request.client.host if request.client else "unknown",
            user_agent=request.headers.get("user-agent", "unknown")
        )
        
        # Process request
        response = None
        status_code = 500
        
        try:
            response = await call_next(request)
            status_code = response.status_code
            
            # Add correlation ID to response headers
            response.headers["X-Correlation-ID"] = correlation_id
            
            return response
            
        except Exception as e:
            logger.error(
                f"Request failed: {type(e).__name__}: {str(e)}",
                error_type=type(e).__name__,
                error_message=str(e),
                path=request.url.path,
                method=request.method
            )
            raise
            
        finally:
            # Calculate duration
            duration = time.time() - start_time
            duration_ms = duration * 1000
            
            # Log response
            logger.info(
                f"â† {request.method} {request.url.path} {status_code} ({duration_ms:.2f}ms)",
                method=request.method,
                path=request.url.path,
                status_code=status_code,
                duration_ms=duration_ms
            )
            
            # Record metrics
            http_requests_total.labels(
                method=request.method,
                endpoint=request.url.path,
                status=str(status_code)
            ).inc()
            
            http_request_duration_seconds.labels(
                method=request.method,
                endpoint=request.url.path
            ).observe(duration)
            
            # Log slow requests
            if duration_ms > 1000:
                logger.warning(
                    f"Slow request: {request.method} {request.url.path} took {duration_ms:.2f}ms",
                    duration_ms=duration_ms,
                    threshold_ms=1000
                )
            
            # Clear correlation ID
            clear_correlation_id()


class ErrorTrackingMiddleware(BaseHTTPMiddleware):
    """
    Middleware for error tracking and reporting.
    
    Catches unhandled exceptions and logs them with full context.
    """
    
    async def dispatch(self, request: Request, call_next: Callable) -> Response:
        try:
            return await call_next(request)
        except Exception as e:
            # Log error with full context
            logger.exception(
                f"Unhandled exception: {type(e).__name__}",
                error_type=type(e).__name__,
                error_message=str(e),
                path=request.url.path,
                method=request.method,
                client_ip=request.client.host if request.client else "unknown"
            )
            
            # Re-raise to let FastAPI handle it
            raise


======= FILE: backend/middleware/rate_limit.py =======

"""
Rate limiting middleware for API protection.

Implements:
- Per-IP rate limiting
- Per-endpoint rate limiting
- Configurable limits
- Redis-backed (with in-memory fallback)
"""
import time
from typing import Dict, Tuple
from fastapi import Request, HTTPException, status
from starlette.middleware.base import BaseHTTPMiddleware
from loguru import logger

from backend.core.redis import redis_client


class RateLimiter:
    """
    Rate limiter with Redis backend and in-memory fallback.
    
    Implements token bucket algorithm.
    """
    
    def __init__(self):
        self._memory_store: Dict[str, Tuple[int, float]] = {}
    
    async def is_allowed(
        self, 
        key: str, 
        max_requests: int = 100, 
        window_seconds: int = 60
    ) -> Tuple[bool, int]:
        """
        Check if request is allowed under rate limit.
        
        Args:
            key: Unique identifier (e.g., IP address)
            max_requests: Maximum requests allowed in window
            window_seconds: Time window in seconds
            
        Returns:
            Tuple of (is_allowed, remaining_requests)
        """
        cache_key = f"rate_limit:{key}"
        current_time = time.time()
        
        # Try Redis first
        try:
            # Get current count
            count_str = await redis_client.get(cache_key)
            
            if count_str is None:
                # First request in window
                await redis_client.set(cache_key, "1", ttl=window_seconds)
                return True, max_requests - 1
            
            count = int(count_str)
            
            if count >= max_requests:
                # Rate limit exceeded
                return False, 0
            
            # Increment counter
            await redis_client.increment(cache_key)
            return True, max_requests - count - 1
            
        except Exception as e:
            logger.warning(f"Redis rate limiting failed, using memory: {e}")
        
        # Fallback to in-memory rate limiting
        if cache_key in self._memory_store:
            count, window_start = self._memory_store[cache_key]
            
            # Check if window expired
            if current_time - window_start >= window_seconds:
                # New window
                self._memory_store[cache_key] = (1, current_time)
                return True, max_requests - 1
            
            if count >= max_requests:
                return False, 0
            
            # Increment count
            self._memory_store[cache_key] = (count + 1, window_start)
            return True, max_requests - count - 1
        else:
            # First request
            self._memory_store[cache_key] = (1, current_time)
            return True, max_requests - 1
    
    def cleanup_memory(self):
        """Clean up expired entries from memory store."""
        current_time = time.time()
        expired_keys = [
            key for key, (_, start_time) in self._memory_store.items()
            if current_time - start_time > 3600  # 1 hour
        ]
        for key in expired_keys:
            del self._memory_store[key]


# Global rate limiter instance
rate_limiter = RateLimiter()


class RateLimitMiddleware(BaseHTTPMiddleware):
    """
    Middleware for rate limiting.
    
    Applies configurable rate limits per IP address.
    """
    
    def __init__(self, app, requests_per_minute: int = 100):
        super().__init__(app)
        self.requests_per_minute = requests_per_minute
        self.window_seconds = 60
    
    async def dispatch(self, request: Request, call_next):
        # Skip rate limiting for health checks
        if request.url.path.startswith("/health"):
            return await call_next(request)
        
        # Get client IP
        client_ip = request.client.host if request.client else "unknown"
        
        # Check rate limit
        is_allowed, remaining = await rate_limiter.is_allowed(
            key=client_ip,
            max_requests=self.requests_per_minute,
            window_seconds=self.window_seconds
        )
        
        if not is_allowed:
            logger.warning(
                f"Rate limit exceeded for {client_ip}",
                client_ip=client_ip,
                path=request.url.path
            )
            raise HTTPException(
                status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                detail="Rate limit exceeded. Please try again later.",
                headers={
                    "Retry-After": str(self.window_seconds),
                    "X-RateLimit-Limit": str(self.requests_per_minute),
                    "X-RateLimit-Remaining": "0"
                }
            )
        
        # Process request
        response = await call_next(request)
        
        # Add rate limit headers
        response.headers["X-RateLimit-Limit"] = str(self.requests_per_minute)
        response.headers["X-RateLimit-Remaining"] = str(remaining)
        
        return response


======= FILE: backend/core/config.py =======

"""
Handles all the config stuff using Pydantic.
Reads from .env file so we can change settings easily.
"""
from pydantic_settings import BaseSettings, SettingsConfigDict
from pydantic import Field, PostgresDsn, RedisDsn
from typing import Optional
from functools import lru_cache


class Settings(BaseSettings):
    """Settings for the app. Loads from env vars."""
    
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        case_sensitive=False,
        extra="allow"
    )
    
    # Application
    app_name: str = "Hybrid ML Scheduler"
    app_version: str = "2.0.0"
    debug: bool = False
    environment: str = Field(default="development", env="ENVIRONMENT")
    
    # Server
    host: str = "0.0.0.0"
    port: int = 8000
    reload: bool = False
    
    # Database
    postgres_user: str = Field(default="postgres", env="POSTGRES_USER")
    postgres_password: str = Field(default="Coloreal@1", env="POSTGRES_PASSWORD")
    postgres_host: str = Field(default="localhost", env="POSTGRES_HOST")
    postgres_port: int = Field(default=5432, env="POSTGRES_PORT")
    postgres_db: str = Field(default="hybrid_scheduler_db", env="POSTGRES_DB")
    
    @property
    def database_url(self) -> str:
        """Builds the Postgres connection string."""
        from urllib.parse import quote_plus
        password = quote_plus(self.postgres_password)
        return f"postgresql://{self.postgres_user}:{password}@{self.postgres_host}:{self.postgres_port}/{self.postgres_db}"
    
    @property
    def async_database_url(self) -> str:
        """Builds the async Postgres connection string."""
        from urllib.parse import quote_plus
        password = quote_plus(self.postgres_password)
        return f"postgresql+asyncpg://{self.postgres_user}:{password}@{self.postgres_host}:{self.postgres_port}/{self.postgres_db}"
    
    # Redis
    redis_host: str = Field(default="localhost", env="REDIS_HOST")
    redis_port: int = Field(default=6379, env="REDIS_PORT")
    redis_db: int = Field(default=0, env="REDIS_DB")
    redis_password: Optional[str] = Field(default=None, env="REDIS_PASSWORD")
    
    @property
    def redis_url(self) -> str:
        """Builds the Redis connection string."""
        if self.redis_password:
            return f"redis://:{self.redis_password}@{self.redis_host}:{self.redis_port}/{self.redis_db}"
        return f"redis://{self.redis_host}:{self.redis_port}/{self.redis_db}"
    
    # Security
    secret_key: str = Field(default="your-secret-key-change-in-production", env="SECRET_KEY")
    jwt_algorithm: str = "HS256"
    access_token_expire_minutes: int = 60 * 24  # 24 hours
    
    # CORS
    cors_origins: list[str] = ["http://localhost:5173", "http://localhost:3000"]
    
    # Cache
    cache_ttl: int = 300  # 5 minutes default TTL
    recent_metrics_cache_size: int = 100
    
    # Simulation
    num_gpus: int = 4
    retrain_interval: int = 50
    batch_size: int = 50
    max_history: int = 1000
    
    # WebSocket
    websocket_heartbeat_interval: int = 30  # seconds
    websocket_message_queue_size: int = 100
    
    # Monitoring
    enable_metrics: bool = True
    metrics_port: int = 9090
    
    # Rate Limiting
    rate_limit_per_minute: int = 60


@lru_cache()
def get_settings() -> Settings:
    """Returns the settings object. Caches it so we don't reload every time."""
    return Settings()


# Export singleton instance
settings = get_settings()


======= FILE: backend/core/database.py =======

"""
Sets up the database connection.
Uses SQLAlchemy with asyncpg so it's non-blocking.
"""
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlalchemy.orm import declarative_base
from sqlalchemy.pool import NullPool
from contextlib import asynccontextmanager
from typing import AsyncGenerator
from loguru import logger

from backend.core.config import settings

# Create async engine
engine = create_async_engine(
    settings.async_database_url,
    echo=settings.debug,
    future=True,
    pool_pre_ping=True,
    pool_size=10,
    max_overflow=20,
)

# Create async session factory
AsyncSessionLocal = async_sessionmaker(
    engine,
    class_=AsyncSession,
    expire_on_commit=False,
    autocommit=False,
    autoflush=False,
)

# Base class for models
Base = declarative_base()


async def init_db():
    """Creates all the tables if they don't exist."""
    try:
        async with engine.begin() as conn:
            # Import models to register them
            from backend.models.domain import Task, SchedulerResult, Metric
            
            # Create all tables
            await conn.run_sync(Base.metadata.create_all)
            logger.info("Database tables created successfully")
    except Exception as e:
        logger.error(f"Error initializing database: {e}")
        raise


async def get_db() -> AsyncGenerator[AsyncSession, None]:
    """
    Dependency for FastAPI to get a DB session.
    Usage: async def my_endpoint(db: AsyncSession = Depends(get_db)):
    """
    async with AsyncSessionLocal() as session:
        try:
            yield session
            await session.commit()
        except Exception:
            await session.rollback()
            raise
        finally:
            await session.close()


@asynccontextmanager
async def get_db_context():
    """Context manager for when we need a DB session outside of a request."""
    async with AsyncSessionLocal() as session:
        try:
            yield session
            await session.commit()
        except Exception:
            await session.rollback()
            raise


async def close_db():
    """Closes the DB connection pool."""
    await engine.dispose()
    logger.info("Database connections closed")


======= FILE: backend/core/__init__.py =======

"""Core package initialization."""


======= FILE: backend/core/redis.py =======

"""
Redis connection and caching utilities.
"""
from redis import asyncio as aioredis
from typing import Optional, Any
import json
from loguru import logger

from backend.core.config import settings


class RedisClient:
    """Async Redis client wrapper with caching utilities."""
    
    def __init__(self):
        self.redis: Optional[aioredis.Redis] = None
    
    async def connect(self):
        """Establish Redis connection."""
        try:
            self.redis = await aioredis.from_url(
                settings.redis_url,
                encoding="utf-8",
                decode_responses=True,
                max_connections=10,
            )
            # Test connection
            await self.redis.ping()
            logger.info("Redis connected successfully")
        except Exception as e:
            logger.warning(f"Redis connection failed: {e}. Caching disabled.")
            self.redis = None
    
    async def disconnect(self):
        """Close Redis connection."""
        if self.redis:
            await self.redis.close()
            logger.info("Redis disconnected")
    
    async def get(self, key: str) -> Optional[Any]:
        """Get value from cache."""
        if not self.redis:
            return None
        
        try:
            value = await self.redis.get(key)
            if value:
                return json.loads(value)
        except Exception as e:
            logger.error(f"Redis GET error: {e}")
        return None
    
    async def set(self, key: str, value: Any, ttl: int = None):
        """Set value in cache with optional TTL."""
        if not self.redis:
            return
        
        try:
            ttl = ttl or settings.cache_ttl
            await self.redis.setex(
                key,
                ttl,
                json.dumps(value, default=str)
            )
        except Exception as e:
            logger.error(f"Redis SET error: {e}")
    
    async def delete(self, key: str):
        """Delete key from cache."""
        if not self.redis:
            return
        
        try:
            await self.redis.delete(key)
        except Exception as e:
            logger.error(f"Redis DELETE error: {e}")
    
    async def exists(self, key: str) -> bool:
        """Check if key exists."""
        if not self.redis:
            return False
        
        try:
            return await self.redis.exists(key) > 0
        except Exception as e:
            logger.error(f"Redis EXISTS error: {e}")
            return False
    
    async def lpush(self, key: str, *values):
        """Push values to list (left)."""
        if not self.redis:
            return
        
        try:
            await self.redis.lpush(key, *[json.dumps(v, default=str) for v in values])
        except Exception as e:
            logger.error(f"Redis LPUSH error: {e}")
    
    async def lrange(self, key: str, start: int = 0, end: int = -1) -> list:
        """Get range from list."""
        if not self.redis:
            return []
        
        try:
            values = await self.redis.lrange(key, start, end)
            return [json.loads(v) for v in values]
        except Exception as e:
            logger.error(f"Redis LRANGE error: {e}")
            return []
    
    async def ltrim(self, key: str, start: int, end: int):
        """Trim list to specified range."""
        if not self.redis:
            return
        
        try:
            await self.redis.ltrim(key, start, end)
        except Exception as e:
            logger.error(f"Redis LTRIM error: {e}")
    
    async def increment(self, key: str, amount: int = 1) -> int:
        """Increment counter."""
        if not self.redis:
            return 0
        
        try:
            return await self.redis.incrby(key, amount)
        except Exception as e:
            logger.error(f"Redis INCR error: {e}")
            return 0


# Global Redis client instance
redis_client = RedisClient()


async def get_redis() -> RedisClient:
    """Dependency for getting Redis client."""
    return redis_client


======= FILE: backend/security/__init__.py =======

"""Security package."""


======= FILE: backend/security/validation.py =======

"""
Input validation and sanitization utilities.

Provides additional validation beyond Pydantic schemas.
"""
import re
from typing import Optional
from fastapi import HTTPException, status


class InputValidator:
    """Input validation utilities for security."""
    
    # Regex patterns for validation
    ALPHANUMERIC = re.compile(r'^[a-zA-Z0-9_-]+$')
    EMAIL = re.compile(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$')
    UUID = re.compile(r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$', re.IGNORECASE)
    
    # Dangerous patterns (SQL injection, XSS, etc.)
    SQL_INJECTION_PATTERNS = [
        r"(\bUNION\b.*\bSELECT\b)",
        r"(\bSELECT\b.*\bFROM\b)",
        r"(\bINSERT\b.*\bINTO\b)",
        r"(\bUPDATE\b.*\bSET\b)",
        r"(\bDELETE\b.*\bFROM\b)",
        r"(\bDROP\b.*\bTABLE\b)",
        r"(--|\#|\/\*|\*\/)",  # SQL comments
        r"(\bOR\b.*=.*)",
        r"(\bAND\b.*=.*)",
    ]
    
    XSS_PATTERNS = [
        r"<script[^>]*>.*?</script>",
        r"javascript:",
        r"onerror\s*=",
        r"onload\s*=",
        r"<iframe",
        r"<object",
        r"<embed",
    ]
    
    @staticmethod
    def validate_alphanumeric(value: str, field_name: str = "field") -> str:
        """Validate that string contains only alphanumeric characters."""
        if not InputValidator.ALPHANUMERIC.match(value):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"{field_name} must contain only alphanumeric characters, hyphens, and underscores"
            )
        return value
    
    @staticmethod
    def validate_length(
        value: str, 
        min_length: Optional[int] = None,
        max_length: Optional[int] = None,
        field_name: str = "field"
    ) -> str:
        """Validate string length."""
        if min_length and len(value) < min_length:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"{field_name} must be at least {min_length} characters"
            )
        
        if max_length and len(value) > max_length:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"{field_name} must not exceed {max_length} characters"
            )
        
        return value
    
    @staticmethod
    def sanitize_sql(value: str) -> str:
        """
        Check for SQL injection patterns.
        
        Note: This is defense in depth - primary protection is parameterized queries.
        """
        value_upper = value.upper()
        
        for pattern in InputValidator.SQL_INJECTION_PATTERNS:
            if re.search(pattern, value_upper, re.IGNORECASE):
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Invalid input detected. Please check your input and try again."
                )
        
        return value
    
    @staticmethod
    def sanitize_xss(value: str) -> str:
        """Check for XSS patterns."""
        for pattern in InputValidator.XSS_PATTERNS:
            if re.search(pattern, value, re.IGNORECASE):
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Invalid input detected. HTML/JavaScript not allowed."
                )
        
        return value
    
    @staticmethod
    def validate_range(
        value: int | float,
        min_value: Optional[int | float] = None,
        max_value: Optional[int | float] = None,
        field_name: str = "field"
    ) -> int | float:
        """Validate numeric range."""
        if min_value is not None and value < min_value:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"{field_name} must be at least {min_value}"
            )
        
        if max_value is not None and value > max_value:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"{field_name} must not exceed {max_value}"
            )
        
        return value
    
    @staticmethod
    def validate_enum(value: str, allowed_values: list, field_name: str = "field") -> str:
        """Validate that value is in allowed list."""
        if value not in allowed_values:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"{field_name} must be one of: {', '.join(allowed_values)}"
            )
        return value


# Convenience instance
validator = InputValidator()


======= FILE: backend/repositories/__init__.py =======

"""Repositories package initialization."""
from backend.repositories.task_repository import TaskRepository
from backend.repositories.scheduler_result_repository import SchedulerResultRepository
from backend.repositories.training_data_repository import TrainingDataRepository

__all__ = [
    'TaskRepository',
    'SchedulerResultRepository',
    'TrainingDataRepository',
]


======= FILE: backend/repositories/training_data_repository.py =======

"""
Repository pattern for database access.

Provides clean abstraction over database operations.
"""
from typing import List, Optional, Dict, Any
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, and_, or_, desc, func
from datetime import datetime, timedelta
from loguru import logger

from backend.models.domain import TrainingData
from backend.models.schemas import TrainingDataCreate, TrainingDataResponse


class TrainingDataRepository:
    """Repository for training data operations."""
    
    def __init__(self, session: AsyncSession):
        self.session = session
    
    async def create(self, data: TrainingDataCreate) -> TrainingData:
        """Create a new training data record."""
        db_data = TrainingData(**data.model_dump())
        self.session.add(db_data)
        await self.session.flush()
        await self.session.refresh(db_data)
        return db_data
    
    async def create_many(self, data_list: List[TrainingDataCreate]) -> List[TrainingData]:
        """Bulk create training data records (optimized)."""
        db_objects = [TrainingData(**data.model_dump()) for data in data_list]
        self.session.add_all(db_objects)
        await self.session.flush()
        return db_objects
    
    async def get_latest(self, limit: int = 1000) -> List[TrainingData]:
        """Get latest training data records."""
        stmt = (
            select(TrainingData)
            .order_by(desc(TrainingData.created_at))
            .limit(limit)
        )
        result = await self.session.execute(stmt)
        return list(result.scalars().all())
    
    async def get_by_date_range(
        self, 
        start_date: datetime, 
        end_date: datetime
    ) -> List[TrainingData]:
        """Get training data within date range."""
        stmt = (
            select(TrainingData)
            .where(
                and_(
                    TrainingData.created_at >= start_date,
                    TrainingData.created_at <= end_date
                )
            )
            .order_by(desc(TrainingData.created_at))
        )
        result = await self.session.execute(stmt)
        return list(result.scalars().all())
    
    async def count_all(self) -> int:
        """Count total training data records."""
        stmt = select(func.count()).select_from(TrainingData)
        result = await self.session.execute(stmt)
        return result.scalar() or 0
    
    async def delete_old_records(self, keep_last_n: int = 10000):
        """Delete old records, keeping only the latest N."""
        count = await self.count_all()
        if count <= keep_last_n:
            return 0
        
        # Get the cutoff timestamp
        stmt = (
            select(TrainingData.created_at)
            .order_by(desc(TrainingData.created_at))
            .offset(keep_last_n)
            .limit(1)
        )
        result = await self.session.execute(stmt)
        cutoff_time = result.scalar()
        
        if cutoff_time:
            stmt = TrainingData.__table__.delete().where(
                TrainingData.created_at < cutoff_time
            )
            result = await self.session.execute(stmt)
            await self.session.commit()
            return result.rowcount
        return 0


======= FILE: backend/repositories/task_repository.py =======

"""
Repository for tasks.
"""
from typing import List, Optional
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, desc

from backend.models.domain import Task as TaskModel
from backend.models.schemas import TaskCreate, TaskResponse


class TaskRepository:
    """Repository for task operations."""
    
    def __init__(self, session: AsyncSession):
        self.session = session
    
    async def create(self, task: TaskCreate) -> TaskModel:
        """Create a new task."""
        db_task = TaskModel(**task.model_dump())
        self.session.add(db_task)
        await self.session.flush()
        await self.session.refresh(db_task)
        return db_task
    
    async def create_many(self, tasks: List[TaskCreate]) -> List[TaskModel]:
        """Bulk create tasks."""
        db_objects = [TaskModel(**t.model_dump()) for t in tasks]
        self.session.add_all(db_objects)
        await self.session.flush()
        return db_objects
    
    async def get_by_task_id(self, task_id: int) -> Optional[TaskModel]:
        """Get task by task_id."""
        stmt = select(TaskModel).where(TaskModel.task_id == task_id)
        result = await self.session.execute(stmt)
        return result.scalar_one_or_none()
    
    async def get_latest(self, limit: int = 100) -> List[TaskModel]:
        """Get latest tasks."""
        stmt = (
            select(TaskModel)
            .order_by(desc(TaskModel.created_at))
            .limit(limit)
        )
        result = await self.session.execute(stmt)
        return list(result.scalars().all())


======= FILE: backend/repositories/scheduler_result_repository.py =======

"""
Repository for scheduler results.
"""
from typing import List, Optional, Dict
from sqlalchemy.ext.asyncio import AsyncSession  
from sqlalchemy import select, and_, desc, func
from datetime import datetime

from backend.models.domain import SchedulerResult, Task
from backend.models.schemas import SchedulerResultCreate, SchedulerResultResponse


class SchedulerResultRepository:
    """Repository for scheduler result operations."""
    
    def __init__(self, session: AsyncSession):
        self.session = session
    
    async def create(self, result: SchedulerResultCreate) -> SchedulerResult:
        """Create a new scheduler result."""
        db_result = SchedulerResult(**result.model_dump())
        self.session.add(db_result)
        await self.session.flush()
        await self.session.refresh(db_result)
        return db_result
    
    async def create_many(self, results: List[SchedulerResultCreate]) -> List[SchedulerResult]:
        """Bulk create scheduler results."""
        db_objects = [SchedulerResult(**r.model_dump()) for r in results]
        self.session.add_all(db_objects)
        await self.session.flush()
        return db_objects
    
    async def get_by_task_id(self, task_id: int) -> List[SchedulerResult]:
        """Get all results for a specific task."""
        stmt = (
            select(SchedulerResult)
            .where(SchedulerResult.task_id == task_id)
            .order_by(SchedulerResult.executed_at)
        )
        result = await self.session.execute(stmt)
        return list(result.scalars().all())
    
    async def get_by_scheduler(
        self, 
        scheduler_name: str, 
        limit: int = 100
    ) -> List[SchedulerResult]:
        """Get recent results for a scheduler."""
        stmt = (
            select(SchedulerResult)
            .where(SchedulerResult.scheduler_name == scheduler_name)
            .order_by(desc(SchedulerResult.executed_at))
            .limit(limit)
        )
        result = await self.session.execute(stmt)
        return list(result.scalars().all())
    
    async def get_latest(self, limit: int = 100) -> List[SchedulerResult]:
        """Get latest results across all schedulers."""
        stmt = (
            select(SchedulerResult)
            .order_by(desc(SchedulerResult.executed_at))
            .limit(limit)
        )
        result = await self.session.execute(stmt)
        return list(result.scalars().all())
    
    async def get_scheduler_stats(self, scheduler_name: str) -> Dict:
        """Get aggregate stats for a scheduler."""
        stmt = select(
            func.count(SchedulerResult.id).label('count'),
            func.avg(SchedulerResult.actual_time).label('avg_time'),
            func.min(SchedulerResult.actual_time).label('min_time'),
            func.max(SchedulerResult.actual_time).label('max_time'),
            func.sum(SchedulerResult.energy_consumption).label('total_energy'),
            func.sum(SchedulerResult.execution_cost).label('total_cost')
        ).where(SchedulerResult.scheduler_name == scheduler_name)
        
        result = await self.session.execute(stmt)
        row = result.one()
        
        return {
            'count': row.count or 0,
            'avg_time': float(row.avg_time or 0),
            'min_time': float(row.min_time or 0),
            'max_time': float(row.max_time or 0),
            'total_energy': float(row.total_energy or 0),
            'total_cost': float(row.total_cost or 0)
        }

        
    async def get_comparative_history(self, limit: int = 100) -> List[Dict]:
        """
        Get tasks with all their scheduler results for comparison.
        Returns a list of dicts, one per task, with nested results.
        """
        # We want to fetch the last N tasks and all their results
        # This is a bit complex in pure ORM async, so we'll do it in two steps or a join
        
        # 1. Get last N tasks
        stmt_tasks = (
            select(Task)
            .order_by(desc(Task.task_id))
            .limit(limit)
        )
        tasks_res = await self.session.execute(stmt_tasks)
        tasks = tasks_res.scalars().all()
        
        if not tasks:
            return []
            
        task_ids = [t.task_id for t in tasks]
        
        # 2. Get results for these tasks
        stmt_results = (
            select(SchedulerResult)
            .where(SchedulerResult.task_id.in_(task_ids))
        )
        results_res = await self.session.execute(stmt_results)
        results = results_res.scalars().all()
        
        # 3. Assemble
        # Map task_id -> list of results
        results_map = {}
        for r in results:
            if r.task_id not in results_map:
                results_map[r.task_id] = {}
            results_map[r.task_id][r.scheduler_name] = {
                'time': r.actual_time,
                'gpu_fraction': r.gpu_fraction,
                'cost': r.execution_cost
            }
            
        # Create final list
        comparative_data = []
        for t in tasks:
            comparative_data.append({
                'task_id': t.task_id,
                'size': t.size,
                'intensity': t.compute_intensity,
                'results': results_map.get(t.task_id, {})
            })
            
        return comparative_data


======= FILE: backend/models/domain.py =======

"""
Database models (SQLAlchemy).
"""
from sqlalchemy import Column, Integer, Float, String, DateTime, Text, Index, ForeignKey, Boolean
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.sql import func
from datetime import datetime

from backend.core.database import Base


class Task(Base):
    # Represents a task in the system.
    
    __tablename__ = "tasks"
    
    id = Column(Integer, primary_key=True, index=True)
    task_id = Column(Integer, unique=True, index=True, nullable=False)
    size = Column(Float, nullable=False)
    compute_intensity = Column(Float, nullable=False)
    memory_required = Column(Float, nullable=False)
    duration_estimate = Column(Float, nullable=False)
    arrival_time = Column(Float, nullable=False)
    dependencies = Column(JSONB, default=list)
    
    # Timestamps
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())
    
    # Indexes for common queries
    __table_args__ = (
        Index('ix_tasks_compute_intensity', 'compute_intensity'),
        Index('ix_tasks_size', 'size'),
        Index('ix_tasks_created_at', 'created_at'),
    )


class SchedulerResult(Base):
    # Stores the results of a scheduler running a task.
    
    __tablename__ = "scheduler_results"
    
    id = Column(Integer, primary_key=True, index=True)
    task_id = Column(Integer, ForeignKey('tasks.task_id'), nullable=False, index=True)
    scheduler_name = Column(String(50), nullable=False, index=True)
    
    # Resource allocation
    gpu_fraction = Column(Float, nullable=False)
    cpu_fraction = Column(Float, nullable=False)
    gpu_id = Column(Integer)
    
    # Performance metrics
    actual_time = Column(Float, nullable=False)
    energy_consumption = Column(Float)
    execution_cost = Column(Float)
    
    # Additional metadata
    extra_metadata = Column(JSONB, default=dict)
    
    # Timestamps
    executed_at = Column(DateTime(timezone=True), server_default=func.now(), index=True)
    
    __table_args__ = (
        Index('ix_scheduler_results_scheduler_time', 'scheduler_name', 'executed_at'),
        Index('ix_scheduler_results_task_scheduler', 'task_id', 'scheduler_name'),
    )


class Metric(Base):
    # Aggregated stats for schedulers (like total wins, avg time).
    
    __tablename__ = "metrics"
    
    id = Column(Integer, primary_key=True, index=True)
    scheduler_name = Column(String(50), nullable=False, index=True)
    
    # Aggregate stats
    total_tasks = Column(Integer, default=0)
    total_time = Column(Float, default=0.0)
    avg_time = Column(Float, default=0.0)
    min_time = Column(Float)
    max_time = Column(Float)
    
    # Energy and cost
    total_energy = Column(Float, default=0.0)
    total_cost = Column(Float, default=0.0)
    
    # Win/loss tracking
    wins = Column(Integer, default=0)
    losses = Column(Integer, default=0)
    
    # Time window
    window_start = Column(DateTime(timezone=True), nullable=False)
    window_end = Column(DateTime(timezone=True), nullable=False)
    
    # Timestamps
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())
    
    __table_args__ = (
        Index('ix_metrics_scheduler_window', 'scheduler_name', 'window_start', 'window_end'),
    )


class TrainingData(Base):
    # Data used for training the ML models.
    
    __tablename__ = "training_data"
    
    id = Column(Integer, primary_key=True, index=True)
    
    # Task features
    size = Column(Float, nullable=False)
    compute_intensity = Column(Float, nullable=False)
    memory_required = Column(Float, nullable=False)
    memory_per_size = Column(Float, nullable=False)
    compute_to_memory = Column(Float, nullable=False)
    
    # Optimal solution (from Oracle)
    optimal_gpu_fraction = Column(Float, nullable=False)
    optimal_time = Column(Float, nullable=False)
    
    # Timestamp
    created_at = Column(DateTime(timezone=True), server_default=func.now(), index=True)



class SimulationState(Base):
    # Keeps track of the simulation state (paused, running, etc).
    
    __tablename__ = "simulation_state"
    
    id = Column(Integer, primary_key=True)
    is_running = Column(Boolean, default=False)
    is_paused = Column(Boolean, default=False)
    tasks_processed = Column(Integer, default=0)
    last_retrain_at = Column(DateTime(timezone=True))
    
    # Configuration snapshot
    config = Column(JSONB, default=dict)
    
    # Timestamps
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())


======= FILE: backend/models/__init__.py =======

"""Models package initialization."""


======= FILE: backend/models/schemas.py =======

"""
Pydantic schemas for API request/response validation.
"""
from pydantic import BaseModel, Field, ConfigDict
from datetime import datetime
from typing import Optional, List, Dict, Any


# Task Schemas
class TaskBase(BaseModel):
    """Base task schema with common fields."""
    task_id: int
    size: float
    compute_intensity: float
    memory_required: float
    duration_estimate: float
    arrival_time: float
    dependencies: List[int] = Field(default_factory=list)


class TaskCreate(TaskBase):
    """Schema for creating a new task."""
    pass


class TaskResponse(TaskBase):
    """Schema for task response."""
    id: int
    created_at: datetime
    updated_at: Optional[datetime] = None
    
    model_config = ConfigDict(from_attributes=True)


# Scheduler Result Schemas
class SchedulerResultBase(BaseModel):
    """Base scheduler result schema."""
    task_id: int
    scheduler_name: str
    gpu_fraction: float
    cpu_fraction: float
    gpu_id: Optional[int] = None
    actual_time: float
    energy_consumption: Optional[float] = None
    execution_cost: Optional[float] = None
    extra_metadata: Dict[str, Any] = Field(default_factory=dict)


class SchedulerResultCreate(SchedulerResultBase):
    """Schema for creating scheduler result."""
    pass


class SchedulerResultResponse(SchedulerResultBase):
    """Schema for scheduler result response."""
    id: int
    executed_at: datetime
    
    model_config = ConfigDict(from_attributes=True)


# Metric Schemas
class MetricBase(BaseModel):
    """Base metric schema."""
    scheduler_name: str
    total_tasks: int = 0
    total_time: float = 0.0
    avg_time: float = 0.0
    min_time: Optional[float] = None
    max_time: Optional[float] = None
    total_energy: float = 0.0
    total_cost: float = 0.0
    wins: int = 0
    losses: int = 0
    window_start: datetime
    window_end: datetime


class MetricCreate(MetricBase):
    """Schema for creating metric."""
    pass


class MetricResponse(MetricBase):
    """Schema for metric response."""
    id: int
    created_at: datetime
    updated_at: Optional[datetime] = None
    
    model_config = ConfigDict(from_attributes=True)


# Training Data Schemas
class TrainingDataBase(BaseModel):
    """Base training data schema."""
    size: float
    compute_intensity: float
    memory_required: float
    memory_per_size: float
    compute_to_memory: float
    optimal_gpu_fraction: float
    optimal_time: float


class TrainingDataCreate(TrainingDataBase):
    """Schema for creating training data."""
    pass


class TrainingDataResponse(TrainingDataBase):
    """Schema for training data response."""
    id: int
    created_at: datetime
    
    model_config = ConfigDict(from_attributes=True)


# Simulation State Schemas
class SimulationStateBase(BaseModel):
    """Base simulation state schema."""
    is_running: bool = False
    is_paused: bool = False
    tasks_processed: int = 0
    last_retrain_at: Optional[datetime] = None
    config: Dict[str, Any] = Field(default_factory=dict)


class SimulationStateResponse(SimulationStateBase):
    """Schema for simulation state response."""
    id: int
    created_at: datetime
    updated_at: Optional[datetime] = None
    
    model_config = ConfigDict(from_attributes=True)


# WebSocket Message Schemas
class WebSocketMessage(BaseModel):
    """Schema for WebSocket messages."""
    type: str  # 'task', 'result', 'metric', 'state'
    data: Dict[str, Any]
    timestamp: datetime = Field(default_factory=datetime.utcnow)


# Dashboard Data Schemas
class SchedulerComparison(BaseModel):
    """Schema for scheduler comparison data."""
    scheduler_name: str
    avg_time: float
    total_tasks: int
    wins: int
    energy_consumption: float
    cost: float


class DashboardState(BaseModel):
    """Schema for complete dashboard state."""
    task: Optional[TaskResponse] = None
    results: List[SchedulerResultResponse]
    metrics: List[MetricResponse]
    schedulers: List[SchedulerComparison]
    simulation_state: SimulationStateResponse


# Health Check Schemas
class HealthCheck(BaseModel):
    """Schema for health check response."""
    status: str
    version: str
    database: str
    redis: str
    timestamp: datetime = Field(default_factory=datetime.utcnow)


======= FILE: backend/api/server.py =======


import sys
from pathlib import Path
from contextlib import asynccontextmanager

# Add project root to python path
ROOT_DIR = Path(__file__).parent.parent.parent.parent
sys.path.append(str(ROOT_DIR))

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from loguru import logger

from backend.core.config import settings
from backend.core.database import init_db, close_db
from backend.api.routes import health, metrics, simulation, websocket, observability
from src.simulation_engine import ContinuousSimulation

# Initialize Simulation Engine
simulation_instance = ContinuousSimulation(websocket.manager.broadcast)

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("Starting up Hybrid ML Scheduler Backend...")
    await init_db()
    
    # Inject simulation engine into routes
    simulation.set_simulation_engine(simulation_instance)
    
    yield
    
    # Shutdown
    logger.info("Shutting down...")
    if simulation_instance.is_running:
        simulation_instance.stop()
    await close_db()

app = FastAPI(
    title=settings.app_name,
    version=settings.app_version,
    lifespan=lifespan
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include Routers
app.include_router(health.router)
app.include_router(metrics.router)
app.include_router(simulation.router)
app.include_router(websocket.router)
app.include_router(observability.router)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("backend.api.server:app", host="0.0.0.0", port=8000, reload=True)


======= FILE: backend/api/__init__.py =======

"""API package."""


======= FILE: backend/api/routes/simulation.py =======

"""
Simulation control and data endpoints.
"""
from fastapi import APIRouter, HTTPException
from typing import Optional
from loguru import logger

router = APIRouter(prefix="/api", tags=["simulation"])

# This will be set by the main application
simulation_engine = None


def set_simulation_engine(engine):
    """Set the simulation engine instance."""
    global simulation_engine
    simulation_engine = engine


@router.post("/start")
async def start_simulation():
    """Start the simulation."""
    if simulation_engine is None:
        raise HTTPException(status_code=500, detail="Simulation engine not initialized")
    
    if simulation_engine.is_running:
        return {"status": "already_running", "message": "Simulation is already running"}
    
    # Start simulation in background
    import asyncio
    asyncio.create_task(simulation_engine.start())
    
    return {"status": "started", "message": "Simulation started successfully"}


@router.post("/stop")
async def stop_simulation():
    """Stop the simulation."""
    if simulation_engine is None:
        raise HTTPException(status_code=500, detail="Simulation engine not initialized")
    
    if not simulation_engine.is_running:
        return {"status": "not_running", "message": "Simulation is not running"}
    
    simulation_engine.stop()
    return {"status": "stopped", "message": "Simulation stopped successfully"}


@router.post("/pause")
async def pause_simulation():
    """Pause the simulation."""
    if simulation_engine is None:
        raise HTTPException(status_code=500, detail="Simulation engine not initialized")
    
    if not simulation_engine.is_running:
        raise HTTPException(status_code=400, detail="Simulation is not running")
    
    simulation_engine.pause()
    return {"status": "paused", "message": "Simulation paused"}


@router.post("/resume")
async def resume_simulation():
    """Resume a paused simulation."""
    if simulation_engine is None:
        raise HTTPException(status_code=500, detail="Simulation engine not initialized")
    
    if not simulation_engine.is_paused:
        raise HTTPException(status_code=400, detail="Simulation is not paused")
    
    simulation_engine.resume()
    return {"status": "resumed", "message": "Simulation resumed"}


@router.get("/status")
async def get_simulation_status():
    """Get current simulation status."""
    if simulation_engine is None:
        raise HTTPException(status_code=500, detail="Simulation engine not initialized")
    
    return {
        "is_running": simulation_engine.is_running,
        "is_paused": simulation_engine.is_paused,
        "tasks_processed": simulation_engine.tasks_processed,
        "metrics": simulation_engine.metrics
    }


@router.get("/full_history")
async def get_full_history():
    """
    Get full simulation history (training data).
    
    Returns:
        List[Dict]: List of historical task execution data.
    """
    if simulation_engine is None:
        raise HTTPException(status_code=500, detail="Simulation engine not initialized")
    
    try:
        from backend.services.simulation_data_service import SimulationDataService
        # Get latest 1000 records
        data = await SimulationDataService.get_latest_training_data(limit=1000)
        return data
    except Exception as e:
        logger.error(f"Failed to fetch full history: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/history/comparative")
async def get_comparative_history(limit: int = 100):
    """
    Get comparative history of tasks and scheduler results.
    
    Args:
        limit: Maximum number of tasks to return
        
    Returns:
        List[Dict]: List of tasks with their scheduler results.
    """
    if simulation_engine is None:
        raise HTTPException(status_code=500, detail="Simulation engine not initialized")
    
    try:
        from backend.services.simulation_data_service import SimulationDataService
        return await SimulationDataService.get_comparative_history(limit)
    except Exception as e:
        logger.error(f"Failed to fetch comparative history: {e}")
        raise HTTPException(status_code=500, detail=str(e))


======= FILE: backend/api/routes/metrics.py =======

"""
Metrics and monitoring endpoints.
"""
from fastapi import APIRouter, Response
from prometheus_client import generate_latest, CONTENT_TYPE_LATEST
from backend.services.cache_service import cache_service

router = APIRouter(prefix="/metrics", tags=["metrics"])


@router.get("/prometheus")
async def prometheus_metrics():
    """
    Prometheus metrics endpoint.
    
    Exposes all collected metrics in Prometheus format.
    """
    metrics = generate_latest()
    return Response(content=metrics, media_type=CONTENT_TYPE_LATEST)


@router.get("/cache")
async def cache_stats():
    """
    Get cache performance statistics.
    
    Returns hit/miss rates and cache size.
    """
    stats = cache_service.get_stats()
    return {
        "cache_stats": stats,
        "status": "operational"
    }


@router.post("/cache/reset")
async def reset_cache_stats():
    """Reset cache statistics (not the cache itself)."""
    cache_service.reset_stats()
    return {"status": "stats_reset"}


@router.post("/cache/invalidate/{namespace}")
async def invalidate_cache_namespace(namespace: str):
    """
    Invalidate all cache entries in a namespace.
    
    Args:
        namespace: Cache namespace to invalidate (e.g., 'metrics', 'leaderboard')
    """
    count = await cache_service.invalidate_namespace(namespace)
    return {
        "status": "invalidated",
        "namespace": namespace,
        "keys_deleted": count
    }


======= FILE: backend/api/routes/health.py =======

"""
Health check endpoints for monitoring system status.
"""
from fastapi import APIRouter, HTTPException
from datetime import datetime
from loguru import logger

from backend.core.database import engine
from backend.core.redis import redis_client
from backend.core.config import settings
from backend.__version__ import __version__

router = APIRouter(prefix="/health", tags=["health"])


@router.get("/")
async def health_check():
    """Basic health check endpoint."""
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "version": __version__
    }


@router.get("/ready")
async def readiness_check():
    """
    Readiness check - verifies all dependencies are available.
    
    Returns 200 if ready, 503 if not ready.
    """
    checks = {
        "database": "unknown",
        "redis": "unknown"
    }
    
    # Check database
    try:
        async with engine.connect() as conn:
            await conn.execute("SELECT 1")
        checks["database"] = "healthy"
    except Exception as e:
        checks["database"] = f"unhealthy: {str(e)}"
        logger.error(f"Database health check failed: {e}")
    
    # Check Redis
    try:
        await redis_client.connect()
        await redis_client.set("health_check", "ok", ttl=10)
        value = await redis_client.get("health_check")
        if value == "ok":
            checks["redis"] = "healthy"
        else:
            checks["redis"] = "unhealthy: test failed"
    except Exception as e:
        checks["redis"] = f"degraded: {str(e)}"
        logger.warning(f"Redis health check failed (degraded mode): {e}")
    
    # Determine overall status
    is_ready = checks["database"] == "healthy"
    status_code = 200 if is_ready else 503
    
    return {
        "status": "ready" if is_ready else "not_ready",
        "checks": checks,
        "timestamp": datetime.utcnow().isoformat()
    }


@router.get("/live")
async def liveness_check():
    """
    Liveness check - verifies the application is running.
    
    This is a simple endpoint that should always return 200
    unless the application is completely crashed.
    """
    return {
        "status": "alive",
        "timestamp": datetime.utcnow().isoformat()
    }


@router.get("/info")
async def system_info():
    """Get system information."""
    return {
        "app_name": "Hybrid ML Scheduler",
        "version": __version__,
        "environment": settings.environment,
        "debug": settings.debug,
        "database": {
            "host": settings.postgres_host,
            "port": settings.postgres_port,
            "database": settings.postgres_db,
        },
        "redis": {
            "host": settings.redis_host,
            "port": settings.redis_port,
            "enabled": True
        },
        "timestamp": datetime.utcnow().isoformat()
    }


======= FILE: backend/api/routes/__init__.py =======

"""API routes package."""
from backend.api.routes import health, websocket, simulation

__all__ = ['health', 'websocket', 'simulation']


======= FILE: backend/api/routes/observability.py =======

"""
Observability endpoints for logging and tracing.
"""
from fastapi import APIRouter, HTTPException, Query
from typing import Optional, List
from pathlib import Path
import json
from datetime import datetime, timedelta

from backend.services.logging_service import get_correlation_id

router = APIRouter(prefix="/observability", tags=["observability"])


@router.get("/correlation-id")
async def get_current_correlation_id():
    """Get the current request's correlation ID."""
    correlation_id = get_correlation_id()
    return {
        "correlation_id": correlation_id or "None",
        "message": "Correlation ID for current request"
    }


@router.get("/logs/tail")
async def tail_logs(
    lines: int = Query(100, ge=1, le=1000, description="Number of lines to return"),
    level: Optional[str] = Query(None, description="Filter by log level (INFO, ERROR, etc.)")
):
    """
    Get recent log entries.
    
    Args:
        lines: Number of log lines to return (max 1000)
        level: Optional log level filter
    """
    log_file = Path("logs/app.log")
    
    if not log_file.exists():
        return {
            "logs": [],
            "message": "Log file not found. Logs may not be initialized yet."
        }
    
    try:
        # Read last N lines
        with open(log_file, 'r') as f:
            all_lines = f.readlines()
        
        # Get last N lines
        recent_lines = all_lines[-lines:] if len(all_lines) > lines else all_lines
        
        # Parse JSON logs and filter by level if specified
        parsed_logs = []
        for line in recent_lines:
            try:
                log_entry = json.loads(line.strip())
                if level is None or log_entry.get('level', '').upper() == level.upper():
                    parsed_logs.append(log_entry)
            except json.JSONDecodeError:
                # Skip malformed lines
                continue
        
        return {
            "logs": parsed_logs,
            "count": len(parsed_logs),
            "total_lines": len(all_lines)
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to read logs: {str(e)}")


@router.get("/logs/errors")
async def get_error_logs(
    hours: int = Query(24, ge=1, le=168, description="Hours to look back"),
    limit: int = Query(100, ge=1, le=1000, description="Max errors to return")
):
    """
    Get recent error logs.
    
    Args:
        hours: Number of hours to look back (max 7 days)
        limit: Maximum number of errors to return
    """
    log_file = Path("logs/error.log")
    
    if not log_file.exists():
        return {
            "errors": [],
            "message": "Error log file not found."
        }
    
    try:
        cutoff_time = datetime.utcnow() - timedelta(hours=hours)
        
        with open(log_file, 'r') as f:
            all_lines = f.readlines()
        
        # Parse and filter by time
        errors = []
        for line in reversed(all_lines):  # Start from most recent
            if len(errors) >= limit:
                break
            
            try:
                log_entry = json.loads(line.strip())
                log_time = datetime.fromisoformat(log_entry.get('timestamp', ''))
                
                if log_time >= cutoff_time:
                    errors.append(log_entry)
            except (json.JSONDecodeError, ValueError):
                continue
        
        return {
            "errors": errors,
            "count": len(errors),
            "hours": hours
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to read error logs: {str(e)}")


@router.get("/logs/search")
async def search_logs(
    query: str = Query(..., min_length=1, description="Search term"),
    limit: int = Query(100, ge=1, le=1000, description="Max results")
):
    """
    Search logs for a specific term.
    
    Args:
        query: Search query string
        limit: Maximum results to return
    """
    log_file = Path("logs/app.log")
    
    if not log_file.exists():
        return {
            "results": [],
            "message": "Log file not found."
        }
    
    try:
        results = []
        
        with open(log_file, 'r') as f:
            for line in f:
                if len(results) >= limit:
                    break
                
                if query.lower() in line.lower():
                    try:
                        log_entry = json.loads(line.strip())
                        results.append(log_entry)
                    except json.JSONDecodeError:
                        continue
        
        return {
            "results": results,
            "count": len(results),
            "query": query
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to search logs: {str(e)}")


@router.get("/health-check")
async def observability_health():
    """Check observability system health."""
    log_dir = Path("logs")
    
    return {
        "status": "healthy",
        "logs_directory": str(log_dir.absolute()),
        "logs_directory_exists": log_dir.exists(),
        "app_log_exists": (log_dir / "app.log").exists(),
        "error_log_exists": (log_dir / "error.log").exists(),
        "correlation_id_support": True,
        "tracing_enabled": True
    }


======= FILE: backend/api/routes/websocket.py =======

"""
WebSocket route for real-time simulation updates.
"""
from fastapi import APIRouter, WebSocket, WebSocketDisconnect
from typing import List
from loguru import logger

router = APIRouter(tags=["websocket"])


class ConnectionManager:
    """Manages WebSocket connections."""
    
    def __init__(self):
        self.active_connections: List[WebSocket] = []
    
    async def connect(self, websocket: WebSocket):
        """Accept a new WebSocket connection."""
        await websocket.accept()
        self.active_connections.append(websocket)
        logger.info(f"WebSocket connected. Total connections: {len(self.active_connections)}")
    
    def disconnect(self, websocket: WebSocket):
        """Remove a WebSocket connection."""
        if websocket in self.active_connections:
            self.active_connections.remove(websocket)
        logger.info(f"WebSocket disconnected. Total connections: {len(self.active_connections)}")
    
    async def broadcast(self, message: dict):
        """Broadcast message to all connected clients."""
        disconnected = []
        for connection in self.active_connections:
            try:
                await connection.send_json(message)
            except Exception as e:
                logger.error(f"Error broadcasting to connection: {e}")
                disconnected.append(connection)
        
        # Clean up disconnected clients
        for conn in disconnected:
            self.disconnect(conn)
        
        if self.active_connections:
            logger.debug(f"Broadcasted to {len(self.active_connections)} connections")


# Global connection manager
manager = ConnectionManager()


@router.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """
    WebSocket endpoint for real-time simulation updates.
    
    Clients connect to this endpoint to receive live updates
    about simulation progress, scheduler performance, and metrics.
    """
    await manager.connect(websocket)
    try:
        while True:
            # Keep connection alive, receive any client messages
            data = await websocket.receive_text()
            # Could handle client commands here if needed
            logger.debug(f"Received from client: {data}")
    except WebSocketDisconnect:
        manager.disconnect(websocket)
    except Exception as e:
        logger.error(f"WebSocket error: {e}")
        manager.disconnect(websocket)


======= FILE: backend/services/logging_service.py =======

"""
Structured logging configuration with correlation IDs and JSON formatting.

Provides production-ready logging with:
- Correlation IDs for request tracking
- JSON structured format
- Log levels by environment
- Performance logging
"""
import sys
import uuid
import json
from contextvars import ContextVar
from datetime import datetime
from typing import Optional, Dict, Any
from loguru import logger
from pathlib import Path

# Context variable for correlation ID (thread-safe)
correlation_id_var: ContextVar[Optional[str]] = ContextVar('correlation_id', default=None)


class CorrelationIdFilter:
    """Add correlation ID to log records."""
    
    def __call__(self, record):
        correlation_id = correlation_id_var.get()
        record["extra"]["correlation_id"] = correlation_id or "N/A"
        return True


def patch_json(record):
    """Patch record with JSON serialization."""
    subset = {
        "timestamp": record["time"].isoformat(),
        "level": record["level"].name,
        "message": record["message"],
        "correlation_id": record["extra"].get("correlation_id", "N/A"),
        "module": record["name"],
        "function": record["function"],
        "line": record["line"],
    }
    
    # Add extra fields
    for key, value in record["extra"].items():
        if key not in ["correlation_id", "json"]:
            subset[key] = value
    
    # Add exception if present
    if record["exception"]:
        subset["exception"] = {
            "type": record["exception"].type.__name__,
            "value": str(record["exception"].value),
            "traceback": record["exception"].traceback
        }
    
    record["extra"]["json"] = json.dumps(subset)


def setup_logging(environment: str = "development", log_dir: str = "logs"):
    """
    Configure structured logging for the application.
    
    Args:
        environment: Environment name (development, staging, production)
        log_dir: Directory for log files
    """
    # Remove default logger
    logger.remove()
    
    # Configure patcher
    logger.configure(patcher=patch_json)
    
    # Create log directory
    Path(log_dir).mkdir(parents=True, exist_ok=True)
    
    # Console logging (human-readable in dev, JSON in prod)
    if environment == "development":
        # Human-readable format for development
        logger.add(
            sys.stderr,
            format="<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> | <level>{message}</level> | <yellow>corr_id={extra[correlation_id]}</yellow>",
            level="DEBUG",
            colorize=True,
            filter=CorrelationIdFilter()
        )
    else:
        # JSON format for production
        logger.add(
            sys.stderr,
            format="{extra[json]}",
            level="INFO",
            serialize=False,
            filter=CorrelationIdFilter()
        )
    
    # File logging - JSON format always
    logger.add(
        f"{log_dir}/app.log",
        rotation="100 MB",
        retention="30 days",
        compression="gz",
        format="{extra[json]}",
        level="INFO",
        serialize=False,
        filter=CorrelationIdFilter()
    )
    
    # Error log file
    logger.add(
        f"{log_dir}/error.log",
        rotation="50 MB",
        retention="60 days",
        compression="gz",
        format="{extra[json]}",
        level="ERROR",
        serialize=False,
        filter=CorrelationIdFilter()
    )
    
    logger.info(f"Logging configured for environment: {environment}")


def get_correlation_id() -> Optional[str]:
    """Get current correlation ID."""
    return correlation_id_var.get()


def set_correlation_id(correlation_id: Optional[str] = None) -> str:
    """
    Set correlation ID for current context.
    
    Args:
        correlation_id: Optional correlation ID, generates UUID if not provided
        
    Returns:
        The correlation ID that was set
    """
    if correlation_id is None:
        correlation_id = str(uuid.uuid4())
    correlation_id_var.set(correlation_id)
    return correlation_id


def clear_correlation_id():
    """Clear correlation ID from current context."""
    correlation_id_var.set(None)


def log_performance(operation: str, duration_ms: float, **extra):
    """
    Log performance metrics in structured format.
    
    Args:
        operation: Name of the operation
        duration_ms: Duration in milliseconds
        **extra: Additional context fields
    """
    logger.info(
        f"Performance: {operation} completed in {duration_ms:.2f}ms",
        operation=operation,
        duration_ms=duration_ms,
        **extra
    )


def log_error(error: Exception, context: Dict[str, Any] = None):
    """
    Log error with structured context.
    
    Args:
        error: The exception that occurred
        context: Additional context dictionary
    """
    logger.error(
        f"Error occurred: {type(error).__name__}: {str(error)}",
        error_type=type(error).__name__,
        error_message=str(error),
        context=context or {}
    )


# Convenience logger instance
structured_logger = logger


======= FILE: backend/services/performance_service.py =======

"""
Performance monitoring service using Prometheus metrics.
"""
from prometheus_client import Counter, Histogram, Gauge, Summary
from functools import wraps
import time
from typing import Callable
from loguru import logger


# Define metrics
http_requests_total = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

http_request_duration_seconds = Histogram(
    'http_request_duration_seconds',
    'HTTP request latency',
    ['method', 'endpoint']
)

database_query_duration_seconds = Histogram(
    'database_query_duration_seconds',
    'Database query latency',
    ['operation', 'table']
)

cache_operations_total = Counter(
    'cache_operations_total',
    'Total cache operations',
    ['operation', 'result']  # result: hit, miss, set, delete
)

simulation_tasks_processed = Counter(
    'simulation_tasks_processed_total',
    'Total simulation tasks processed'
)

simulation_tasks_in_flight = Gauge(
    'simulation_tasks_in_flight',
    'Current number of tasks being processed'
)

model_retraining_duration_seconds = Summary(
    'model_retraining_duration_seconds',
    'Model retraining duration'
)

websocket_connections = Gauge(
    'websocket_connections_active',
    'Number of active WebSocket connections'
)


def track_time(metric: Histogram, **labels):
    """
    Decorator to track execution time of async functions.
    
    Usage:
        @track_time(database_query_duration_seconds, operation='select', table='tasks')
        async def get_tasks():
            ...
    """
    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                return result
            finally:
                duration = time.time() - start_time
                metric.labels(**labels).observe(duration)
        return wrapper
    return decorator


def track_time_sync(metric: Histogram, **labels):
    """
    Decorator to track execution time of sync functions.
    """
    def decorator(func: Callable):
        @wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                return result
            finally:
                duration = time.time() - start_time
                metric.labels(**labels).observe(duration)
        return wrapper
    return decorator


class PerformanceMonitor:
    """Performance monitoring utilities."""
    
    @staticmethod
    def record_cache_hit(namespace: str):
        """Record a cache hit."""
        cache_operations_total.labels(operation='get', result='hit').inc()
        logger.debug(f"Cache hit: {namespace}")
    
    @staticmethod
    def record_cache_miss(namespace: str):
        """Record a cache miss."""
        cache_operations_total.labels(operation='get', result='miss').inc()
        logger.debug(f"Cache miss: {namespace}")
    
    @staticmethod
    def record_cache_set(namespace: str):
        """Record a cache set operation."""
        cache_operations_total.labels(operation='set', result='success').inc()
    
    @staticmethod
    def record_task_processed():
        """Record a simulation task processed."""
        simulation_tasks_processed.inc()
    
    @staticmethod
    def set_websocket_connections(count: int):
        """Update WebSocket connections gauge."""
        websocket_connections.set(count)
    
    @staticmethod
    async def track_db_query(operation: str, table: str, query_func: Callable):
        """Track database query performance."""
        start_time = time.time()
        try:
            result = await query_func()
            duration = time.time() - start_time
            database_query_duration_seconds.labels(
                operation=operation,
                table=table
            ).observe(duration)
            
            if duration > 1.0:
                logger.warning(f"Slow query: {operation} on {table} took {duration:.2f}s")
            
            return result
        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"Query failed after {duration:.2f}s: {e}")
            raise


# Global monitor instance
performance_monitor = PerformanceMonitor()


======= FILE: backend/services/__init__.py =======

"""Services package initialization."""
from backend.services.simulation_data_service import SimulationDataService

__all__ = ['SimulationDataService']


======= FILE: backend/services/simulation_data_service.py =======

"""
Handles saving simulation data to the database.
Wraps the repository stuff so the main code doesn't have to deal with it.
"""
import asyncio
from typing import List, Dict, Optional
from datetime import datetime
from loguru import logger

from backend.core.database import get_db_context
from backend.repositories import TrainingDataRepository, SchedulerResultRepository, TaskRepository
from backend.models.schemas import TrainingDataCreate, SchedulerResultCreate, TaskCreate
from backend.models.domain import Task
from src.workload_generator import Task as TaskObj
from sqlalchemy import select


class SimulationDataService:
    # Service for managing simulation data persistence.
    
    @staticmethod
    async def save_training_data_batch(data_list: List[Dict]) -> int:
        """
        Saves a batch of training data to the DB.
        Returns the number of records saved.
        """
        if not data_list:
            return 0
            
        try:
            async with get_db_context() as db:
                repo = TrainingDataRepository(db)
                
                # Convert dicts to Pydantic models
                training_data = [
                    TrainingDataCreate(
                        size=d['size'],
                        compute_intensity=d['compute_intensity'],
                        memory_required=d['memory_required'],
                        memory_per_size=d['memory_required'] / (d['size'] + 1),
                        compute_to_memory=d['compute_intensity'] / (d['memory_required'] + 1),
                        optimal_gpu_fraction=d['optimal_gpu_fraction'],
                        optimal_time=d['optimal_time']
                    )
                    for d in data_list
                ]
                
                # Bulk insert
                await repo.create_many(training_data)
                await db.commit()
                
                logger.debug(f"Saved {len(training_data)} training records to database")
                return len(training_data)
                
        except Exception as e:
            logger.error(f"Failed to save training data batch: {e}")
            return 0
    
    @staticmethod
    async def get_latest_training_data(limit: int = 1000) -> List[Dict]:
        """
        Grabs the latest training data so we can retrain the model.
        Uses a 30s cache so we don't hammer the DB.
        """
        from backend.services.cache_service import cache_service
        
        # Try cache first
        cache_key = f"training_data_latest_{limit}"
        cached_data = await cache_service.get("training_data", cache_key)
        if cached_data is not None:
            logger.debug(f"Returning cached training data ({len(cached_data)} records)")
            return cached_data
        
        # Cache miss - fetch from database
        try:
            async with get_db_context() as db:
                repo = TrainingDataRepository(db)
                records = await repo.get_latest(limit=limit)
                
                # Convert to list of dicts
                data = [
                    {
                        'size': r.size,
                        'compute_intensity': r.compute_intensity,
                        'memory_required': r.memory_required,
                        'memory_per_size': r.memory_per_size,
                        'compute_to_memory': r.compute_to_memory,
                        'optimal_gpu_fraction': r.optimal_gpu_fraction,
                        'optimal_time': r.optimal_time
                    }
                    for r in records
                ]
                
                # Cache for 30 seconds
                await cache_service.set("training_data", cache_key, data, ttl=30)
                logger.debug(f"Cached {len(data)} training records")
                
                return data
        except Exception as e:
            logger.error(f"Failed to get training data: {e}")
            return []
    
    @staticmethod
    async def save_scheduler_results(task: TaskObj, results: Dict[str, Dict]) -> int:
        """
        Saves the results from the scheduler for a specific task.
        Returns how many results got saved.
        """
        try:
            async with get_db_context() as db:
                result_repo = SchedulerResultRepository(db)
                task_repo = TaskRepository(db)
                
                # First, save the task if it doesn't exist
                task_data = TaskCreate(
                    task_id=task.task_id,
                    size=task.size,
                    compute_intensity=task.compute_intensity,
                    memory_required=task.memory_required,
                    duration_estimate=task.duration_estimate,
                    arrival_time=task.arrival_time,
                    dependencies=task.dependencies if hasattr(task, 'dependencies') else []
                )
                
                # Check if task exists, if not create it
                existing_task = await task_repo.get_by_task_id(task.task_id)
                if not existing_task:
                    await task_repo.create(task_data)
                
                # Convert results to Pydantic models
                scheduler_results = []
                for scheduler_name, result in results.items():
                    scheduler_results.append(
                        SchedulerResultCreate(
                            task_id=task.task_id,
                            scheduler_name=scheduler_name,
                            gpu_fraction=result['gpu_fraction'],
                            cpu_fraction=1.0 - result['gpu_fraction'],
                            gpu_id=result.get('gpu_id'),
                            actual_time=result['actual_time'],
                            energy_consumption=result.get('energy'),
                            execution_cost=result.get('cost'),
                            extra_metadata={}
                        )
                    )
                
                # Bulk insert results
                await result_repo.create_many(scheduler_results)
                await db.commit()
                
                logger.debug(f"Saved {len(scheduler_results)} scheduler results for task {task.task_id}")
                return len(scheduler_results)
                
        except Exception as e:
            logger.error(f"Failed to save scheduler results: {e}")
            return 0
    
    @staticmethod
    async def save_scheduler_results_batch(batch_data: List[tuple]) -> int:
        """
        Saves a batch of (task, results) tuples to the DB.
        """
        if not batch_data:
            return 0
            
        try:
            async with get_db_context() as db:
                result_repo = SchedulerResultRepository(db)
                task_repo = TaskRepository(db)
                
                tasks_to_create = []
                results_to_create = []
                
                # Process batch
                for task, results in batch_data:
                    # Prepare Task
                    # Check if we already added this task to our local list to avoid dupes in batch
                    if not any(t.task_id == task.task_id for t in tasks_to_create):
                        tasks_to_create.append(TaskCreate(
                            task_id=task.task_id,
                            size=task.size,
                            compute_intensity=task.compute_intensity,
                            memory_required=task.memory_required,
                            duration_estimate=task.duration_estimate,
                            arrival_time=task.arrival_time,
                            dependencies=task.dependencies if hasattr(task, 'dependencies') else []
                        ))
                    
                    # Prepare Results
                    for scheduler_name, result in results.items():
                        results_to_create.append(SchedulerResultCreate(
                            task_id=task.task_id,
                            scheduler_name=scheduler_name,
                            gpu_fraction=result['gpu_fraction'],
                            cpu_fraction=1.0 - result['gpu_fraction'],
                            gpu_id=result.get('gpu_id'),
                            actual_time=result['actual_time'],
                            energy_consumption=result.get('energy'),
                            execution_cost=result.get('cost'),
                            extra_metadata={}
                        ))
                
                # Bulk insert Tasks
                # First check which tasks already exist to avoid UniqueViolation
                if tasks_to_create:
                    existing_ids_res = await db.execute(
                        select(Task.task_id).where(Task.task_id.in_([t.task_id for t in tasks_to_create]))
                    )
                    existing_ids = set(existing_ids_res.scalars().all())
                    
                    # Filter out existing tasks
                    new_tasks = [t for t in tasks_to_create if t.task_id not in existing_ids]
                    
                    if new_tasks:
                        await task_repo.create_many(new_tasks)
                
                # Bulk insert Results
                if results_to_create:
                    await result_repo.create_many(results_to_create)
                
                await db.commit()
                
                logger.debug(f"Saved batch: {len(tasks_to_create)} tasks, {len(results_to_create)} results")
                return len(results_to_create)
                
        except Exception as e:
            logger.error(f"Failed to save scheduler results batch: {e}")
            return 0
        """
        Gets the stats for a scheduler (like avg time, energy, etc).
        Caches it for 10s.
        """
        from backend.services.cache_service import cache_service
        
        # Try cache first
        cache_key = f"scheduler_stats_{scheduler_name}"
        cached_stats = await cache_service.get("scheduler_stats", cache_key)
        if cached_stats is not None:
            logger.debug(f"Returning cached stats for {scheduler_name}")
            return cached_stats
        
        # Cache miss - fetch from database
        try:
            async with get_db_context() as db:
                repo = SchedulerResultRepository(db)
                stats = await repo.get_scheduler_stats(scheduler_name)
                
                # Cache for 10 seconds
                await cache_service.set("scheduler_stats", cache_key, stats, ttl=10)
                logger.debug(f"Cached stats for {scheduler_name}")
                
                return stats
        except Exception as e:
            logger.error(f"Failed to get scheduler stats: {e}")
            return {}
    
    @staticmethod
    async def cleanup_old_data(keep_last_n: int = 10000):
        """
        Deletes old training data so the DB doesn't get too huge.
        """
        try:
            async with get_db_context() as db:
                repo = TrainingDataRepository(db)
                deleted = await repo.delete_old_records(keep_last_n)
                if deleted > 0:
                    logger.info(f"Cleaned up {deleted} old training records")
        except Exception as e:
            logger.error(f"Failed to cleanup old data: {e}")

    @staticmethod
    async def get_comparative_history(limit: int = 100) -> List[Dict]:
        """
        Gets the comparative history of tasks and scheduler results.
        """
        try:
            async with get_db_context() as db:
                repo = SchedulerResultRepository(db)
                return await repo.get_comparative_history(limit)
        except Exception as e:
            logger.error(f"Failed to get comparative history: {e}")
            return []


======= FILE: backend/services/tracing_service.py =======

"""
Distributed tracing with OpenTelemetry.

Provides request tracing across services and database calls.
"""
from typing import Callable, Optional
from functools import wraps
import time
from loguru import logger

# We'll use a lightweight tracing approach since OpenTelemetry full setup
# requires additional infrastructure. This provides the core tracing functionality.


class Span:
    """
    Lightweight span implementation for distributed tracing.
    
    In production, this would be replaced with OpenTelemetry spans.
    """
    
    def __init__(self, name: str, parent_id: Optional[str] = None):
        self.name = name
        self.span_id = str(time.time_ns())
        self.parent_id = parent_id
        self.start_time = time.time()
        self.end_time = None
        self.attributes = {}
        self.events = []
        self.status = "ok"
    
    def set_attribute(self, key: str, value):
        """Set a span attribute."""
        self.attributes[key] = value
    
    def add_event(self, name: str, attributes: dict = None):
        """Add an event to the span."""
        self.events.append({
            "name": name,
            "timestamp": time.time(),
            "attributes": attributes or {}
        })
    
    def set_status(self, status: str):
        """Set span status (ok, error)."""
        self.status = status
    
    def end(self):
        """End the span."""
        self.end_time = time.time()
        duration_ms = (self.end_time - self.start_time) * 1000
        
        logger.debug(
            f"Span completed: {self.name}",
            span_id=self.span_id,
            parent_id=self.parent_id,
            duration_ms=duration_ms,
            status=self.status,
            attributes=self.attributes
        )
        
        return duration_ms


class Tracer:
    """
    Lightweight tracer for distributed tracing.
    
    Provides basic tracing functionality with span management.
    """
    
    def __init__(self, name: str):
        self.name = name
        self.current_span = None
    
    def start_span(self, name: str, parent_id: Optional[str] = None) -> Span:
        """Start a new span."""
        span = Span(name, parent_id)
        self.current_span = span
        logger.debug(f"Started span: {name}", span_id=span.span_id)
        return span
    
    def end_span(self, span: Span):
        """End a span."""
        duration = span.end()
        if span == self.current_span:
            self.current_span = span.parent_id
        return duration


# Global tracer instance
tracer = Tracer("hybrid-scheduler")


def trace_function(span_name: Optional[str] = None, **span_attributes):
    """
    Decorator to trace function execution.
    
    Usage:
        @trace_function("process_task", task_type="ml")
        async def process_task():
            ...
    """
    def decorator(func: Callable):
        nonlocal span_name
        if span_name is None:
            span_name = f"{func.__module__}.{func.__name__}"
        
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            span = tracer.start_span(span_name)
            
            # Add attributes
            for key, value in span_attributes.items():
                span.set_attribute(key, value)
            
            try:
                result = await func(*args, **kwargs)
                span.set_status("ok")
                return result
            except Exception as e:
                span.set_status("error")
                span.set_attribute("error.type", type(e).__name__)
                span.set_attribute("error.message", str(e))
                raise
            finally:
                tracer.end_span(span)
        
        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            span = tracer.start_span(span_name)
            
            # Add attributes
            for key, value in span_attributes.items():
                span.set_attribute(key, value)
            
            try:
                result = func(*args, **kwargs)
                span.set_status("ok")
                return result
            except Exception as e:
                span.set_status("error")
                span.set_attribute("error.type", type(e).__name__)
                span.set_attribute("error.message", str(e))
                raise
            finally:
                tracer.end_span(span)
        
        # Return appropriate wrapper based on function type
        import inspect
        if inspect.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper
    
    return decorator


class TraceContext:
    """
    Context manager for manual span creation.
    
    Usage:
        async with TraceContext("operation_name") as span:
            span.set_attribute("key", "value")
            await do_work()
    """
    
    def __init__(self, span_name: str):
        self.span_name = span_name
        self.span = None
    
    def __enter__(self):
        self.span = tracer.start_span(self.span_name)
        return self.span
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if exc_type is not None:
            self.span.set_status("error")
            self.span.set_attribute("error.type", exc_type.__name__)
            self.span.set_attribute("error.message", str(exc_val))
        else:
            self.span.set_status("ok")
        
        tracer.end_span(self.span)
        return False
    
    async def __aenter__(self):
        self.span = tracer.start_span(self.span_name)
        return self.span
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if exc_type is not None:
            self.span.set_status("error")
            self.span.set_attribute("error.type", exc_type.__name__)
            self.span.set_attribute("error.message", str(exc_val))
        else:
            self.span.set_status("ok")
        
        tracer.end_span(self.span)
        return False


======= FILE: backend/services/cache_service.py =======

"""
Caching service for performance optimization.

Provides Redis-backed caching with automatic fallback to in-memory cache.
"""
from typing import Optional, Dict, Any, List
from datetime import datetime, timedelta
import json
from loguru import logger

from backend.core.redis import redis_client


class CacheService:
    """
    Centralized caching service with Redis backend and in-memory fallback.
    
    Features:
    - Automatic serialization/deserialization
    - TTL support
    - In-memory fallback when Redis unavailable
    - Namespace support for organized caching
    """
    
    def __init__(self):
        self._memory_cache: Dict[str, tuple[Any, Optional[datetime]]] = {}
        self._cache_hits = 0
        self._cache_misses = 0
    
    def _make_key(self, namespace: str, key: str) -> str:
        """Create namespaced cache key."""
        return f"{namespace}:{key}"
    
    async def get(self, namespace: str, key: str) -> Optional[Any]:
        """
        Get value from cache.
        
        Args:
            namespace: Cache namespace (e.g., 'metrics', 'leaderboard')
            key: Cache key
            
        Returns:
            Cached value or None if not found/expired
        """
        cache_key = self._make_key(namespace, key)
        
        # Try Redis first
        try:
            value = await redis_client.get(cache_key)
            if value is not None:
                self._cache_hits += 1
                logger.debug(f"Cache HIT (Redis): {cache_key}")
                return json.loads(value) if isinstance(value, str) else value
        except Exception as e:
            logger.warning(f"Redis get failed: {e}")
        
        # Fallback to memory cache
        if cache_key in self._memory_cache:
            value, expires_at = self._memory_cache[cache_key]
            if expires_at is None or expires_at >= datetime.utcnow():
                self._cache_hits += 1
                logger.debug(f"Cache HIT (Memory): {cache_key}")
                return value
            else:
                # Expired, remove it
                del self._memory_cache[cache_key]
        
        self._cache_misses += 1
        logger.debug(f"Cache MISS: {cache_key}")
        return None
    
    async def set(
        self, 
        namespace: str, 
        key: str, 
        value: Any, 
        ttl: Optional[int] = None
    ) -> bool:
        """
        Set value in cache.
        
        Args:
            namespace: Cache namespace
            key: Cache key
            value: Value to cache (must be JSON serializable)
            ttl: Time to live in seconds (None = no expiry)
            
        Returns:
            True if successful
        """
        cache_key = self._make_key(namespace, key)
        
        # Try Redis first
        try:
            serialized = json.dumps(value)
            await redis_client.set(cache_key, serialized, ttl=ttl)
            logger.debug(f"Cache SET (Redis): {cache_key}, ttl={ttl}")
            return True
        except Exception as e:
            logger.warning(f"Redis set failed: {e}, using memory cache")
        
        # Fallback to memory cache
        expires_at = None
        if ttl is not None:
            expires_at = datetime.utcnow() + timedelta(seconds=ttl)
        
        self._memory_cache[cache_key] = (value, expires_at)
        logger.debug(f"Cache SET (Memory): {cache_key}, ttl={ttl}")
        return True
    
    async def delete(self, namespace: str, key: str) -> bool:
        """Delete value from cache."""
        cache_key = self._make_key(namespace, key)
        
        # Delete from Redis
        try:
            await redis_client.delete(cache_key)
        except Exception as e:
            logger.warning(f"Redis delete failed: {e}")
        
        # Delete from memory
        if cache_key in self._memory_cache:
            del self._memory_cache[cache_key]
        
        logger.debug(f"Cache DELETE: {cache_key}")
        return True
    
    async def invalidate_namespace(self, namespace: str) -> int:
        """
        Invalidate all keys in a namespace.
        
        Args:
            namespace: Namespace to invalidate
            
        Returns:
            Number of keys deleted
        """
        # For memory cache, filter and delete
        keys_to_delete = [
            k for k in self._memory_cache.keys() 
            if k.startswith(f"{namespace}:")
        ]
        
        for key in keys_to_delete:
            del self._memory_cache[key]
        
        logger.info(f"Invalidated {len(keys_to_delete)} keys in namespace '{namespace}'")
        return len(keys_to_delete)
    
    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics."""
        total_requests = self._cache_hits + self._cache_misses
        hit_rate = (
            (self._cache_hits / total_requests * 100) 
            if total_requests > 0 
            else 0
        )
        
        return {
            "hits": self._cache_hits,
            "misses": self._cache_misses,
            "total_requests": total_requests,
            "hit_rate_percent": round(hit_rate, 2),
            "memory_cache_size": len(self._memory_cache)
        }
    
    def reset_stats(self):
        """Reset cache statistics."""
        self._cache_hits = 0
        self._cache_misses = 0


# Global cache service instance
cache_service = CacheService()


======= FILE: docs/wiki/02_code_reference.md =======

# 02. Code Reference

This document provides a walk-through of the main classes, functions, and data structures.

## Core Simulation Classes

### `VirtualMultiGPU` (`src/simulator.py`)
This is the "physics engine" representing a high-end compute node.
* **Host CPU**: Executing tasks with a base speed of `1.0x`. Memory boundaries and OS overhead are simulated here.
* **Virtual GPUs (4 Units)**: Acting as physical accelerators. Tasks can experience a speedup of up to `4x`. 
* **Interconnect simulation**: Simulates PCIe Gen 3 (16GB/s). Task latency incorporates transfer time (`Size / Bandwidth`).

### `ContinuousSimulation` (`src/simulation_engine.py`)
The orchestrator managing the scheduler loop.
* **`start()`**: Async method driving the task generation and broadcast loop.
* **`_run_all_schedulers(task)`**: Distributes the given `task` to RR, Random, Greedy, Hybrid ML, RL, and Oracle simulators, collecting actual time simulated.
* **`_calculate_metrics(result)`**: Transforms execution time and GPU fraction into Joules and Dollars based on modeled GPU (50W, $0.0001/s) and CPU (30W, $0.00002/s) models.

### `WorkloadGenerator` (`src/workload_generator.py`)
Generates tasks.
* **`Task` Data Class**: Stores properties for `task_id`, `size`, `compute_intensity`, and `memory_required`.

## AI/ML Components

### `OfflineTrainer` (`src/offline_trainer.py` / `src/ml_models.py`)
Trains the supervised Hybrid ML model (Random Forest / XGBoost) on historical data generated by the WorkloadGenerator paired with Oracle brute-force best results. Uses `StandardScaler` to normalize input task feature vectors `[Size, Intensity, Memory]`.

### `DQNScheduler` (`src/dqn_scheduler.py`)
The PyTorch-based Reinforcement Learning Agent.
* **`get_action(task)`**: Implements Epsilon-Greedy logic. Flips a coin to either explore (random action) or exploit (forward pass through `DQN` class).
* **`observe(task, action, reward_metrics)`**: Allows the agent to populate its Experience Replay memory buffer. Calculates a scalar reward based on cost/energy parameters.
* **`DQN` Neural Net**: 3 input neurons, 2 hidden layers (256 neurons each), and 5 output neurons representing Q-values for CPU and GPUs 0-3.

## Pydantic Schemas (`backend/models/schemas.py`)
* **`TaskCreate`**: Validates `task_id` (int), `size` (float), `compute_intensity` (float, 0.0-1.0), and `memory` (float).
* **`SchedulerResultCreate`**: Validates simulated outcomes (`gpu_fraction` float, `actual_time` float).


======= FILE: docs/wiki/05_data_workflow.md =======

# 05. Data Workflow

This document illustrates how a single Task moves through the Hybrid ML Scheduler ecosystem.

1. **Task Ingestion**
   * The `WorkloadGenerator` generates a unique `Task` object.
   * `Task` is parameterized by `size` (MB), `compute_intensity` (FLOPs/Bytes), and `memory_required`.

2. **Parallel Scheduling**
   * The `ContinuousSimulation` loops the `Task` through all 6 Virtual Schedulers (Round Robin, Random, Greedy, Hybrid ML, RL Agent, Oracle).
   * **Hybrid ML Inference**: The Task is passed to `sklearn.RandomForestRegressor.predict([Size, Intensity, Memory])` to fetch an optimal GPU fraction `[0.0, 1.0]`.
   * **RL Agent Inference**: The Task state tensor `[Size, Intensity, Memory]` passes through a PyTorch DQN. An Epsilon-Greedy calculation picks a discrete action (GPU0, GPU1...) and outputs an action fraction.

3. **Physics Verification & Execution**
   * The `VirtualMultiGPU.simulate_task_execution()` determines the real simulated time execution latency.
   * Total actual Time latency = `Task Size / (Transfer Bandwidth)` + simulated mathematical computation bounds.

4. **Metrics Generation**
   * Energy ($J$) = Time $\times$ (Power W).
   * Cost ($\$$) = Energy $/ (3.6 \times 10^6) \times \$0.15 $.

5. **Persistence**
   * The Oracle brute forces the actual best outcome. 
   * The Task parameters, optimal fraction, and metric logs are persisted into `scheduler_results` PostgreSQL table (`data/long_term_history.csv` async backup layer).

6. **Feedback & Visualization**
   * The RL Agent `observe()`s the Cost scalar metric for experience replay back-propagation.
   * The FastAPI WebSocket broadcasts the updated aggregated metric leaderboard state to the React dashboard.


======= FILE: docs/wiki/04_configuration.md =======

# 04. Configuration

This document outlines the system configuration files, deployment settings, and customization mechanisms.

## `config.yaml` Settings

The root-level `config.yaml` file drives the `ContinuousSimulation` and all offline batch processes.

```yaml
hardware:
  device: "mps"  # Backend hardware profiling, Metal Performance Shaders
  num_virtual_gpus: 4 # Number of simulated GPUs
  
simulation:
  retrain_interval: 50 # Retrain ML models every 50 tasks

workload_generation:
  seed: 42
  arrival_rate: 100 # Tasks per second
  task_size_range: [100, 10000]
  compute_intensity_range: [0.0, 1.0]

ml_models:
  model_type: "random_forest"
  random_forest:
    n_estimators: 100
```

## Dashboard Env Variables (`dashboard/.env`)
The React dashboard can be configured using standard Vite environment variables, controlling variables like the absolute backend URL, websocket retries, and UI feature toggles.

## Docker & Server Deployment
The system can be containerized using `docker-compose.yml`. A typical setup runs the FastAPI `uvicorn` backend on port `8000`, the PostgreSQL DB on `5432`, and the React frontend on `5173`. 
The `run_live_dashboard.sh` initiates this local sequence directly for development.


======= FILE: docs/wiki/01_architecture.md =======

# 01. Architecture

This document describes the high-level architecture of the Hybrid ML Scheduler.

## Overall Pipeline Diagram

```mermaid
graph TD
    subgraph "Phase 1: Profiling"
        H(Hardware Profiler) -->|Benchmark| P[Hardware Profile JSON]
    end

    subgraph "Phase 2 & 3: Training"
        WG[Workload Gen] -->|Training Data| ML[Random Forest Trainer]
        WG -->|Pre-train Data| RL[DQN Trainer]
    end

    subgraph "Phase 4: Runtime"
        Gen[Generator Stream] -->|Task| Sch{Scheduler Decision}
        Sch -->|Hybrid Prediction| S1[Simulate ML]
        Sch -->|DQN Policy| S2[Simulate RL]
        
        S1 -->|Results| DB[(PostgreSQL)]
        S2 -->|Reward| RL
    end
    
    DB -->|Sync| API[FastAPI Backend]
    API -->|WebSocket| UI[React Dashboard]
```

## Core Components

1. **Simulation Engine (`src/simulation_engine.py`)** 
   The core loop driving the entire system. It generates tasks, distributes them to all 6 schedulers (Round Robin, Random, Greedy, Hybrid ML, RL Agent, Oracle), computes metrics, handles ML retraining, and broadcasts results.

2. **FastAPI Backend (`backend/` & `src/dashboard_server_v2.py`)**
   Serves as the bridge between the backend physics engine and the front-end dashboard using asynchronous WebSockets. Currently migrating towards a robust Service/Repository pattern inside the `backend/` module.

3. **React Dashboard (`dashboard/src/App.jsx`)**
   The single-page application (SPA) providing real-time visibility. It features widget-based visualizations for global comparisons, real-time load, radar charts, and ML decision auditing.

4. **Schedulers**
   * **Hybrid ML (`src/online_scheduler.py` & `src/ml_models.py`)**: Uses a Scikit-Learn `RandomForestRegressor` to predict optimal GPU fractions based on task physics.
   * **RL Agent (`src/dqn_scheduler.py`)**: Uses PyTorch (DQN) with a 256-neuron architecture to map state (Size, Intensity, Memory) to CPU/GPU assignments, optimizing for makespan and energy.

5. **Workload Generator (`src/workload_generator.py`)**
   Generates a synthetic task stream mimicking a Log-Normal or Pareto distribution common in High-Performance Computing (HPC) clusters.


======= FILE: docs/wiki/08_project_journey.md =======

# 08. Project Journey

This document captures the evolution and architectural phases of the Hybrid ML Scheduler across its lifecycle.

## Phase 1: Heuristic "Rules-Based" Approach (Legacy)
Our earliest iterations focused entirely on hardcoded Round Robin, Greedy, and Random simulations to validate the underlying `VirtualGPU` physics math for task distributions. We quickly realized the NP-Hard nature of Amdahl's Law in heterogeneous clusters required complex runtime adjustments based on Memory intensity vs Hardware availability constraints.

## Phase 2: Supervised Training (Hybrid ML)
We introduced Scikit-Learn `RandomForestRegressor`. By running a hypothetical perfect "Oracle" offline batch job representing $10,000$ Task variations, we generated an optimal training dataset representing the "perfect GPU fractional split for any known size". The system operated in an Offline configuration.

## Phase 3: Unsupervised DQN Evolution (The RL Agent)
Because Offline dataset inference lacks real-time insight into the cluster's active "live load" variation, we engineered the PyTorch Deep Q-Network Agent. We replaced Static Inference passing with dynamic Epsilon-Greedy inference logic, calculating latency penalty Rewards against Energy Consumption. The RL agent successfully generated dynamic adaptation.

## Phase 4: Full Stack Orchestration (The Live Dashboard)
We merged the independent Python batch scripts into the FastAPI asynchronous backend core. Using WebSockets, the simulated payload distributions and live metric tracking were cast onto the React/Vite visualization dashboard, creating a real-time cluster "brain monitor".

## Phase 5 (Current): Database Logging & Architectural Refactoring
We are actively transitioning the internal data logs into structured PostgreSQL repositories, separating ML model serialization, fixing Author/Dependency module mismatch bugs, and standardizing error handling codebases to guarantee end-to-end framework resilience.


======= FILE: docs/wiki/03_api_reference.md =======

# 03. API Reference

This document covers the APIs exposed by the Hybrid ML Scheduler backend for the dashboard and interaction.

## REST Endpoints (`/api/*`)

*   **`GET /api/status`**
    *   **Returns**: `{"is_running": true, "tasks_processed": 500, ...}`
    *   **Description**: Used by the React dashboard as a heartbeat check.

*   **`POST /api/pause`**
    *   **Action**: Suspends the simulation loops inside the `ContinuousSimulation` class, halting Workload execution.

*   **`POST /api/resume`**
    *   **Action**: Resumes the Workload execution and task simulation loop.

*   **`GET /api/history/comparative`**
    *   **Returns**: JSON array of detailed scheduler results suitable for long-term historical charting.
    *   **Parameters**: Supports a `limit` query string to constrain the returned batch of records.

*   **`GET /api/full_history`**
    *   **Returns**: Retrieve complete historical training data.
    *   **Format**: `[{"task_id": 0, "size": 960, "compute_intensity": 0.26, ...}, ...]`

*   **`DELETE /api/history`**
    *   **Action**: Clear all historical data from PostgreSQL.

## WebSocket Protocol (`ws://localhost:8000/ws`)

The dashboard connects to this endpoint to receive real-time streams of simulated task outputs.

*   **Update Frequency**: Dispatched approximately 2x per second (Every 500ms).
*   **Payload Format**:
    ```json
    {
      "type": "simulation_update",
      "task": { 
        "id": 101, 
        "size": 960,
        "intensity": 0.9,
        "memory": 81
      },
      "comparison": [ 
        {"name": "rl_agent", "avg_time": 1.2}, 
        ... 
      ],
      "latest_results": { 
        "hybrid_ml": {"time": 0.45, "energy": 22.5, "cost": 0.0009},
        "oracle": {"time": 0.42, "energy": 21.0, "cost": 0.0008}
      },
      "utilization": {
         "average_utilization": 0.65,
         "gpu_0": {"utilization": 0.72}
      }
    }
    ```


======= FILE: docs/wiki/07_troubleshooting.md =======

# 07. Troubleshooting & FAQ

This document addresses common errors and provides the 100-Question project FAQ base.

## Common Errors and Fixes

1. **`WebSocketConnectionError: ws://localhost:8000/ws`**
   * **Problem**: The Vite dashboard shows a flashing generic API Error Modal.
   * **Fix**: Ensure the FastAPI backend is running via `run_live_dashboard.sh`. Ensure no other processes are squatting on `:8000`.

2. **`ModuleNotFoundError: No module named 'torch'`**
   * **Problem**: Missing Apple Silicon/MPS compatibility or PyTorch native.
   * **Fix**: Follow up via the specific Python MPS wheel (`pip install -r requirements.txt`). Ensure python 3.10+ is standard.

3. **`Exception: RL pre-train collision during batch fetch`**
   * **Problem**: During Docker deploy, the RL DQN Experience Replay buffer triggers an under-fill logic crash.
   * **Fix**: Lower the `simulate_task_execution` batch queue buffer length dynamically in `config.yaml`.

## Selected FAQ

* **What is the state space?** Task Size, Intensity, Memory.
* **What is the action space?** CPU, GPU 0, GPU 1, GPU 2, GPU 3.
* **What is the reward function?** Negative combination of Time and Energy. Why? Because RL maximizes score, and we want to minimize execution time constraints.
* **Does it model Network Latency?** Implicitly via "Transfer Time" (`Time = Size / Bandwidth`).
* **Why use SQLite/CSV for logging?** Zero configuration setup for local developers. The long term strategy relies on Postgres.
* **What is "Arrival Rate"?** How many tasks enter the queue per second from the generator.


======= FILE: docs/wiki/README.md =======

# Hybrid ML Scheduler: Documentation Brain

Welcome to the Documentation Brain for the **Hybrid ML Scheduler** project. This wiki serves as the definitive guide to the architecture, codebase, APIs, and workflows of our system. 

## Project Overview

The Hybrid ML Scheduler is a simulation framework for comparing different task scheduling strategies in heterogeneous computing environments (e.g., CPU + GPUs). It aims to solve the "Heterogeneous Scheduling Bottleneck" by testing Reinforcement Learning (RL) and Machine Learning (Random Forest) models against standard heuristic schedulers (Round Robin, Greedy) and a theoretical Oracle. A real-time React dashboard visualizes the cluster load, scheduler decisions, performance, energy, and costs in real-time.

## Quick Start
```bash
# Clone the repository
git clone <repository-url>
cd hybrid_ml_scheduler

# Install dependencies
pip install -r requirements.txt
cd dashboard && npm install && cd ..

# Run Live Dashboard Mode
./run_live_dashboard.sh

# Run Heavy Scientific Offline Mode
python scripts/run_heavy_simulation.py
```

## System State
Currently, the pipeline has a fully functional continuous simulation engine (`src/simulation_engine.py`) and a real-time Vite/React dashboard. We are stabilizing the RL agent's pre-training logic to minimize early exploration penalties and building out our full database logging via PostgreSQL.

## Table of Contents

- [01 Architecture](01_architecture.md): System design, core components, and diagrams.
- [02 Code Reference](02_code_reference.md): Walkthrough of classes, physics models, and schemas.
- [03 API Reference](03_api_reference.md): WebSockets, FastAPI endpoints, and data payloads.
- [04 Configuration](04_configuration.md): `config.yaml`, scaling the simulation, and environment settings.
- [05 Data Workflow](05_data_workflow.md): How a task goes from generation to execution and metric logging.
- [06 Current Challenges](06_current_challenges.md): Technical debt, TODOs, and known bugs.
- [07 Troubleshooting](07_troubleshooting.md): Common errors and the 100-Question FAQ.
- [08 Project Journey](08_project_journey.md): Evolution of the project architecture and legacy phases.


======= FILE: docs/wiki/06_current_challenges.md =======

# 06. Current Challenges

Below are the known limitations, tech debt, and immediate TODOs for the Hybrid ML Scheduler ecosystem.

## Known Challenges & Bugs

* **Initial RL Agent Latency Penalties**: The Reinforcement Learning (DQN) Agent often makes terrible decisions for the first hundred loops (exploration phase), exploding the cluster cost metrics.
  * **Workaround**: We instituted a hacky Heuristic `pretrain()` algorithm that simulates 1000 tasks statically to seed the Experience Replay.
* **Hybrid ML Scikit-Learn Bootstrapping**: Similar to the RL Agent, the Random Forest model cannot infer answers on `Task 1`. 
  * **Workaround**: Currently injecting fake manual data into `_initial_pretrain()` as scaffolding.
* **Cost Dominance over MakeSpan**: The current mathematical formulation penalizes GPUs by 5x (cost scaling from AWS). Because CPU execution is artificially 5x cheaper, both ML models frequently default to exclusively scheduling on CPU to farm positive evaluation scores, dragging the total system Makespan.
  * **TODO**: Abstract out the Evaluation reward variables into user-adjustable levers rather than hardcoded metrics in `simulator.py` power models.

## Backlog / Technical Debt
* Modify the React dashboard WebSocket payload to shrink the massive array broadcasting (Current implementation slows down the Vite frontend heavily during long-term continuous 10H+ scaling runs).
* Migrate SQLite `data/long_term_history.csv` to full-scale distributed PostgreSQL schema logs in Azure.
* DAG Data structures: `Task`s currently run homogeneously and independently. The engine needs a DAG topological sort dependency map logic overhaul.


======= FILE: dashboard/index.html =======

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>dashboard</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>


======= FILE: dashboard/tailwind.config.js =======

/** @type {import('tailwindcss').Config} */
export default {
    content: [
        "./index.html",
        "./src/**/*.{js,ts,jsx,tsx}",
    ],
    theme: {
        extend: {
            colors: {
                // Cyberpunk Palette
                bg: {
                    dark: '#050505',
                    panel: 'rgba(24, 24, 27, 0.6)',
                },
                primary: {
                    DEFAULT: '#00f3ff', // Neon Cyan
                    dim: 'rgba(0, 243, 255, 0.1)',
                },
                secondary: {
                    DEFAULT: '#bc13fe', // Neon Purple
                    dim: 'rgba(188, 19, 254, 0.1)',
                },
                accent: {
                    DEFAULT: '#ff0055', // Neon Pink
                },
                success: {
                    DEFAULT: '#0aff00', // Neon Green
                }
            },
            fontFamily: {
                mono: ['"JetBrains Mono"', 'monospace'],
                sans: ['"Rajdhani"', 'sans-serif'],
            },
            boxShadow: {
                'neon-cyan': '0 0 10px rgba(0, 243, 255, 0.5), 0 0 20px rgba(0, 243, 255, 0.3)',
                'neon-purple': '0 0 10px rgba(188, 19, 254, 0.5), 0 0 20px rgba(188, 19, 254, 0.3)',
            },
            animation: {
                'pulse-slow': 'pulse 3s cubic-bezier(0.4, 0, 0.6, 1) infinite',
                'scanline': 'scanline 8s linear infinite',
            },
            keyframes: {
                scanline: {
                    '0%': { transform: 'translateY(-100%)' },
                    '100%': { transform: 'translateY(100%)' },
                }
            }
        },
    },
    plugins: [],
}


======= FILE: dashboard/vite.config.js =======

import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'

// https://vite.dev/config/
export default defineConfig({
  plugins: [react()],
  test: {
    globals: true,
    environment: 'jsdom',
    setupFiles: './src/setupTests.js',
  },
})


======= FILE: dashboard/README.md =======

# React + Vite

This template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.

Currently, two official plugins are available:

- [@vitejs/plugin-react](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react) uses [Babel](https://babeljs.io/) (or [oxc](https://oxc.rs) when used in [rolldown-vite](https://vite.dev/guide/rolldown)) for Fast Refresh
- [@vitejs/plugin-react-swc](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react-swc) uses [SWC](https://swc.rs/) for Fast Refresh

## React Compiler

The React Compiler is not enabled on this template because of its impact on dev & build performances. To add it, see [this documentation](https://react.dev/learn/react-compiler/installation).

## Expanding the ESLint configuration

If you are developing a production application, we recommend using TypeScript with type-aware lint rules enabled. Check out the [TS template](https://github.com/vitejs/vite/tree/main/packages/create-vite/template-react-ts) for information on how to integrate TypeScript and [`typescript-eslint`](https://typescript-eslint.io) in your project.


======= FILE: dashboard/package-lock.json =======

{
  "name": "dashboard",
  "version": "0.0.0",
  "lockfileVersion": 3,
  "requires": true,
  "packages": {
    "": {
      "name": "dashboard",
      "version": "0.0.0",
      "dependencies": {
        "lodash": "^4.17.21",
        "lucide-react": "^0.555.0",
        "react": "^19.2.0",
        "react-dom": "^19.2.0",
        "react-grid-layout": "^1.5.2",
        "recharts": "^3.5.0"
      },
      "devDependencies": {
        "@eslint/js": "^9.39.1",
        "@tailwindcss/postcss": "^4.1.17",
        "@testing-library/jest-dom": "^6.9.1",
        "@testing-library/react": "^16.3.0",
        "@types/react": "^19.2.5",
        "@types/react-dom": "^19.2.3",
        "@vitejs/plugin-react": "^5.1.1",
        "autoprefixer": "^10.4.22",
        "eslint": "^9.39.1",
        "eslint-plugin-react-hooks": "^7.0.1",
        "eslint-plugin-react-refresh": "^0.4.24",
        "globals": "^16.5.0",
        "jsdom": "^27.2.0",
        "postcss": "^8.5.6",
        "tailwindcss": "^4.1.17",
        "vite": "^7.2.4",
        "vitest": "^4.0.14"
      }
    },
    "node_modules/@acemir/cssom": {
      "version": "0.9.24",
      "resolved": "https://registry.npmjs.org/@acemir/cssom/-/cssom-0.9.24.tgz",
      "integrity": "sha512-5YjgMmAiT2rjJZU7XK1SNI7iqTy92DpaYVgG6x63FxkJ11UpYfLndHJATtinWJClAXiOlW9XWaUyAQf8pMrQPg==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@adobe/css-tools": {
      "version": "4.4.4",
      "resolved": "https://registry.npmjs.org/@adobe/css-tools/-/css-tools-4.4.4.tgz",
      "integrity": "sha512-Elp+iwUx5rN5+Y8xLt5/GRoG20WGoDCQ/1Fb+1LiGtvwbDavuSk0jhD/eZdckHAuzcDzccnkv+rEjyWfRx18gg==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@alloc/quick-lru": {
      "version": "5.2.0",
      "resolved": "https://registry.npmjs.org/@alloc/quick-lru/-/quick-lru-5.2.0.tgz",
      "integrity": "sha512-UrcABB+4bUrFABwbluTIBErXwvbsU/V7TZWfmbgJfbkwiBuziS9gxdODUyuiecfdGQ85jglMW6juS3+z5TsKLw==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/@asamuzakjp/css-color": {
      "version": "4.1.0",
      "resolved": "https://registry.npmjs.org/@asamuzakjp/css-color/-/css-color-4.1.0.tgz",
      "integrity": "sha512-9xiBAtLn4aNsa4mDnpovJvBn72tNEIACyvlqaNJ+ADemR+yeMJWnBudOi2qGDviJa7SwcDOU/TRh5dnET7qk0w==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@csstools/css-calc": "^2.1.4",
        "@csstools/css-color-parser": "^3.1.0",
        "@csstools/css-parser-algorithms": "^3.0.5",
        "@csstools/css-tokenizer": "^3.0.4",
        "lru-cache": "^11.2.2"
      }
    },
    "node_modules/@asamuzakjp/css-color/node_modules/lru-cache": {
      "version": "11.2.2",
      "resolved": "https://registry.npmjs.org/lru-cache/-/lru-cache-11.2.2.tgz",
      "integrity": "sha512-F9ODfyqML2coTIsQpSkRHnLSZMtkU8Q+mSfcaIyKwy58u+8k5nvAYeiNhsyMARvzNcXJ9QfWVrcPsC9e9rAxtg==",
      "dev": true,
      "license": "ISC",
      "engines": {
        "node": "20 || >=22"
      }
    },
    "node_modules/@asamuzakjp/dom-selector": {
      "version": "6.7.4",
      "resolved": "https://registry.npmjs.org/@asamuzakjp/dom-selector/-/dom-selector-6.7.4.tgz",
      "integrity": "sha512-buQDjkm+wDPXd6c13534URWZqbz0RP5PAhXZ+LIoa5LgwInT9HVJvGIJivg75vi8I13CxDGdTnz+aY5YUJlIAA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@asamuzakjp/nwsapi": "^2.3.9",
        "bidi-js": "^1.0.3",
        "css-tree": "^3.1.0",
        "is-potential-custom-element-name": "^1.0.1",
        "lru-cache": "^11.2.2"
      }
    },
    "node_modules/@asamuzakjp/dom-selector/node_modules/lru-cache": {
      "version": "11.2.2",
      "resolved": "https://registry.npmjs.org/lru-cache/-/lru-cache-11.2.2.tgz",
      "integrity": "sha512-F9ODfyqML2coTIsQpSkRHnLSZMtkU8Q+mSfcaIyKwy58u+8k5nvAYeiNhsyMARvzNcXJ9QfWVrcPsC9e9rAxtg==",
      "dev": true,
      "license": "ISC",
      "engines": {
        "node": "20 || >=22"
      }
    },
    "node_modules/@asamuzakjp/nwsapi": {
      "version": "2.3.9",
      "resolved": "https://registry.npmjs.org/@asamuzakjp/nwsapi/-/nwsapi-2.3.9.tgz",
      "integrity": "sha512-n8GuYSrI9bF7FFZ/SjhwevlHc8xaVlb/7HmHelnc/PZXBD2ZR49NnN9sMMuDdEGPeeRQ5d0hqlSlEpgCX3Wl0Q==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@babel/code-frame": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/code-frame/-/code-frame-7.27.1.tgz",
      "integrity": "sha512-cjQ7ZlQ0Mv3b47hABuTevyTuYN4i+loJKGeV9flcCgIK37cCXRh+L1bd3iBHlynerhQ7BhCkn2BPbQUL+rGqFg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-validator-identifier": "^7.27.1",
        "js-tokens": "^4.0.0",
        "picocolors": "^1.1.1"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/compat-data": {
      "version": "7.28.5",
      "resolved": "https://registry.npmjs.org/@babel/compat-data/-/compat-data-7.28.5.tgz",
      "integrity": "sha512-6uFXyCayocRbqhZOB+6XcuZbkMNimwfVGFji8CTZnCzOHVGvDqzvitu1re2AU5LROliz7eQPhB8CpAMvnx9EjA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/core": {
      "version": "7.28.5",
      "resolved": "https://registry.npmjs.org/@babel/core/-/core-7.28.5.tgz",
      "integrity": "sha512-e7jT4DxYvIDLk1ZHmU/m/mB19rex9sv0c2ftBtjSBv+kVM/902eh0fINUzD7UwLLNR+jU585GxUJ8/EBfAM5fw==",
      "dev": true,
      "license": "MIT",
      "peer": true,
      "dependencies": {
        "@babel/code-frame": "^7.27.1",
        "@babel/generator": "^7.28.5",
        "@babel/helper-compilation-targets": "^7.27.2",
        "@babel/helper-module-transforms": "^7.28.3",
        "@babel/helpers": "^7.28.4",
        "@babel/parser": "^7.28.5",
        "@babel/template": "^7.27.2",
        "@babel/traverse": "^7.28.5",
        "@babel/types": "^7.28.5",
        "@jridgewell/remapping": "^2.3.5",
        "convert-source-map": "^2.0.0",
        "debug": "^4.1.0",
        "gensync": "^1.0.0-beta.2",
        "json5": "^2.2.3",
        "semver": "^6.3.1"
      },
      "engines": {
        "node": ">=6.9.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/babel"
      }
    },
    "node_modules/@babel/generator": {
      "version": "7.28.5",
      "resolved": "https://registry.npmjs.org/@babel/generator/-/generator-7.28.5.tgz",
      "integrity": "sha512-3EwLFhZ38J4VyIP6WNtt2kUdW9dokXA9Cr4IVIFHuCpZ3H8/YFOl5JjZHisrn1fATPBmKKqXzDFvh9fUwHz6CQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/parser": "^7.28.5",
        "@babel/types": "^7.28.5",
        "@jridgewell/gen-mapping": "^0.3.12",
        "@jridgewell/trace-mapping": "^0.3.28",
        "jsesc": "^3.0.2"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-compilation-targets": {
      "version": "7.27.2",
      "resolved": "https://registry.npmjs.org/@babel/helper-compilation-targets/-/helper-compilation-targets-7.27.2.tgz",
      "integrity": "sha512-2+1thGUUWWjLTYTHZWK1n8Yga0ijBz1XAhUXcKy81rd5g6yh7hGqMp45v7cadSbEHc9G3OTv45SyneRN3ps4DQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/compat-data": "^7.27.2",
        "@babel/helper-validator-option": "^7.27.1",
        "browserslist": "^4.24.0",
        "lru-cache": "^5.1.1",
        "semver": "^6.3.1"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-globals": {
      "version": "7.28.0",
      "resolved": "https://registry.npmjs.org/@babel/helper-globals/-/helper-globals-7.28.0.tgz",
      "integrity": "sha512-+W6cISkXFa1jXsDEdYA8HeevQT/FULhxzR99pxphltZcVaugps53THCeiWA8SguxxpSp3gKPiuYfSWopkLQ4hw==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-module-imports": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/helper-module-imports/-/helper-module-imports-7.27.1.tgz",
      "integrity": "sha512-0gSFWUPNXNopqtIPQvlD5WgXYI5GY2kP2cCvoT8kczjbfcfuIljTbcWrulD1CIPIX2gt1wghbDy08yE1p+/r3w==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/traverse": "^7.27.1",
        "@babel/types": "^7.27.1"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-module-transforms": {
      "version": "7.28.3",
      "resolved": "https://registry.npmjs.org/@babel/helper-module-transforms/-/helper-module-transforms-7.28.3.tgz",
      "integrity": "sha512-gytXUbs8k2sXS9PnQptz5o0QnpLL51SwASIORY6XaBKF88nsOT0Zw9szLqlSGQDP/4TljBAD5y98p2U1fqkdsw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-module-imports": "^7.27.1",
        "@babel/helper-validator-identifier": "^7.27.1",
        "@babel/traverse": "^7.28.3"
      },
      "engines": {
        "node": ">=6.9.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0"
      }
    },
    "node_modules/@babel/helper-plugin-utils": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/helper-plugin-utils/-/helper-plugin-utils-7.27.1.tgz",
      "integrity": "sha512-1gn1Up5YXka3YYAHGKpbideQ5Yjf1tDa9qYcgysz+cNCXukyLl6DjPXhD3VRwSb8c0J9tA4b2+rHEZtc6R0tlw==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-string-parser": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/helper-string-parser/-/helper-string-parser-7.27.1.tgz",
      "integrity": "sha512-qMlSxKbpRlAridDExk92nSobyDdpPijUq2DW6oDnUqd0iOGxmQjyqhMIihI9+zv4LPyZdRje2cavWPbCbWm3eA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-validator-identifier": {
      "version": "7.28.5",
      "resolved": "https://registry.npmjs.org/@babel/helper-validator-identifier/-/helper-validator-identifier-7.28.5.tgz",
      "integrity": "sha512-qSs4ifwzKJSV39ucNjsvc6WVHs6b7S03sOh2OcHF9UHfVPqWWALUsNUVzhSBiItjRZoLHx7nIarVjqKVusUZ1Q==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-validator-option": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/helper-validator-option/-/helper-validator-option-7.27.1.tgz",
      "integrity": "sha512-YvjJow9FxbhFFKDSuFnVCe2WxXk1zWc22fFePVNEaWJEu8IrZVlda6N0uHwzZrUM1il7NC9Mlp4MaJYbYd9JSg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helpers": {
      "version": "7.28.4",
      "resolved": "https://registry.npmjs.org/@babel/helpers/-/helpers-7.28.4.tgz",
      "integrity": "sha512-HFN59MmQXGHVyYadKLVumYsA9dBFun/ldYxipEjzA4196jpLZd8UjEEBLkbEkvfYreDqJhZxYAWFPtrfhNpj4w==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/template": "^7.27.2",
        "@babel/types": "^7.28.4"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/parser": {
      "version": "7.28.5",
      "resolved": "https://registry.npmjs.org/@babel/parser/-/parser-7.28.5.tgz",
      "integrity": "sha512-KKBU1VGYR7ORr3At5HAtUQ+TV3SzRCXmA/8OdDZiLDBIZxVyzXuztPjfLd3BV1PRAQGCMWWSHYhL0F8d5uHBDQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/types": "^7.28.5"
      },
      "bin": {
        "parser": "bin/babel-parser.js"
      },
      "engines": {
        "node": ">=6.0.0"
      }
    },
    "node_modules/@babel/plugin-transform-react-jsx-self": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/plugin-transform-react-jsx-self/-/plugin-transform-react-jsx-self-7.27.1.tgz",
      "integrity": "sha512-6UzkCs+ejGdZ5mFFC/OCUrv028ab2fp1znZmCZjAOBKiBK2jXD1O+BPSfX8X2qjJ75fZBMSnQn3Rq2mrBJK2mw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-plugin-utils": "^7.27.1"
      },
      "engines": {
        "node": ">=6.9.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0-0"
      }
    },
    "node_modules/@babel/plugin-transform-react-jsx-source": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/plugin-transform-react-jsx-source/-/plugin-transform-react-jsx-source-7.27.1.tgz",
      "integrity": "sha512-zbwoTsBruTeKB9hSq73ha66iFeJHuaFkUbwvqElnygoNbj/jHRsSeokowZFN3CZ64IvEqcmmkVe89OPXc7ldAw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-plugin-utils": "^7.27.1"
      },
      "engines": {
        "node": ">=6.9.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0-0"
      }
    },
    "node_modules/@babel/runtime": {
      "version": "7.28.4",
      "resolved": "https://registry.npmjs.org/@babel/runtime/-/runtime-7.28.4.tgz",
      "integrity": "sha512-Q/N6JNWvIvPnLDvjlE1OUBLPQHH6l3CltCEsHIujp45zQUSSh8K+gHnaEX45yAT1nyngnINhvWtzN+Nb9D8RAQ==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/template": {
      "version": "7.27.2",
      "resolved": "https://registry.npmjs.org/@babel/template/-/template-7.27.2.tgz",
      "integrity": "sha512-LPDZ85aEJyYSd18/DkjNh4/y1ntkE5KwUHWTiqgRxruuZL2F1yuHligVHLvcHY2vMHXttKFpJn6LwfI7cw7ODw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/code-frame": "^7.27.1",
        "@babel/parser": "^7.27.2",
        "@babel/types": "^7.27.1"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/traverse": {
      "version": "7.28.5",
      "resolved": "https://registry.npmjs.org/@babel/traverse/-/traverse-7.28.5.tgz",
      "integrity": "sha512-TCCj4t55U90khlYkVV/0TfkJkAkUg3jZFA3Neb7unZT8CPok7iiRfaX0F+WnqWqt7OxhOn0uBKXCw4lbL8W0aQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/code-frame": "^7.27.1",
        "@babel/generator": "^7.28.5",
        "@babel/helper-globals": "^7.28.0",
        "@babel/parser": "^7.28.5",
        "@babel/template": "^7.27.2",
        "@babel/types": "^7.28.5",
        "debug": "^4.3.1"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/types": {
      "version": "7.28.5",
      "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.28.5.tgz",
      "integrity": "sha512-qQ5m48eI/MFLQ5PxQj4PFaprjyCTLI37ElWMmNs0K8Lk3dVeOdNpB3ks8jc7yM5CDmVC73eMVk/trk3fgmrUpA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-string-parser": "^7.27.1",
        "@babel/helper-validator-identifier": "^7.28.5"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@csstools/color-helpers": {
      "version": "5.1.0",
      "resolved": "https://registry.npmjs.org/@csstools/color-helpers/-/color-helpers-5.1.0.tgz",
      "integrity": "sha512-S11EXWJyy0Mz5SYvRmY8nJYTFFd1LCNV+7cXyAgQtOOuzb4EsgfqDufL+9esx72/eLhsRdGZwaldu/h+E4t4BA==",
      "dev": true,
      "funding": [
        {
          "type": "github",
          "url": "https://github.com/sponsors/csstools"
        },
        {
          "type": "opencollective",
          "url": "https://opencollective.com/csstools"
        }
      ],
      "license": "MIT-0",
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@csstools/css-calc": {
      "version": "2.1.4",
      "resolved": "https://registry.npmjs.org/@csstools/css-calc/-/css-calc-2.1.4.tgz",
      "integrity": "sha512-3N8oaj+0juUw/1H3YwmDDJXCgTB1gKU6Hc/bB502u9zR0q2vd786XJH9QfrKIEgFlZmhZiq6epXl4rHqhzsIgQ==",
      "dev": true,
      "funding": [
        {
          "type": "github",
          "url": "https://github.com/sponsors/csstools"
        },
        {
          "type": "opencollective",
          "url": "https://opencollective.com/csstools"
        }
      ],
      "license": "MIT",
      "engines": {
        "node": ">=18"
      },
      "peerDependencies": {
        "@csstools/css-parser-algorithms": "^3.0.5",
        "@csstools/css-tokenizer": "^3.0.4"
      }
    },
    "node_modules/@csstools/css-color-parser": {
      "version": "3.1.0",
      "resolved": "https://registry.npmjs.org/@csstools/css-color-parser/-/css-color-parser-3.1.0.tgz",
      "integrity": "sha512-nbtKwh3a6xNVIp/VRuXV64yTKnb1IjTAEEh3irzS+HkKjAOYLTGNb9pmVNntZ8iVBHcWDA2Dof0QtPgFI1BaTA==",
      "dev": true,
      "funding": [
        {
          "type": "github",
          "url": "https://github.com/sponsors/csstools"
        },
        {
          "type": "opencollective",
          "url": "https://opencollective.com/csstools"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "@csstools/color-helpers": "^5.1.0",
        "@csstools/css-calc": "^2.1.4"
      },
      "engines": {
        "node": ">=18"
      },
      "peerDependencies": {
        "@csstools/css-parser-algorithms": "^3.0.5",
        "@csstools/css-tokenizer": "^3.0.4"
      }
    },
    "node_modules/@csstools/css-parser-algorithms": {
      "version": "3.0.5",
      "resolved": "https://registry.npmjs.org/@csstools/css-parser-algorithms/-/css-parser-algorithms-3.0.5.tgz",
      "integrity": "sha512-DaDeUkXZKjdGhgYaHNJTV9pV7Y9B3b644jCLs9Upc3VeNGg6LWARAT6O+Q+/COo+2gg/bM5rhpMAtf70WqfBdQ==",
      "dev": true,
      "funding": [
        {
          "type": "github",
          "url": "https://github.com/sponsors/csstools"
        },
        {
          "type": "opencollective",
          "url": "https://opencollective.com/csstools"
        }
      ],
      "license": "MIT",
      "peer": true,
      "engines": {
        "node": ">=18"
      },
      "peerDependencies": {
        "@csstools/css-tokenizer": "^3.0.4"
      }
    },
    "node_modules/@csstools/css-syntax-patches-for-csstree": {
      "version": "1.0.19",
      "resolved": "https://registry.npmjs.org/@csstools/css-syntax-patches-for-csstree/-/css-syntax-patches-for-csstree-1.0.19.tgz",
      "integrity": "sha512-QW5/SM2ARltEhoKcmRI1LoLf3/C7dHGswwCnfLcoMgqurBT4f8GvwXMgAbK/FwcxthmJRK5MGTtddj0yQn0J9g==",
      "dev": true,
      "funding": [
        {
          "type": "github",
          "url": "https://github.com/sponsors/csstools"
        },
        {
          "type": "opencollective",
          "url": "https://opencollective.com/csstools"
        }
      ],
      "license": "MIT-0",
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@csstools/css-tokenizer": {
      "version": "3.0.4",
      "resolved": "https://registry.npmjs.org/@csstools/css-tokenizer/-/css-tokenizer-3.0.4.tgz",
      "integrity": "sha512-Vd/9EVDiu6PPJt9yAh6roZP6El1xHrdvIVGjyBsHR0RYwNHgL7FJPyIIW4fANJNG6FtyZfvlRPpFI4ZM/lubvw==",
      "dev": true,
      "funding": [
        {
          "type": "github",
          "url": "https://github.com/sponsors/csstools"
        },
        {
          "type": "opencollective",
          "url": "https://opencollective.com/csstools"
        }
      ],
      "license": "MIT",
      "peer": true,
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/aix-ppc64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/aix-ppc64/-/aix-ppc64-0.25.12.tgz",
      "integrity": "sha512-Hhmwd6CInZ3dwpuGTF8fJG6yoWmsToE+vYgD4nytZVxcu1ulHpUQRAB1UJ8+N1Am3Mz4+xOByoQoSZf4D+CpkA==",
      "cpu": [
        "ppc64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "aix"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/android-arm": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/android-arm/-/android-arm-0.25.12.tgz",
      "integrity": "sha512-VJ+sKvNA/GE7Ccacc9Cha7bpS8nyzVv0jdVgwNDaR4gDMC/2TTRc33Ip8qrNYUcpkOHUT5OZ0bUcNNVZQ9RLlg==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/android-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/android-arm64/-/android-arm64-0.25.12.tgz",
      "integrity": "sha512-6AAmLG7zwD1Z159jCKPvAxZd4y/VTO0VkprYy+3N2FtJ8+BQWFXU+OxARIwA46c5tdD9SsKGZ/1ocqBS/gAKHg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/android-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/android-x64/-/android-x64-0.25.12.tgz",
      "integrity": "sha512-5jbb+2hhDHx5phYR2By8GTWEzn6I9UqR11Kwf22iKbNpYrsmRB18aX/9ivc5cabcUiAT/wM+YIZ6SG9QO6a8kg==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/darwin-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/darwin-arm64/-/darwin-arm64-0.25.12.tgz",
      "integrity": "sha512-N3zl+lxHCifgIlcMUP5016ESkeQjLj/959RxxNYIthIg+CQHInujFuXeWbWMgnTo4cp5XVHqFPmpyu9J65C1Yg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/darwin-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/darwin-x64/-/darwin-x64-0.25.12.tgz",
      "integrity": "sha512-HQ9ka4Kx21qHXwtlTUVbKJOAnmG1ipXhdWTmNXiPzPfWKpXqASVcWdnf2bnL73wgjNrFXAa3yYvBSd9pzfEIpA==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/freebsd-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/freebsd-arm64/-/freebsd-arm64-0.25.12.tgz",
      "integrity": "sha512-gA0Bx759+7Jve03K1S0vkOu5Lg/85dou3EseOGUes8flVOGxbhDDh/iZaoek11Y8mtyKPGF3vP8XhnkDEAmzeg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "freebsd"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/freebsd-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/freebsd-x64/-/freebsd-x64-0.25.12.tgz",
      "integrity": "sha512-TGbO26Yw2xsHzxtbVFGEXBFH0FRAP7gtcPE7P5yP7wGy7cXK2oO7RyOhL5NLiqTlBh47XhmIUXuGciXEqYFfBQ==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "freebsd"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-arm": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-arm/-/linux-arm-0.25.12.tgz",
      "integrity": "sha512-lPDGyC1JPDou8kGcywY0YILzWlhhnRjdof3UlcoqYmS9El818LLfJJc3PXXgZHrHCAKs/Z2SeZtDJr5MrkxtOw==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-arm64/-/linux-arm64-0.25.12.tgz",
      "integrity": "sha512-8bwX7a8FghIgrupcxb4aUmYDLp8pX06rGh5HqDT7bB+8Rdells6mHvrFHHW2JAOPZUbnjUpKTLg6ECyzvas2AQ==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-ia32": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-ia32/-/linux-ia32-0.25.12.tgz",
      "integrity": "sha512-0y9KrdVnbMM2/vG8KfU0byhUN+EFCny9+8g202gYqSSVMonbsCfLjUO+rCci7pM0WBEtz+oK/PIwHkzxkyharA==",
      "cpu": [
        "ia32"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-loong64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-loong64/-/linux-loong64-0.25.12.tgz",
      "integrity": "sha512-h///Lr5a9rib/v1GGqXVGzjL4TMvVTv+s1DPoxQdz7l/AYv6LDSxdIwzxkrPW438oUXiDtwM10o9PmwS/6Z0Ng==",
      "cpu": [
        "loong64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-mips64el": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-mips64el/-/linux-mips64el-0.25.12.tgz",
      "integrity": "sha512-iyRrM1Pzy9GFMDLsXn1iHUm18nhKnNMWscjmp4+hpafcZjrr2WbT//d20xaGljXDBYHqRcl8HnxbX6uaA/eGVw==",
      "cpu": [
        "mips64el"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-ppc64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-ppc64/-/linux-ppc64-0.25.12.tgz",
      "integrity": "sha512-9meM/lRXxMi5PSUqEXRCtVjEZBGwB7P/D4yT8UG/mwIdze2aV4Vo6U5gD3+RsoHXKkHCfSxZKzmDssVlRj1QQA==",
      "cpu": [
        "ppc64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-riscv64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-riscv64/-/linux-riscv64-0.25.12.tgz",
      "integrity": "sha512-Zr7KR4hgKUpWAwb1f3o5ygT04MzqVrGEGXGLnj15YQDJErYu/BGg+wmFlIDOdJp0PmB0lLvxFIOXZgFRrdjR0w==",
      "cpu": [
        "riscv64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-s390x": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-s390x/-/linux-s390x-0.25.12.tgz",
      "integrity": "sha512-MsKncOcgTNvdtiISc/jZs/Zf8d0cl/t3gYWX8J9ubBnVOwlk65UIEEvgBORTiljloIWnBzLs4qhzPkJcitIzIg==",
      "cpu": [
        "s390x"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-x64/-/linux-x64-0.25.12.tgz",
      "integrity": "sha512-uqZMTLr/zR/ed4jIGnwSLkaHmPjOjJvnm6TVVitAa08SLS9Z0VM8wIRx7gWbJB5/J54YuIMInDquWyYvQLZkgw==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/netbsd-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/netbsd-arm64/-/netbsd-arm64-0.25.12.tgz",
      "integrity": "sha512-xXwcTq4GhRM7J9A8Gv5boanHhRa/Q9KLVmcyXHCTaM4wKfIpWkdXiMog/KsnxzJ0A1+nD+zoecuzqPmCRyBGjg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "netbsd"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/netbsd-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/netbsd-x64/-/netbsd-x64-0.25.12.tgz",
      "integrity": "sha512-Ld5pTlzPy3YwGec4OuHh1aCVCRvOXdH8DgRjfDy/oumVovmuSzWfnSJg+VtakB9Cm0gxNO9BzWkj6mtO1FMXkQ==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "netbsd"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/openbsd-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/openbsd-arm64/-/openbsd-arm64-0.25.12.tgz",
      "integrity": "sha512-fF96T6KsBo/pkQI950FARU9apGNTSlZGsv1jZBAlcLL1MLjLNIWPBkj5NlSz8aAzYKg+eNqknrUJ24QBybeR5A==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "openbsd"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/openbsd-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/openbsd-x64/-/openbsd-x64-0.25.12.tgz",
      "integrity": "sha512-MZyXUkZHjQxUvzK7rN8DJ3SRmrVrke8ZyRusHlP+kuwqTcfWLyqMOE3sScPPyeIXN/mDJIfGXvcMqCgYKekoQw==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "openbsd"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/openharmony-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/openharmony-arm64/-/openharmony-arm64-0.25.12.tgz",
      "integrity": "sha512-rm0YWsqUSRrjncSXGA7Zv78Nbnw4XL6/dzr20cyrQf7ZmRcsovpcRBdhD43Nuk3y7XIoW2OxMVvwuRvk9XdASg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "openharmony"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/sunos-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/sunos-x64/-/sunos-x64-0.25.12.tgz",
      "integrity": "sha512-3wGSCDyuTHQUzt0nV7bocDy72r2lI33QL3gkDNGkod22EsYl04sMf0qLb8luNKTOmgF/eDEDP5BFNwoBKH441w==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "sunos"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/win32-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/win32-arm64/-/win32-arm64-0.25.12.tgz",
      "integrity": "sha512-rMmLrur64A7+DKlnSuwqUdRKyd3UE7oPJZmnljqEptesKM8wx9J8gx5u0+9Pq0fQQW8vqeKebwNXdfOyP+8Bsg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/win32-ia32": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/win32-ia32/-/win32-ia32-0.25.12.tgz",
      "integrity": "sha512-HkqnmmBoCbCwxUKKNPBixiWDGCpQGVsrQfJoVGYLPT41XWF8lHuE5N6WhVia2n4o5QK5M4tYr21827fNhi4byQ==",
      "cpu": [
        "ia32"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/win32-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/win32-x64/-/win32-x64-0.25.12.tgz",
      "integrity": "sha512-alJC0uCZpTFrSL0CCDjcgleBXPnCrEAhTBILpeAp7M/OFgoqtAetfBzX0xM00MUsVVPpVjlPuMbREqnZCXaTnA==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@eslint-community/eslint-utils": {
      "version": "4.9.0",
      "resolved": "https://registry.npmjs.org/@eslint-community/eslint-utils/-/eslint-utils-4.9.0.tgz",
      "integrity": "sha512-ayVFHdtZ+hsq1t2Dy24wCmGXGe4q9Gu3smhLYALJrr473ZH27MsnSL+LKUlimp4BWJqMDMLmPpx/Q9R3OAlL4g==",
      "license": "MIT",
      "dependencies": {
        "eslint-visitor-keys": "^3.4.3"
      },
      "engines": {
        "node": "^12.22.0 || ^14.17.0 || >=16.0.0"
      },
      "funding": {
        "url": "https://opencollective.com/eslint"
      },
      "peerDependencies": {
        "eslint": "^6.0.0 || ^7.0.0 || >=8.0.0"
      }
    },
    "node_modules/@eslint-community/eslint-utils/node_modules/eslint-visitor-keys": {
      "version": "3.4.3",
      "resolved": "https://registry.npmjs.org/eslint-visitor-keys/-/eslint-visitor-keys-3.4.3.tgz",
      "integrity": "sha512-wpc+LXeiyiisxPlEkUzU6svyS1frIO3Mgxj1fdy7Pm8Ygzguax2N3Fa/D/ag1WqbOprdI+uY6wMUl8/a2G+iag==",
      "license": "Apache-2.0",
      "engines": {
        "node": "^12.22.0 || ^14.17.0 || >=16.0.0"
      },
      "funding": {
        "url": "https://opencollective.com/eslint"
      }
    },
    "node_modules/@eslint-community/regexpp": {
      "version": "4.12.2",
      "resolved": "https://registry.npmjs.org/@eslint-community/regexpp/-/regexpp-4.12.2.tgz",
      "integrity": "sha512-EriSTlt5OC9/7SXkRSCAhfSxxoSUgBm33OH+IkwbdpgoqsSsUg7y3uh+IICI/Qg4BBWr3U2i39RpmycbxMq4ew==",
      "license": "MIT",
      "engines": {
        "node": "^12.0.0 || ^14.0.0 || >=16.0.0"
      }
    },
    "node_modules/@eslint/config-array": {
      "version": "0.21.1",
      "resolved": "https://registry.npmjs.org/@eslint/config-array/-/config-array-0.21.1.tgz",
      "integrity": "sha512-aw1gNayWpdI/jSYVgzN5pL0cfzU02GT3NBpeT/DXbx1/1x7ZKxFPd9bwrzygx/qiwIQiJ1sw/zD8qY/kRvlGHA==",
      "license": "Apache-2.0",
      "dependencies": {
        "@eslint/object-schema": "^2.1.7",
        "debug": "^4.3.1",
        "minimatch": "^3.1.2"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      }
    },
    "node_modules/@eslint/config-helpers": {
      "version": "0.4.2",
      "resolved": "https://registry.npmjs.org/@eslint/config-helpers/-/config-helpers-0.4.2.tgz",
      "integrity": "sha512-gBrxN88gOIf3R7ja5K9slwNayVcZgK6SOUORm2uBzTeIEfeVaIhOpCtTox3P6R7o2jLFwLFTLnC7kU/RGcYEgw==",
      "license": "Apache-2.0",
      "dependencies": {
        "@eslint/core": "^0.17.0"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      }
    },
    "node_modules/@eslint/core": {
      "version": "0.17.0",
      "resolved": "https://registry.npmjs.org/@eslint/core/-/core-0.17.0.tgz",
      "integrity": "sha512-yL/sLrpmtDaFEiUj1osRP4TI2MDz1AddJL+jZ7KSqvBuliN4xqYY54IfdN8qD8Toa6g1iloph1fxQNkjOxrrpQ==",
      "license": "Apache-2.0",
      "dependencies": {
        "@types/json-schema": "^7.0.15"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      }
    },
    "node_modules/@eslint/eslintrc": {
      "version": "3.3.1",
      "resolved": "https://registry.npmjs.org/@eslint/eslintrc/-/eslintrc-3.3.1.tgz",
      "integrity": "sha512-gtF186CXhIl1p4pJNGZw8Yc6RlshoePRvE0X91oPGb3vZ8pM3qOS9W9NGPat9LziaBV7XrJWGylNQXkGcnM3IQ==",
      "license": "MIT",
      "dependencies": {
        "ajv": "^6.12.4",
        "debug": "^4.3.2",
        "espree": "^10.0.1",
        "globals": "^14.0.0",
        "ignore": "^5.2.0",
        "import-fresh": "^3.2.1",
        "js-yaml": "^4.1.0",
        "minimatch": "^3.1.2",
        "strip-json-comments": "^3.1.1"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      },
      "funding": {
        "url": "https://opencollective.com/eslint"
      }
    },
    "node_modules/@eslint/eslintrc/node_modules/globals": {
      "version": "14.0.0",
      "resolved": "https://registry.npmjs.org/globals/-/globals-14.0.0.tgz",
      "integrity": "sha512-oahGvuMGQlPw/ivIYBjVSrWAfWLBeku5tpPE2fOPLi+WHffIWbuh2tCjhyQhTBPMf5E9jDEH4FOmTYgYwbKwtQ==",
      "license": "MIT",
      "engines": {
        "node": ">=18"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/@eslint/js": {
      "version": "9.39.1",
      "resolved": "https://registry.npmjs.org/@eslint/js/-/js-9.39.1.tgz",
      "integrity": "sha512-S26Stp4zCy88tH94QbBv3XCuzRQiZ9yXofEILmglYTh/Ug/a9/umqvgFtYBAo3Lp0nsI/5/qH1CCrbdK3AP1Tw==",
      "license": "MIT",
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      },
      "funding": {
        "url": "https://eslint.org/donate"
      }
    },
    "node_modules/@eslint/object-schema": {
      "version": "2.1.7",
      "resolved": "https://registry.npmjs.org/@eslint/object-schema/-/object-schema-2.1.7.tgz",
      "integrity": "sha512-VtAOaymWVfZcmZbp6E2mympDIHvyjXs/12LqWYjVw6qjrfF+VK+fyG33kChz3nnK+SU5/NeHOqrTEHS8sXO3OA==",
      "license": "Apache-2.0",
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      }
    },
    "node_modules/@eslint/plugin-kit": {
      "version": "0.4.1",
      "resolved": "https://registry.npmjs.org/@eslint/plugin-kit/-/plugin-kit-0.4.1.tgz",
      "integrity": "sha512-43/qtrDUokr7LJqoF2c3+RInu/t4zfrpYdoSDfYyhg52rwLV6TnOvdG4fXm7IkSB3wErkcmJS9iEhjVtOSEjjA==",
      "license": "Apache-2.0",
      "dependencies": {
        "@eslint/core": "^0.17.0",
        "levn": "^0.4.1"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      }
    },
    "node_modules/@humanfs/core": {
      "version": "0.19.1",
      "resolved": "https://registry.npmjs.org/@humanfs/core/-/core-0.19.1.tgz",
      "integrity": "sha512-5DyQ4+1JEUzejeK1JGICcideyfUbGixgS9jNgex5nqkW+cY7WZhxBigmieN5Qnw9ZosSNVC9KQKyb+GUaGyKUA==",
      "license": "Apache-2.0",
      "engines": {
        "node": ">=18.18.0"
      }
    },
    "node_modules/@humanfs/node": {
      "version": "0.16.7",
      "resolved": "https://registry.npmjs.org/@humanfs/node/-/node-0.16.7.tgz",
      "integrity": "sha512-/zUx+yOsIrG4Y43Eh2peDeKCxlRt/gET6aHfaKpuq267qXdYDFViVHfMaLyygZOnl0kGWxFIgsBy8QFuTLUXEQ==",
      "license": "Apache-2.0",
      "dependencies": {
        "@humanfs/core": "^0.19.1",
        "@humanwhocodes/retry": "^0.4.0"
      },
      "engines": {
        "node": ">=18.18.0"
      }
    },
    "node_modules/@humanwhocodes/module-importer": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/@humanwhocodes/module-importer/-/module-importer-1.0.1.tgz",
      "integrity": "sha512-bxveV4V8v5Yb4ncFTT3rPSgZBOpCkjfK0y4oVVVJwIuDVBRMDXrPyXRL988i5ap9m9bnyEEjWfm5WkBmtffLfA==",
      "license": "Apache-2.0",
      "engines": {
        "node": ">=12.22"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/nzakas"
      }
    },
    "node_modules/@humanwhocodes/retry": {
      "version": "0.4.3",
      "resolved": "https://registry.npmjs.org/@humanwhocodes/retry/-/retry-0.4.3.tgz",
      "integrity": "sha512-bV0Tgo9K4hfPCek+aMAn81RppFKv2ySDQeMoSZuvTASywNTnVJCArCZE2FWqpvIatKu7VMRLWlR1EazvVhDyhQ==",
      "license": "Apache-2.0",
      "engines": {
        "node": ">=18.18"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/nzakas"
      }
    },
    "node_modules/@jridgewell/gen-mapping": {
      "version": "0.3.13",
      "resolved": "https://registry.npmjs.org/@jridgewell/gen-mapping/-/gen-mapping-0.3.13.tgz",
      "integrity": "sha512-2kkt/7niJ6MgEPxF0bYdQ6etZaA+fQvDcLKckhy1yIQOzaoKjBBjSj63/aLVjYE3qhRt5dvM+uUyfCg6UKCBbA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jridgewell/sourcemap-codec": "^1.5.0",
        "@jridgewell/trace-mapping": "^0.3.24"
      }
    },
    "node_modules/@jridgewell/remapping": {
      "version": "2.3.5",
      "resolved": "https://registry.npmjs.org/@jridgewell/remapping/-/remapping-2.3.5.tgz",
      "integrity": "sha512-LI9u/+laYG4Ds1TDKSJW2YPrIlcVYOwi2fUC6xB43lueCjgxV4lffOCZCtYFiH6TNOX+tQKXx97T4IKHbhyHEQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jridgewell/gen-mapping": "^0.3.5",
        "@jridgewell/trace-mapping": "^0.3.24"
      }
    },
    "node_modules/@jridgewell/resolve-uri": {
      "version": "3.1.2",
      "resolved": "https://registry.npmjs.org/@jridgewell/resolve-uri/-/resolve-uri-3.1.2.tgz",
      "integrity": "sha512-bRISgCIjP20/tbWSPWMEi54QVPRZExkuD9lJL+UIxUKtwVJA8wW1Trb1jMs1RFXo1CBTNZ/5hpC9QvmKWdopKw==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.0.0"
      }
    },
    "node_modules/@jridgewell/sourcemap-codec": {
      "version": "1.5.5",
      "resolved": "https://registry.npmjs.org/@jridgewell/sourcemap-codec/-/sourcemap-codec-1.5.5.tgz",
      "integrity": "sha512-cYQ9310grqxueWbl+WuIUIaiUaDcj7WOq5fVhEljNVgRfOUhY9fy2zTvfoqWsnebh8Sl70VScFbICvJnLKB0Og==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@jridgewell/trace-mapping": {
      "version": "0.3.31",
      "resolved": "https://registry.npmjs.org/@jridgewell/trace-mapping/-/trace-mapping-0.3.31.tgz",
      "integrity": "sha512-zzNR+SdQSDJzc8joaeP8QQoCQr8NuYx2dIIytl1QeBEZHJ9uW6hebsrYgbz8hJwUQao3TWCMtmfV8Nu1twOLAw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jridgewell/resolve-uri": "^3.1.0",
        "@jridgewell/sourcemap-codec": "^1.4.14"
      }
    },
    "node_modules/@reduxjs/toolkit": {
      "version": "2.11.0",
      "resolved": "https://registry.npmjs.org/@reduxjs/toolkit/-/toolkit-2.11.0.tgz",
      "integrity": "sha512-hBjYg0aaRL1O2Z0IqWhnTLytnjDIxekmRxm1snsHjHaKVmIF1HiImWqsq+PuEbn6zdMlkIj9WofK1vR8jjx+Xw==",
      "license": "MIT",
      "dependencies": {
        "@standard-schema/spec": "^1.0.0",
        "@standard-schema/utils": "^0.3.0",
        "immer": "^11.0.0",
        "redux": "^5.0.1",
        "redux-thunk": "^3.1.0",
        "reselect": "^5.1.0"
      },
      "peerDependencies": {
        "react": "^16.9.0 || ^17.0.0 || ^18 || ^19",
        "react-redux": "^7.2.1 || ^8.1.3 || ^9.0.0"
      },
      "peerDependenciesMeta": {
        "react": {
          "optional": true
        },
        "react-redux": {
          "optional": true
        }
      }
    },
    "node_modules/@reduxjs/toolkit/node_modules/immer": {
      "version": "11.0.0",
      "resolved": "https://registry.npmjs.org/immer/-/immer-11.0.0.tgz",
      "integrity": "sha512-XtRG4SINt4dpqlnJvs70O2j6hH7H0X8fUzFsjMn1rwnETaxwp83HLNimXBjZ78MrKl3/d3/pkzDH0o0Lkxm37Q==",
      "license": "MIT",
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/immer"
      }
    },
    "node_modules/@rolldown/pluginutils": {
      "version": "1.0.0-beta.47",
      "resolved": "https://registry.npmjs.org/@rolldown/pluginutils/-/pluginutils-1.0.0-beta.47.tgz",
      "integrity": "sha512-8QagwMH3kNCuzD8EWL8R2YPW5e4OrHNSAHRFDdmFqEwEaD/KcNKjVoumo+gP2vW5eKB2UPbM6vTYiGZX0ixLnw==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@rollup/rollup-android-arm-eabi": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-android-arm-eabi/-/rollup-android-arm-eabi-4.53.3.tgz",
      "integrity": "sha512-mRSi+4cBjrRLoaal2PnqH82Wqyb+d3HsPUN/W+WslCXsZsyHa9ZeQQX/pQsZaVIWDkPcpV6jJ+3KLbTbgnwv8w==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ]
    },
    "node_modules/@rollup/rollup-android-arm64": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-android-arm64/-/rollup-android-arm64-4.53.3.tgz",
      "integrity": "sha512-CbDGaMpdE9sh7sCmTrTUyllhrg65t6SwhjlMJsLr+J8YjFuPmCEjbBSx4Z/e4SmDyH3aB5hGaJUP2ltV/vcs4w==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ]
    },
    "node_modules/@rollup/rollup-darwin-arm64": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-darwin-arm64/-/rollup-darwin-arm64-4.53.3.tgz",
      "integrity": "sha512-Nr7SlQeqIBpOV6BHHGZgYBuSdanCXuw09hon14MGOLGmXAFYjx1wNvquVPmpZnl0tLjg25dEdr4IQ6GgyToCUA==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ]
    },
    "node_modules/@rollup/rollup-darwin-x64": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-darwin-x64/-/rollup-darwin-x64-4.53.3.tgz",
      "integrity": "sha512-DZ8N4CSNfl965CmPktJ8oBnfYr3F8dTTNBQkRlffnUarJ2ohudQD17sZBa097J8xhQ26AwhHJ5mvUyQW8ddTsQ==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ]
    },
    "node_modules/@rollup/rollup-freebsd-arm64": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-freebsd-arm64/-/rollup-freebsd-arm64-4.53.3.tgz",
      "integrity": "sha512-yMTrCrK92aGyi7GuDNtGn2sNW+Gdb4vErx4t3Gv/Tr+1zRb8ax4z8GWVRfr3Jw8zJWvpGHNpss3vVlbF58DZ4w==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "freebsd"
      ]
    },
    "node_modules/@rollup/rollup-freebsd-x64": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-freebsd-x64/-/rollup-freebsd-x64-4.53.3.tgz",
      "integrity": "sha512-lMfF8X7QhdQzseM6XaX0vbno2m3hlyZFhwcndRMw8fbAGUGL3WFMBdK0hbUBIUYcEcMhVLr1SIamDeuLBnXS+Q==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "freebsd"
      ]
    },
    "node_modules/@rollup/rollup-linux-arm-gnueabihf": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-arm-gnueabihf/-/rollup-linux-arm-gnueabihf-4.53.3.tgz",
      "integrity": "sha512-k9oD15soC/Ln6d2Wv/JOFPzZXIAIFLp6B+i14KhxAfnq76ajt0EhYc5YPeX6W1xJkAdItcVT+JhKl1QZh44/qw==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-arm-musleabihf": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-arm-musleabihf/-/rollup-linux-arm-musleabihf-4.53.3.tgz",
      "integrity": "sha512-vTNlKq+N6CK/8UktsrFuc+/7NlEYVxgaEgRXVUVK258Z5ymho29skzW1sutgYjqNnquGwVUObAaxae8rZ6YMhg==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-arm64-gnu": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-arm64-gnu/-/rollup-linux-arm64-gnu-4.53.3.tgz",
      "integrity": "sha512-RGrFLWgMhSxRs/EWJMIFM1O5Mzuz3Xy3/mnxJp/5cVhZ2XoCAxJnmNsEyeMJtpK+wu0FJFWz+QF4mjCA7AUQ3w==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-arm64-musl": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-arm64-musl/-/rollup-linux-arm64-musl-4.53.3.tgz",
      "integrity": "sha512-kASyvfBEWYPEwe0Qv4nfu6pNkITLTb32p4yTgzFCocHnJLAHs+9LjUu9ONIhvfT/5lv4YS5muBHyuV84epBo/A==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-loong64-gnu": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-loong64-gnu/-/rollup-linux-loong64-gnu-4.53.3.tgz",
      "integrity": "sha512-JiuKcp2teLJwQ7vkJ95EwESWkNRFJD7TQgYmCnrPtlu50b4XvT5MOmurWNrCj3IFdyjBQ5p9vnrX4JM6I8OE7g==",
      "cpu": [
        "loong64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-ppc64-gnu": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-ppc64-gnu/-/rollup-linux-ppc64-gnu-4.53.3.tgz",
      "integrity": "sha512-EoGSa8nd6d3T7zLuqdojxC20oBfNT8nexBbB/rkxgKj5T5vhpAQKKnD+h3UkoMuTyXkP5jTjK/ccNRmQrPNDuw==",
      "cpu": [
        "ppc64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-riscv64-gnu": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-riscv64-gnu/-/rollup-linux-riscv64-gnu-4.53.3.tgz",
      "integrity": "sha512-4s+Wped2IHXHPnAEbIB0YWBv7SDohqxobiiPA1FIWZpX+w9o2i4LezzH/NkFUl8LRci/8udci6cLq+jJQlh+0g==",
      "cpu": [
        "riscv64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-riscv64-musl": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-riscv64-musl/-/rollup-linux-riscv64-musl-4.53.3.tgz",
      "integrity": "sha512-68k2g7+0vs2u9CxDt5ktXTngsxOQkSEV/xBbwlqYcUrAVh6P9EgMZvFsnHy4SEiUl46Xf0IObWVbMvPrr2gw8A==",
      "cpu": [
        "riscv64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-s390x-gnu": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-s390x-gnu/-/rollup-linux-s390x-gnu-4.53.3.tgz",
      "integrity": "sha512-VYsFMpULAz87ZW6BVYw3I6sWesGpsP9OPcyKe8ofdg9LHxSbRMd7zrVrr5xi/3kMZtpWL/wC+UIJWJYVX5uTKg==",
      "cpu": [
        "s390x"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-x64-gnu": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-x64-gnu/-/rollup-linux-x64-gnu-4.53.3.tgz",
      "integrity": "sha512-3EhFi1FU6YL8HTUJZ51imGJWEX//ajQPfqWLI3BQq4TlvHy4X0MOr5q3D2Zof/ka0d5FNdPwZXm3Yyib/UEd+w==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-x64-musl": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-x64-musl/-/rollup-linux-x64-musl-4.53.3.tgz",
      "integrity": "sha512-eoROhjcc6HbZCJr+tvVT8X4fW3/5g/WkGvvmwz/88sDtSJzO7r/blvoBDgISDiCjDRZmHpwud7h+6Q9JxFwq1Q==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-openharmony-arm64": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-openharmony-arm64/-/rollup-openharmony-arm64-4.53.3.tgz",
      "integrity": "sha512-OueLAWgrNSPGAdUdIjSWXw+u/02BRTcnfw9PN41D2vq/JSEPnJnVuBgw18VkN8wcd4fjUs+jFHVM4t9+kBSNLw==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "openharmony"
      ]
    },
    "node_modules/@rollup/rollup-win32-arm64-msvc": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-win32-arm64-msvc/-/rollup-win32-arm64-msvc-4.53.3.tgz",
      "integrity": "sha512-GOFuKpsxR/whszbF/bzydebLiXIHSgsEUp6M0JI8dWvi+fFa1TD6YQa4aSZHtpmh2/uAlj/Dy+nmby3TJ3pkTw==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ]
    },
    "node_modules/@rollup/rollup-win32-ia32-msvc": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-win32-ia32-msvc/-/rollup-win32-ia32-msvc-4.53.3.tgz",
      "integrity": "sha512-iah+THLcBJdpfZ1TstDFbKNznlzoxa8fmnFYK4V67HvmuNYkVdAywJSoteUszvBQ9/HqN2+9AZghbajMsFT+oA==",
      "cpu": [
        "ia32"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ]
    },
    "node_modules/@rollup/rollup-win32-x64-gnu": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-win32-x64-gnu/-/rollup-win32-x64-gnu-4.53.3.tgz",
      "integrity": "sha512-J9QDiOIZlZLdcot5NXEepDkstocktoVjkaKUtqzgzpt2yWjGlbYiKyp05rWwk4nypbYUNoFAztEgixoLaSETkg==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ]
    },
    "node_modules/@rollup/rollup-win32-x64-msvc": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-win32-x64-msvc/-/rollup-win32-x64-msvc-4.53.3.tgz",
      "integrity": "sha512-UhTd8u31dXadv0MopwGgNOBpUVROFKWVQgAg5N1ESyCz8AuBcMqm4AuTjrwgQKGDfoFuz02EuMRHQIw/frmYKQ==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ]
    },
    "node_modules/@standard-schema/spec": {
      "version": "1.0.0",
      "resolved": "https://registry.npmjs.org/@standard-schema/spec/-/spec-1.0.0.tgz",
      "integrity": "sha512-m2bOd0f2RT9k8QJx1JN85cZYyH1RqFBdlwtkSlf4tBDYLCiiZnv1fIIwacK6cqwXavOydf0NPToMQgpKq+dVlA==",
      "license": "MIT"
    },
    "node_modules/@standard-schema/utils": {
      "version": "0.3.0",
      "resolved": "https://registry.npmjs.org/@standard-schema/utils/-/utils-0.3.0.tgz",
      "integrity": "sha512-e7Mew686owMaPJVNNLs55PUvgz371nKgwsc4vxE49zsODpJEnxgxRo2y/OKrqueavXgZNMDVj3DdHFlaSAeU8g==",
      "license": "MIT"
    },
    "node_modules/@tailwindcss/node": {
      "version": "4.1.17",
      "resolved": "https://registry.npmjs.org/@tailwindcss/node/-/node-4.1.17.tgz",
      "integrity": "sha512-csIkHIgLb3JisEFQ0vxr2Y57GUNYh447C8xzwj89U/8fdW8LhProdxvnVH6U8M2Y73QKiTIH+LWbK3V2BBZsAg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jridgewell/remapping": "^2.3.4",
        "enhanced-resolve": "^5.18.3",
        "jiti": "^2.6.1",
        "lightningcss": "1.30.2",
        "magic-string": "^0.30.21",
        "source-map-js": "^1.2.1",
        "tailwindcss": "4.1.17"
      }
    },
    "node_modules/@tailwindcss/oxide": {
      "version": "4.1.17",
      "resolved": "https://registry.npmjs.org/@tailwindcss/oxide/-/oxide-4.1.17.tgz",
      "integrity": "sha512-F0F7d01fmkQhsTjXezGBLdrl1KresJTcI3DB8EkScCldyKp3Msz4hub4uyYaVnk88BAS1g5DQjjF6F5qczheLA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">= 10"
      },
      "optionalDependencies": {
        "@tailwindcss/oxide-android-arm64": "4.1.17",
        "@tailwindcss/oxide-darwin-arm64": "4.1.17",
        "@tailwindcss/oxide-darwin-x64": "4.1.17",
        "@tailwindcss/oxide-freebsd-x64": "4.1.17",
        "@tailwindcss/oxide-linux-arm-gnueabihf": "4.1.17",
        "@tailwindcss/oxide-linux-arm64-gnu": "4.1.17",
        "@tailwindcss/oxide-linux-arm64-musl": "4.1.17",
        "@tailwindcss/oxide-linux-x64-gnu": "4.1.17",
        "@tailwindcss/oxide-linux-x64-musl": "4.1.17",
        "@tailwindcss/oxide-wasm32-wasi": "4.1.17",
        "@tailwindcss/oxide-win32-arm64-msvc": "4.1.17",
        "@tailwindcss/oxide-win32-x64-msvc": "4.1.17"
      }
    },
    "node_modules/@tailwindcss/oxide-android-arm64": {
      "version": "4.1.17",
      "resolved": "https://registry.npmjs.org/@tailwindcss/oxide-android-arm64/-/oxide-android-arm64-4.1.17.tgz",
      "integrity": "sha512-BMqpkJHgOZ5z78qqiGE6ZIRExyaHyuxjgrJ6eBO5+hfrfGkuya0lYfw8fRHG77gdTjWkNWEEm+qeG2cDMxArLQ==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ],
      "engines": {
        "node": ">= 10"
      }
    },
    "node_modules/@tailwindcss/oxide-darwin-arm64": {
      "version": "4.1.17",
      "resolved": "https://registry.npmjs.org/@tailwindcss/oxide-darwin-arm64/-/oxide-darwin-arm64-4.1.17.tgz",
      "integrity": "sha512-EquyumkQweUBNk1zGEU/wfZo2qkp/nQKRZM8bUYO0J+Lums5+wl2CcG1f9BgAjn/u9pJzdYddHWBiFXJTcxmOg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": ">= 10"
      }
    },
    "node_modules/@tailwindcss/oxide-darwin-x64": {
      "version": "4.1.17",
      "resolved": "https://registry.npmjs.org/@tailwindcss/oxide-darwin-x64/-/oxide-darwin-x64-4.1.17.tgz",
      "integrity": "sha512-gdhEPLzke2Pog8s12oADwYu0IAw04Y2tlmgVzIN0+046ytcgx8uZmCzEg4VcQh+AHKiS7xaL8kGo/QTiNEGRog==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": ">= 10"
      }
    },
    "node_modules/@tailwindcss/oxide-freebsd-x64": {
      "version": "4.1.17",
      "resolved": "https://registry.npmjs.org/@tailwindcss/oxide-freebsd-x64/-/oxide-freebsd-x64-4.1.17.tgz",
      "integrity": "sha512-hxGS81KskMxML9DXsaXT1H0DyA+ZBIbyG/sSAjWNe2EDl7TkPOBI42GBV3u38itzGUOmFfCzk1iAjDXds8Oh0g==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "freebsd"
      ],
      "engines": {
        "node": ">= 10"
      }
    },
    "node_modules/@tailwindcss/oxide-linux-arm-gnueabihf": {
      "version": "4.1.17",
      "resolved": "https://registry.npmjs.org/@tailwindcss/oxide-linux-arm-gnueabihf/-/oxide-linux-arm-gnueabihf-4.1.17.tgz",
      "integrity": "sha512-k7jWk5E3ldAdw0cNglhjSgv501u7yrMf8oeZ0cElhxU6Y2o7f8yqelOp3fhf7evjIS6ujTI3U8pKUXV2I4iXHQ==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">= 10"
      }
    },
    "node_modules/@tailwindcss/oxide-linux-arm64-gnu": {
      "version": "4.1.17",
      "resolved": "https://registry.npmjs.org/@tailwindcss/oxide-linux-arm64-gnu/-/oxide-linux-arm64-gnu-4.1.17.tgz",
      "integrity": "sha512-HVDOm/mxK6+TbARwdW17WrgDYEGzmoYayrCgmLEw7FxTPLcp/glBisuyWkFz/jb7ZfiAXAXUACfyItn+nTgsdQ==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">= 10"
      }
    },
    "node_modules/@tailwindcss/oxide-linux-arm64-musl": {
      "version": "4.1.17",
      "resolved": "https://registry.npmjs.org/@tailwindcss/oxide-linux-arm64-musl/-/oxide-linux-arm64-musl-4.1.17.tgz",
      "integrity": "sha512-HvZLfGr42i5anKtIeQzxdkw/wPqIbpeZqe7vd3V9vI3RQxe3xU1fLjss0TjyhxWcBaipk7NYwSrwTwK1hJARMg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">= 10"
      }
    },
    "node_modules/@tailwindcss/oxide-linux-x64-gnu": {
      "version": "4.1.17",
      "resolved": "https://registry.npmjs.org/@tailwindcss/oxide-linux-x64-gnu/-/oxide-linux-x64-gnu-4.1.17.tgz",
      "integrity": "sha512-M3XZuORCGB7VPOEDH+nzpJ21XPvK5PyjlkSFkFziNHGLc5d6g3di2McAAblmaSUNl8IOmzYwLx9NsE7bplNkwQ==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">= 10"
      }
    },
    "node_modules/@tailwindcss/oxide-linux-x64-musl": {
      "version": "4.1.17",
      "resolved": "https://registry.npmjs.org/@tailwindcss/oxide-linux-x64-musl/-/oxide-linux-x64-musl-4.1.17.tgz",
      "integrity": "sha512-k7f+pf9eXLEey4pBlw+8dgfJHY4PZ5qOUFDyNf7SI6lHjQ9Zt7+NcscjpwdCEbYi6FI5c2KDTDWyf2iHcCSyyQ==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">= 10"
      }
    },
    "node_modules/@tailwindcss/oxide-wasm32-wasi": {
      "version": "4.1.17",
      "resolved": "https://registry.npmjs.org/@tailwindcss/oxide-wasm32-wasi/-/oxide-wasm32-wasi-4.1.17.tgz",
      "integrity": "sha512-cEytGqSSoy7zK4JRWiTCx43FsKP/zGr0CsuMawhH67ONlH+T79VteQeJQRO/X7L0juEUA8ZyuYikcRBf0vsxhg==",
      "bundleDependencies": [
        "@napi-rs/wasm-runtime",
        "@emnapi/core",
        "@emnapi/runtime",
        "@tybys/wasm-util",
        "@emnapi/wasi-threads",
        "tslib"
      ],
      "cpu": [
        "wasm32"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "@emnapi/core": "^1.6.0",
        "@emnapi/runtime": "^1.6.0",
        "@emnapi/wasi-threads": "^1.1.0",
        "@napi-rs/wasm-runtime": "^1.0.7",
        "@tybys/wasm-util": "^0.10.1",
        "tslib": "^2.4.0"
      },
      "engines": {
        "node": ">=14.0.0"
      }
    },
    "node_modules/@tailwindcss/oxide-win32-arm64-msvc": {
      "version": "4.1.17",
      "resolved": "https://registry.npmjs.org/@tailwindcss/oxide-win32-arm64-msvc/-/oxide-win32-arm64-msvc-4.1.17.tgz",
      "integrity": "sha512-JU5AHr7gKbZlOGvMdb4722/0aYbU+tN6lv1kONx0JK2cGsh7g148zVWLM0IKR3NeKLv+L90chBVYcJ8uJWbC9A==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">= 10"
      }
    },
    "node_modules/@tailwindcss/oxide-win32-x64-msvc": {
      "version": "4.1.17",
      "resolved": "https://registry.npmjs.org/@tailwindcss/oxide-win32-x64-msvc/-/oxide-win32-x64-msvc-4.1.17.tgz",
      "integrity": "sha512-SKWM4waLuqx0IH+FMDUw6R66Hu4OuTALFgnleKbqhgGU30DY20NORZMZUKgLRjQXNN2TLzKvh48QXTig4h4bGw==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">= 10"
      }
    },
    "node_modules/@tailwindcss/postcss": {
      "version": "4.1.17",
      "resolved": "https://registry.npmjs.org/@tailwindcss/postcss/-/postcss-4.1.17.tgz",
      "integrity": "sha512-+nKl9N9mN5uJ+M7dBOOCzINw94MPstNR/GtIhz1fpZysxL/4a+No64jCBD6CPN+bIHWFx3KWuu8XJRrj/572Dw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@alloc/quick-lru": "^5.2.0",
        "@tailwindcss/node": "4.1.17",
        "@tailwindcss/oxide": "4.1.17",
        "postcss": "^8.4.41",
        "tailwindcss": "4.1.17"
      }
    },
    "node_modules/@testing-library/dom": {
      "version": "10.4.1",
      "resolved": "https://registry.npmjs.org/@testing-library/dom/-/dom-10.4.1.tgz",
      "integrity": "sha512-o4PXJQidqJl82ckFaXUeoAW+XysPLauYI43Abki5hABd853iMhitooc6znOnczgbTYmEP6U6/y1ZyKAIsvMKGg==",
      "dev": true,
      "license": "MIT",
      "peer": true,
      "dependencies": {
        "@babel/code-frame": "^7.10.4",
        "@babel/runtime": "^7.12.5",
        "@types/aria-query": "^5.0.1",
        "aria-query": "5.3.0",
        "dom-accessibility-api": "^0.5.9",
        "lz-string": "^1.5.0",
        "picocolors": "1.1.1",
        "pretty-format": "^27.0.2"
      },
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@testing-library/jest-dom": {
      "version": "6.9.1",
      "resolved": "https://registry.npmjs.org/@testing-library/jest-dom/-/jest-dom-6.9.1.tgz",
      "integrity": "sha512-zIcONa+hVtVSSep9UT3jZ5rizo2BsxgyDYU7WFD5eICBE7no3881HGeb/QkGfsJs6JTkY1aQhT7rIPC7e+0nnA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@adobe/css-tools": "^4.4.0",
        "aria-query": "^5.0.0",
        "css.escape": "^1.5.1",
        "dom-accessibility-api": "^0.6.3",
        "picocolors": "^1.1.1",
        "redent": "^3.0.0"
      },
      "engines": {
        "node": ">=14",
        "npm": ">=6",
        "yarn": ">=1"
      }
    },
    "node_modules/@testing-library/jest-dom/node_modules/dom-accessibility-api": {
      "version": "0.6.3",
      "resolved": "https://registry.npmjs.org/dom-accessibility-api/-/dom-accessibility-api-0.6.3.tgz",
      "integrity": "sha512-7ZgogeTnjuHbo+ct10G9Ffp0mif17idi0IyWNVA/wcwcm7NPOD/WEHVP3n7n3MhXqxoIYm8d6MuZohYWIZ4T3w==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@testing-library/react": {
      "version": "16.3.0",
      "resolved": "https://registry.npmjs.org/@testing-library/react/-/react-16.3.0.tgz",
      "integrity": "sha512-kFSyxiEDwv1WLl2fgsq6pPBbw5aWKrsY2/noi1Id0TK0UParSF62oFQFGHXIyaG4pp2tEub/Zlel+fjjZILDsw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/runtime": "^7.12.5"
      },
      "engines": {
        "node": ">=18"
      },
      "peerDependencies": {
        "@testing-library/dom": "^10.0.0",
        "@types/react": "^18.0.0 || ^19.0.0",
        "@types/react-dom": "^18.0.0 || ^19.0.0",
        "react": "^18.0.0 || ^19.0.0",
        "react-dom": "^18.0.0 || ^19.0.0"
      },
      "peerDependenciesMeta": {
        "@types/react": {
          "optional": true
        },
        "@types/react-dom": {
          "optional": true
        }
      }
    },
    "node_modules/@types/aria-query": {
      "version": "5.0.4",
      "resolved": "https://registry.npmjs.org/@types/aria-query/-/aria-query-5.0.4.tgz",
      "integrity": "sha512-rfT93uj5s0PRL7EzccGMs3brplhcrghnDoV26NqKhCAS1hVo+WdNsPvE/yb6ilfr5hi2MEk6d5EWJTKdxg8jVw==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@types/babel__core": {
      "version": "7.20.5",
      "resolved": "https://registry.npmjs.org/@types/babel__core/-/babel__core-7.20.5.tgz",
      "integrity": "sha512-qoQprZvz5wQFJwMDqeseRXWv3rqMvhgpbXFfVyWhbx9X47POIA6i/+dXefEmZKoAgOaTdaIgNSMqMIU61yRyzA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/parser": "^7.20.7",
        "@babel/types": "^7.20.7",
        "@types/babel__generator": "*",
        "@types/babel__template": "*",
        "@types/babel__traverse": "*"
      }
    },
    "node_modules/@types/babel__generator": {
      "version": "7.27.0",
      "resolved": "https://registry.npmjs.org/@types/babel__generator/-/babel__generator-7.27.0.tgz",
      "integrity": "sha512-ufFd2Xi92OAVPYsy+P4n7/U7e68fex0+Ee8gSG9KX7eo084CWiQ4sdxktvdl0bOPupXtVJPY19zk6EwWqUQ8lg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/types": "^7.0.0"
      }
    },
    "node_modules/@types/babel__template": {
      "version": "7.4.4",
      "resolved": "https://registry.npmjs.org/@types/babel__template/-/babel__template-7.4.4.tgz",
      "integrity": "sha512-h/NUaSyG5EyxBIp8YRxo4RMe2/qQgvyowRwVMzhYhBCONbW8PUsg4lkFMrhgZhUe5z3L3MiLDuvyJ/CaPa2A8A==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/parser": "^7.1.0",
        "@babel/types": "^7.0.0"
      }
    },
    "node_modules/@types/babel__traverse": {
      "version": "7.28.0",
      "resolved": "https://registry.npmjs.org/@types/babel__traverse/-/babel__traverse-7.28.0.tgz",
      "integrity": "sha512-8PvcXf70gTDZBgt9ptxJ8elBeBjcLOAcOtoO/mPJjtji1+CdGbHgm77om1GrsPxsiE+uXIpNSK64UYaIwQXd4Q==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/types": "^7.28.2"
      }
    },
    "node_modules/@types/chai": {
      "version": "5.2.3",
      "resolved": "https://registry.npmjs.org/@types/chai/-/chai-5.2.3.tgz",
      "integrity": "sha512-Mw558oeA9fFbv65/y4mHtXDs9bPnFMZAL/jxdPFUpOHHIXX91mcgEHbS5Lahr+pwZFR8A7GQleRWeI6cGFC2UA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@types/deep-eql": "*",
        "assertion-error": "^2.0.1"
      }
    },
    "node_modules/@types/d3-array": {
      "version": "3.2.2",
      "resolved": "https://registry.npmjs.org/@types/d3-array/-/d3-array-3.2.2.tgz",
      "integrity": "sha512-hOLWVbm7uRza0BYXpIIW5pxfrKe0W+D5lrFiAEYR+pb6w3N2SwSMaJbXdUfSEv+dT4MfHBLtn5js0LAWaO6otw==",
      "license": "MIT"
    },
    "node_modules/@types/d3-color": {
      "version": "3.1.3",
      "resolved": "https://registry.npmjs.org/@types/d3-color/-/d3-color-3.1.3.tgz",
      "integrity": "sha512-iO90scth9WAbmgv7ogoq57O9YpKmFBbmoEoCHDB2xMBY0+/KVrqAaCDyCE16dUspeOvIxFFRI+0sEtqDqy2b4A==",
      "license": "MIT"
    },
    "node_modules/@types/d3-ease": {
      "version": "3.0.2",
      "resolved": "https://registry.npmjs.org/@types/d3-ease/-/d3-ease-3.0.2.tgz",
      "integrity": "sha512-NcV1JjO5oDzoK26oMzbILE6HW7uVXOHLQvHshBUW4UMdZGfiY6v5BeQwh9a9tCzv+CeefZQHJt5SRgK154RtiA==",
      "license": "MIT"
    },
    "node_modules/@types/d3-interpolate": {
      "version": "3.0.4",
      "resolved": "https://registry.npmjs.org/@types/d3-interpolate/-/d3-interpolate-3.0.4.tgz",
      "integrity": "sha512-mgLPETlrpVV1YRJIglr4Ez47g7Yxjl1lj7YKsiMCb27VJH9W8NVM6Bb9d8kkpG/uAQS5AmbA48q2IAolKKo1MA==",
      "license": "MIT",
      "dependencies": {
        "@types/d3-color": "*"
      }
    },
    "node_modules/@types/d3-path": {
      "version": "3.1.1",
      "resolved": "https://registry.npmjs.org/@types/d3-path/-/d3-path-3.1.1.tgz",
      "integrity": "sha512-VMZBYyQvbGmWyWVea0EHs/BwLgxc+MKi1zLDCONksozI4YJMcTt8ZEuIR4Sb1MMTE8MMW49v0IwI5+b7RmfWlg==",
      "license": "MIT"
    },
    "node_modules/@types/d3-scale": {
      "version": "4.0.9",
      "resolved": "https://registry.npmjs.org/@types/d3-scale/-/d3-scale-4.0.9.tgz",
      "integrity": "sha512-dLmtwB8zkAeO/juAMfnV+sItKjlsw2lKdZVVy6LRr0cBmegxSABiLEpGVmSJJ8O08i4+sGR6qQtb6WtuwJdvVw==",
      "license": "MIT",
      "dependencies": {
        "@types/d3-time": "*"
      }
    },
    "node_modules/@types/d3-shape": {
      "version": "3.1.7",
      "resolved": "https://registry.npmjs.org/@types/d3-shape/-/d3-shape-3.1.7.tgz",
      "integrity": "sha512-VLvUQ33C+3J+8p+Daf+nYSOsjB4GXp19/S/aGo60m9h1v6XaxjiT82lKVWJCfzhtuZ3yD7i/TPeC/fuKLLOSmg==",
      "license": "MIT",
      "dependencies": {
        "@types/d3-path": "*"
      }
    },
    "node_modules/@types/d3-time": {
      "version": "3.0.4",
      "resolved": "https://registry.npmjs.org/@types/d3-time/-/d3-time-3.0.4.tgz",
      "integrity": "sha512-yuzZug1nkAAaBlBBikKZTgzCeA+k1uy4ZFwWANOfKw5z5LRhV0gNA7gNkKm7HoK+HRN0wX3EkxGk0fpbWhmB7g==",
      "license": "MIT"
    },
    "node_modules/@types/d3-timer": {
      "version": "3.0.2",
      "resolved": "https://registry.npmjs.org/@types/d3-timer/-/d3-timer-3.0.2.tgz",
      "integrity": "sha512-Ps3T8E8dZDam6fUyNiMkekK3XUsaUEik+idO9/YjPtfj2qruF8tFBXS7XhtE4iIXBLxhmLjP3SXpLhVf21I9Lw==",
      "license": "MIT"
    },
    "node_modules/@types/deep-eql": {
      "version": "4.0.2",
      "resolved": "https://registry.npmjs.org/@types/deep-eql/-/deep-eql-4.0.2.tgz",
      "integrity": "sha512-c9h9dVVMigMPc4bwTvC5dxqtqJZwQPePsWjPlpSOnojbor6pGqdk541lfA7AqFQr5pB1BRdq0juY9db81BwyFw==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@types/estree": {
      "version": "1.0.8",
      "resolved": "https://registry.npmjs.org/@types/estree/-/estree-1.0.8.tgz",
      "integrity": "sha512-dWHzHa2WqEXI/O1E9OjrocMTKJl2mSrEolh1Iomrv6U+JuNwaHXsXx9bLu5gG7BUWFIN0skIQJQ/L1rIex4X6w==",
      "license": "MIT"
    },
    "node_modules/@types/json-schema": {
      "version": "7.0.15",
      "resolved": "https://registry.npmjs.org/@types/json-schema/-/json-schema-7.0.15.tgz",
      "integrity": "sha512-5+fP8P8MFNC+AyZCDxrB2pkZFPGzqQWUzpSeuuVLvm8VMcorNYavBqoFcxK8bQz4Qsbn4oUEEem4wDLfcysGHA==",
      "license": "MIT"
    },
    "node_modules/@types/react": {
      "version": "19.2.7",
      "resolved": "https://registry.npmjs.org/@types/react/-/react-19.2.7.tgz",
      "integrity": "sha512-MWtvHrGZLFttgeEj28VXHxpmwYbor/ATPYbBfSFZEIRK0ecCFLl2Qo55z52Hss+UV9CRN7trSeq1zbgx7YDWWg==",
      "devOptional": true,
      "license": "MIT",
      "peer": true,
      "dependencies": {
        "csstype": "^3.2.2"
      }
    },
    "node_modules/@types/react-dom": {
      "version": "19.2.3",
      "resolved": "https://registry.npmjs.org/@types/react-dom/-/react-dom-19.2.3.tgz",
      "integrity": "sha512-jp2L/eY6fn+KgVVQAOqYItbF0VY/YApe5Mz2F0aykSO8gx31bYCZyvSeYxCHKvzHG5eZjc+zyaS5BrBWya2+kQ==",
      "dev": true,
      "license": "MIT",
      "peer": true,
      "peerDependencies": {
        "@types/react": "^19.2.0"
      }
    },
    "node_modules/@types/use-sync-external-store": {
      "version": "0.0.6",
      "resolved": "https://registry.npmjs.org/@types/use-sync-external-store/-/use-sync-external-store-0.0.6.tgz",
      "integrity": "sha512-zFDAD+tlpf2r4asuHEj0XH6pY6i0g5NeAHPn+15wk3BV6JA69eERFXC1gyGThDkVa1zCyKr5jox1+2LbV/AMLg==",
      "license": "MIT"
    },
    "node_modules/@vitejs/plugin-react": {
      "version": "5.1.1",
      "resolved": "https://registry.npmjs.org/@vitejs/plugin-react/-/plugin-react-5.1.1.tgz",
      "integrity": "sha512-WQfkSw0QbQ5aJ2CHYw23ZGkqnRwqKHD/KYsMeTkZzPT4Jcf0DcBxBtwMJxnu6E7oxw5+JC6ZAiePgh28uJ1HBA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/core": "^7.28.5",
        "@babel/plugin-transform-react-jsx-self": "^7.27.1",
        "@babel/plugin-transform-react-jsx-source": "^7.27.1",
        "@rolldown/pluginutils": "1.0.0-beta.47",
        "@types/babel__core": "^7.20.5",
        "react-refresh": "^0.18.0"
      },
      "engines": {
        "node": "^20.19.0 || >=22.12.0"
      },
      "peerDependencies": {
        "vite": "^4.2.0 || ^5.0.0 || ^6.0.0 || ^7.0.0"
      }
    },
    "node_modules/@vitest/expect": {
      "version": "4.0.14",
      "resolved": "https://registry.npmjs.org/@vitest/expect/-/expect-4.0.14.tgz",
      "integrity": "sha512-RHk63V3zvRiYOWAV0rGEBRO820ce17hz7cI2kDmEdfQsBjT2luEKB5tCOc91u1oSQoUOZkSv3ZyzkdkSLD7lKw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@standard-schema/spec": "^1.0.0",
        "@types/chai": "^5.2.2",
        "@vitest/spy": "4.0.14",
        "@vitest/utils": "4.0.14",
        "chai": "^6.2.1",
        "tinyrainbow": "^3.0.3"
      },
      "funding": {
        "url": "https://opencollective.com/vitest"
      }
    },
    "node_modules/@vitest/mocker": {
      "version": "4.0.14",
      "resolved": "https://registry.npmjs.org/@vitest/mocker/-/mocker-4.0.14.tgz",
      "integrity": "sha512-RzS5NujlCzeRPF1MK7MXLiEFpkIXeMdQ+rN3Kk3tDI9j0mtbr7Nmuq67tpkOJQpgyClbOltCXMjLZicJHsH5Cg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@vitest/spy": "4.0.14",
        "estree-walker": "^3.0.3",
        "magic-string": "^0.30.21"
      },
      "funding": {
        "url": "https://opencollective.com/vitest"
      },
      "peerDependencies": {
        "msw": "^2.4.9",
        "vite": "^6.0.0 || ^7.0.0-0"
      },
      "peerDependenciesMeta": {
        "msw": {
          "optional": true
        },
        "vite": {
          "optional": true
        }
      }
    },
    "node_modules/@vitest/pretty-format": {
      "version": "4.0.14",
      "resolved": "https://registry.npmjs.org/@vitest/pretty-format/-/pretty-format-4.0.14.tgz",
      "integrity": "sha512-SOYPgujB6TITcJxgd3wmsLl+wZv+fy3av2PpiPpsWPZ6J1ySUYfScfpIt2Yv56ShJXR2MOA6q2KjKHN4EpdyRQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "tinyrainbow": "^3.0.3"
      },
      "funding": {
        "url": "https://opencollective.com/vitest"
      }
    },
    "node_modules/@vitest/runner": {
      "version": "4.0.14",
      "resolved": "https://registry.npmjs.org/@vitest/runner/-/runner-4.0.14.tgz",
      "integrity": "sha512-BsAIk3FAqxICqREbX8SetIteT8PiaUL/tgJjmhxJhCsigmzzH8xeadtp7LRnTpCVzvf0ib9BgAfKJHuhNllKLw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@vitest/utils": "4.0.14",
        "pathe": "^2.0.3"
      },
      "funding": {
        "url": "https://opencollective.com/vitest"
      }
    },
    "node_modules/@vitest/snapshot": {
      "version": "4.0.14",
      "resolved": "https://registry.npmjs.org/@vitest/snapshot/-/snapshot-4.0.14.tgz",
      "integrity": "sha512-aQVBfT1PMzDSA16Y3Fp45a0q8nKexx6N5Amw3MX55BeTeZpoC08fGqEZqVmPcqN0ueZsuUQ9rriPMhZ3Mu19Ag==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@vitest/pretty-format": "4.0.14",
        "magic-string": "^0.30.21",
        "pathe": "^2.0.3"
      },
      "funding": {
        "url": "https://opencollective.com/vitest"
      }
    },
    "node_modules/@vitest/spy": {
      "version": "4.0.14",
      "resolved": "https://registry.npmjs.org/@vitest/spy/-/spy-4.0.14.tgz",
      "integrity": "sha512-JmAZT1UtZooO0tpY3GRyiC/8W7dCs05UOq9rfsUUgEZEdq+DuHLmWhPsrTt0TiW7WYeL/hXpaE07AZ2RCk44hg==",
      "dev": true,
      "license": "MIT",
      "funding": {
        "url": "https://opencollective.com/vitest"
      }
    },
    "node_modules/@vitest/utils": {
      "version": "4.0.14",
      "resolved": "https://registry.npmjs.org/@vitest/utils/-/utils-4.0.14.tgz",
      "integrity": "sha512-hLqXZKAWNg8pI+SQXyXxWCTOpA3MvsqcbVeNgSi8x/CSN2wi26dSzn1wrOhmCmFjEvN9p8/kLFRHa6PI8jHazw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@vitest/pretty-format": "4.0.14",
        "tinyrainbow": "^3.0.3"
      },
      "funding": {
        "url": "https://opencollective.com/vitest"
      }
    },
    "node_modules/acorn": {
      "version": "8.15.0",
      "resolved": "https://registry.npmjs.org/acorn/-/acorn-8.15.0.tgz",
      "integrity": "sha512-NZyJarBfL7nWwIq+FDL6Zp/yHEhePMNnnJ0y3qfieCrmNvYct8uvtiV41UvlSe6apAfk0fY1FbWx+NwfmpvtTg==",
      "license": "MIT",
      "peer": true,
      "bin": {
        "acorn": "bin/acorn"
      },
      "engines": {
        "node": ">=0.4.0"
      }
    },
    "node_modules/acorn-jsx": {
      "version": "5.3.2",
      "resolved": "https://registry.npmjs.org/acorn-jsx/-/acorn-jsx-5.3.2.tgz",
      "integrity": "sha512-rq9s+JNhf0IChjtDXxllJ7g41oZk5SlXtp0LHwyA5cejwn7vKmKp4pPri6YEePv2PU65sAsegbXtIinmDFDXgQ==",
      "license": "MIT",
      "peerDependencies": {
        "acorn": "^6.0.0 || ^7.0.0 || ^8.0.0"
      }
    },
    "node_modules/agent-base": {
      "version": "7.1.4",
      "resolved": "https://registry.npmjs.org/agent-base/-/agent-base-7.1.4.tgz",
      "integrity": "sha512-MnA+YT8fwfJPgBx3m60MNqakm30XOkyIoH1y6huTQvC0PwZG7ki8NacLBcrPbNoo8vEZy7Jpuk7+jMO+CUovTQ==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">= 14"
      }
    },
    "node_modules/ajv": {
      "version": "6.12.6",
      "resolved": "https://registry.npmjs.org/ajv/-/ajv-6.12.6.tgz",
      "integrity": "sha512-j3fVLgvTo527anyYyJOGTYJbG+vnnQYvE0m5mmkc1TK+nxAppkCLMIL0aZ4dblVCNoGShhm+kzE4ZUykBoMg4g==",
      "license": "MIT",
      "dependencies": {
        "fast-deep-equal": "^3.1.1",
        "fast-json-stable-stringify": "^2.0.0",
        "json-schema-traverse": "^0.4.1",
        "uri-js": "^4.2.2"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/epoberezkin"
      }
    },
    "node_modules/ansi-regex": {
      "version": "5.0.1",
      "resolved": "https://registry.npmjs.org/ansi-regex/-/ansi-regex-5.0.1.tgz",
      "integrity": "sha512-quJQXlTSUGL2LH9SUXo8VwsY4soanhgo6LNSm84E1LBcE8s3O0wpdiRzyR9z/ZZJMlMWv37qOOb9pdJlMUEKFQ==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/ansi-styles": {
      "version": "4.3.0",
      "resolved": "https://registry.npmjs.org/ansi-styles/-/ansi-styles-4.3.0.tgz",
      "integrity": "sha512-zbB9rCJAT1rbjiVDb2hqKFHNYLxgtk8NURxZ3IZwD3F6NtxbXZQCnnSi1Lkx+IDohdPlFp222wVALIheZJQSEg==",
      "license": "MIT",
      "dependencies": {
        "color-convert": "^2.0.1"
      },
      "engines": {
        "node": ">=8"
      },
      "funding": {
        "url": "https://github.com/chalk/ansi-styles?sponsor=1"
      }
    },
    "node_modules/argparse": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/argparse/-/argparse-2.0.1.tgz",
      "integrity": "sha512-8+9WqebbFzpX9OR+Wa6O29asIogeRMzcGtAINdpMHHyAg10f05aSFVBbcEqGf/PXw1EjAZ+q2/bEBg3DvurK3Q==",
      "license": "Python-2.0"
    },
    "node_modules/aria-query": {
      "version": "5.3.0",
      "resolved": "https://registry.npmjs.org/aria-query/-/aria-query-5.3.0.tgz",
      "integrity": "sha512-b0P0sZPKtyu8HkeRAfCq0IfURZK+SuwMjY1UXGBU27wpAiTwQAIlq56IbIO+ytk/JjS1fMR14ee5WBBfKi5J6A==",
      "dev": true,
      "license": "Apache-2.0",
      "dependencies": {
        "dequal": "^2.0.3"
      }
    },
    "node_modules/assertion-error": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/assertion-error/-/assertion-error-2.0.1.tgz",
      "integrity": "sha512-Izi8RQcffqCeNVgFigKli1ssklIbpHnCYc6AknXGYoB6grJqyeby7jv12JUQgmTAnIDnbck1uxksT4dzN3PWBA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/autoprefixer": {
      "version": "10.4.22",
      "resolved": "https://registry.npmjs.org/autoprefixer/-/autoprefixer-10.4.22.tgz",
      "integrity": "sha512-ARe0v/t9gO28Bznv6GgqARmVqcWOV3mfgUPn9becPHMiD3o9BwlRgaeccZnwTpZ7Zwqrm+c1sUSsMxIzQzc8Xg==",
      "dev": true,
      "funding": [
        {
          "type": "opencollective",
          "url": "https://opencollective.com/postcss/"
        },
        {
          "type": "tidelift",
          "url": "https://tidelift.com/funding/github/npm/autoprefixer"
        },
        {
          "type": "github",
          "url": "https://github.com/sponsors/ai"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "browserslist": "^4.27.0",
        "caniuse-lite": "^1.0.30001754",
        "fraction.js": "^5.3.4",
        "normalize-range": "^0.1.2",
        "picocolors": "^1.1.1",
        "postcss-value-parser": "^4.2.0"
      },
      "bin": {
        "autoprefixer": "bin/autoprefixer"
      },
      "engines": {
        "node": "^10 || ^12 || >=14"
      },
      "peerDependencies": {
        "postcss": "^8.1.0"
      }
    },
    "node_modules/balanced-match": {
      "version": "1.0.2",
      "resolved": "https://registry.npmjs.org/balanced-match/-/balanced-match-1.0.2.tgz",
      "integrity": "sha512-3oSeUO0TMV67hN1AmbXsK4yaqU7tjiHlbxRDZOpH0KW9+CeX4bRAaX0Anxt0tx2MrpRpWwQaPwIlISEJhYU5Pw==",
      "license": "MIT"
    },
    "node_modules/baseline-browser-mapping": {
      "version": "2.8.31",
      "resolved": "https://registry.npmjs.org/baseline-browser-mapping/-/baseline-browser-mapping-2.8.31.tgz",
      "integrity": "sha512-a28v2eWrrRWPpJSzxc+mKwm0ZtVx/G8SepdQZDArnXYU/XS+IF6mp8aB/4E+hH1tyGCoDo3KlUCdlSxGDsRkAw==",
      "dev": true,
      "license": "Apache-2.0",
      "bin": {
        "baseline-browser-mapping": "dist/cli.js"
      }
    },
    "node_modules/bidi-js": {
      "version": "1.0.3",
      "resolved": "https://registry.npmjs.org/bidi-js/-/bidi-js-1.0.3.tgz",
      "integrity": "sha512-RKshQI1R3YQ+n9YJz2QQ147P66ELpa1FQEg20Dk8oW9t2KgLbpDLLp9aGZ7y8WHSshDknG0bknqGw5/tyCs5tw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "require-from-string": "^2.0.2"
      }
    },
    "node_modules/brace-expansion": {
      "version": "1.1.12",
      "resolved": "https://registry.npmjs.org/brace-expansion/-/brace-expansion-1.1.12.tgz",
      "integrity": "sha512-9T9UjW3r0UW5c1Q7GTwllptXwhvYmEzFhzMfZ9H7FQWt+uZePjZPjBP/W1ZEyZ1twGWom5/56TF4lPcqjnDHcg==",
      "license": "MIT",
      "dependencies": {
        "balanced-match": "^1.0.0",
        "concat-map": "0.0.1"
      }
    },
    "node_modules/browserslist": {
      "version": "4.28.0",
      "resolved": "https://registry.npmjs.org/browserslist/-/browserslist-4.28.0.tgz",
      "integrity": "sha512-tbydkR/CxfMwelN0vwdP/pLkDwyAASZ+VfWm4EOwlB6SWhx1sYnWLqo8N5j0rAzPfzfRaxt0mM/4wPU/Su84RQ==",
      "dev": true,
      "funding": [
        {
          "type": "opencollective",
          "url": "https://opencollective.com/browserslist"
        },
        {
          "type": "tidelift",
          "url": "https://tidelift.com/funding/github/npm/browserslist"
        },
        {
          "type": "github",
          "url": "https://github.com/sponsors/ai"
        }
      ],
      "license": "MIT",
      "peer": true,
      "dependencies": {
        "baseline-browser-mapping": "^2.8.25",
        "caniuse-lite": "^1.0.30001754",
        "electron-to-chromium": "^1.5.249",
        "node-releases": "^2.0.27",
        "update-browserslist-db": "^1.1.4"
      },
      "bin": {
        "browserslist": "cli.js"
      },
      "engines": {
        "node": "^6 || ^7 || ^8 || ^9 || ^10 || ^11 || ^12 || >=13.7"
      }
    },
    "node_modules/callsites": {
      "version": "3.1.0",
      "resolved": "https://registry.npmjs.org/callsites/-/callsites-3.1.0.tgz",
      "integrity": "sha512-P8BjAsXvZS+VIDUI11hHCQEv74YT67YUi5JJFNWIqL235sBmjX4+qx9Muvls5ivyNENctx46xQLQ3aTuE7ssaQ==",
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/caniuse-lite": {
      "version": "1.0.30001757",
      "resolved": "https://registry.npmjs.org/caniuse-lite/-/caniuse-lite-1.0.30001757.tgz",
      "integrity": "sha512-r0nnL/I28Zi/yjk1el6ilj27tKcdjLsNqAOZr0yVjWPrSQyHgKI2INaEWw21bAQSv2LXRt1XuCS/GomNpWOxsQ==",
      "dev": true,
      "funding": [
        {
          "type": "opencollective",
          "url": "https://opencollective.com/browserslist"
        },
        {
          "type": "tidelift",
          "url": "https://tidelift.com/funding/github/npm/caniuse-lite"
        },
        {
          "type": "github",
          "url": "https://github.com/sponsors/ai"
        }
      ],
      "license": "CC-BY-4.0"
    },
    "node_modules/chai": {
      "version": "6.2.1",
      "resolved": "https://registry.npmjs.org/chai/-/chai-6.2.1.tgz",
      "integrity": "sha512-p4Z49OGG5W/WBCPSS/dH3jQ73kD6tiMmUM+bckNK6Jr5JHMG3k9bg/BvKR8lKmtVBKmOiuVaV2ws8s9oSbwysg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/chalk": {
      "version": "4.1.2",
      "resolved": "https://registry.npmjs.org/chalk/-/chalk-4.1.2.tgz",
      "integrity": "sha512-oKnbhFyRIXpUuez8iBMmyEa4nbj4IOQyuhc/wy9kY7/WVPcwIO9VA668Pu8RkO7+0G76SLROeyw9CpQ061i4mA==",
      "license": "MIT",
      "dependencies": {
        "ansi-styles": "^4.1.0",
        "supports-color": "^7.1.0"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/chalk/chalk?sponsor=1"
      }
    },
    "node_modules/clsx": {
      "version": "2.1.1",
      "resolved": "https://registry.npmjs.org/clsx/-/clsx-2.1.1.tgz",
      "integrity": "sha512-eYm0QWBtUrBWZWG0d386OGAw16Z995PiOVo2B7bjWSbHedGl5e0ZWaq65kOGgUSNesEIDkB9ISbTg/JK9dhCZA==",
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/color-convert": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/color-convert/-/color-convert-2.0.1.tgz",
      "integrity": "sha512-RRECPsj7iu/xb5oKYcsFHSppFNnsj/52OVTRKb4zP5onXwVF3zVmmToNcOfGC+CRDpfK/U584fMg38ZHCaElKQ==",
      "license": "MIT",
      "dependencies": {
        "color-name": "~1.1.4"
      },
      "engines": {
        "node": ">=7.0.0"
      }
    },
    "node_modules/color-name": {
      "version": "1.1.4",
      "resolved": "https://registry.npmjs.org/color-name/-/color-name-1.1.4.tgz",
      "integrity": "sha512-dOy+3AuW3a2wNbZHIuMZpTcgjGuLU/uBL/ubcZF9OXbDo8ff4O8yVp5Bf0efS8uEoYo5q4Fx7dY9OgQGXgAsQA==",
      "license": "MIT"
    },
    "node_modules/concat-map": {
      "version": "0.0.1",
      "resolved": "https://registry.npmjs.org/concat-map/-/concat-map-0.0.1.tgz",
      "integrity": "sha512-/Srv4dswyQNBfohGpz9o6Yb3Gz3SrUDqBH5rTuhGR7ahtlbYKnVxw2bCFMRljaA7EXHaXZ8wsHdodFvbkhKmqg==",
      "license": "MIT"
    },
    "node_modules/convert-source-map": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/convert-source-map/-/convert-source-map-2.0.0.tgz",
      "integrity": "sha512-Kvp459HrV2FEJ1CAsi1Ku+MY3kasH19TFykTz2xWmMeq6bk2NU3XXvfJ+Q61m0xktWwt+1HSYf3JZsTms3aRJg==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/cross-spawn": {
      "version": "7.0.6",
      "resolved": "https://registry.npmjs.org/cross-spawn/-/cross-spawn-7.0.6.tgz",
      "integrity": "sha512-uV2QOWP2nWzsy2aMp8aRibhi9dlzF5Hgh5SHaB9OiTGEyDTiJJyx0uy51QXdyWbtAHNua4XJzUKca3OzKUd3vA==",
      "license": "MIT",
      "dependencies": {
        "path-key": "^3.1.0",
        "shebang-command": "^2.0.0",
        "which": "^2.0.1"
      },
      "engines": {
        "node": ">= 8"
      }
    },
    "node_modules/css-tree": {
      "version": "3.1.0",
      "resolved": "https://registry.npmjs.org/css-tree/-/css-tree-3.1.0.tgz",
      "integrity": "sha512-0eW44TGN5SQXU1mWSkKwFstI/22X2bG1nYzZTYMAWjylYURhse752YgbE4Cx46AC+bAvI+/dYTPRk1LqSUnu6w==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "mdn-data": "2.12.2",
        "source-map-js": "^1.0.1"
      },
      "engines": {
        "node": "^10 || ^12.20.0 || ^14.13.0 || >=15.0.0"
      }
    },
    "node_modules/css.escape": {
      "version": "1.5.1",
      "resolved": "https://registry.npmjs.org/css.escape/-/css.escape-1.5.1.tgz",
      "integrity": "sha512-YUifsXXuknHlUsmlgyY0PKzgPOr7/FjCePfHNt0jxm83wHZi44VDMQ7/fGNkjY3/jV1MC+1CmZbaHzugyeRtpg==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/cssstyle": {
      "version": "5.3.3",
      "resolved": "https://registry.npmjs.org/cssstyle/-/cssstyle-5.3.3.tgz",
      "integrity": "sha512-OytmFH+13/QXONJcC75QNdMtKpceNk3u8ThBjyyYjkEcy/ekBwR1mMAuNvi3gdBPW3N5TlCzQ0WZw8H0lN/bDw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@asamuzakjp/css-color": "^4.0.3",
        "@csstools/css-syntax-patches-for-csstree": "^1.0.14",
        "css-tree": "^3.1.0"
      },
      "engines": {
        "node": ">=20"
      }
    },
    "node_modules/csstype": {
      "version": "3.2.3",
      "resolved": "https://registry.npmjs.org/csstype/-/csstype-3.2.3.tgz",
      "integrity": "sha512-z1HGKcYy2xA8AGQfwrn0PAy+PB7X/GSj3UVJW9qKyn43xWa+gl5nXmU4qqLMRzWVLFC8KusUX8T/0kCiOYpAIQ==",
      "devOptional": true,
      "license": "MIT"
    },
    "node_modules/d3-array": {
      "version": "3.2.4",
      "resolved": "https://registry.npmjs.org/d3-array/-/d3-array-3.2.4.tgz",
      "integrity": "sha512-tdQAmyA18i4J7wprpYq8ClcxZy3SC31QMeByyCFyRt7BVHdREQZ5lpzoe5mFEYZUWe+oq8HBvk9JjpibyEV4Jg==",
      "license": "ISC",
      "dependencies": {
        "internmap": "1 - 2"
      },
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/d3-color": {
      "version": "3.1.0",
      "resolved": "https://registry.npmjs.org/d3-color/-/d3-color-3.1.0.tgz",
      "integrity": "sha512-zg/chbXyeBtMQ1LbD/WSoW2DpC3I0mpmPdW+ynRTj/x2DAWYrIY7qeZIHidozwV24m4iavr15lNwIwLxRmOxhA==",
      "license": "ISC",
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/d3-ease": {
      "version": "3.0.1",
      "resolved": "https://registry.npmjs.org/d3-ease/-/d3-ease-3.0.1.tgz",
      "integrity": "sha512-wR/XK3D3XcLIZwpbvQwQ5fK+8Ykds1ip7A2Txe0yxncXSdq1L9skcG7blcedkOX+ZcgxGAmLX1FrRGbADwzi0w==",
      "license": "BSD-3-Clause",
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/d3-format": {
      "version": "3.1.0",
      "resolved": "https://registry.npmjs.org/d3-format/-/d3-format-3.1.0.tgz",
      "integrity": "sha512-YyUI6AEuY/Wpt8KWLgZHsIU86atmikuoOmCfommt0LYHiQSPjvX2AcFc38PX0CBpr2RCyZhjex+NS/LPOv6YqA==",
      "license": "ISC",
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/d3-interpolate": {
      "version": "3.0.1",
      "resolved": "https://registry.npmjs.org/d3-interpolate/-/d3-interpolate-3.0.1.tgz",
      "integrity": "sha512-3bYs1rOD33uo8aqJfKP3JWPAibgw8Zm2+L9vBKEHJ2Rg+viTR7o5Mmv5mZcieN+FRYaAOWX5SJATX6k1PWz72g==",
      "license": "ISC",
      "dependencies": {
        "d3-color": "1 - 3"
      },
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/d3-path": {
      "version": "3.1.0",
      "resolved": "https://registry.npmjs.org/d3-path/-/d3-path-3.1.0.tgz",
      "integrity": "sha512-p3KP5HCf/bvjBSSKuXid6Zqijx7wIfNW+J/maPs+iwR35at5JCbLUT0LzF1cnjbCHWhqzQTIN2Jpe8pRebIEFQ==",
      "license": "ISC",
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/d3-scale": {
      "version": "4.0.2",
      "resolved": "https://registry.npmjs.org/d3-scale/-/d3-scale-4.0.2.tgz",
      "integrity": "sha512-GZW464g1SH7ag3Y7hXjf8RoUuAFIqklOAq3MRl4OaWabTFJY9PN/E1YklhXLh+OQ3fM9yS2nOkCoS+WLZ6kvxQ==",
      "license": "ISC",
      "dependencies": {
        "d3-array": "2.10.0 - 3",
        "d3-format": "1 - 3",
        "d3-interpolate": "1.2.0 - 3",
        "d3-time": "2.1.1 - 3",
        "d3-time-format": "2 - 4"
      },
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/d3-shape": {
      "version": "3.2.0",
      "resolved": "https://registry.npmjs.org/d3-shape/-/d3-shape-3.2.0.tgz",
      "integrity": "sha512-SaLBuwGm3MOViRq2ABk3eLoxwZELpH6zhl3FbAoJ7Vm1gofKx6El1Ib5z23NUEhF9AsGl7y+dzLe5Cw2AArGTA==",
      "license": "ISC",
      "dependencies": {
        "d3-path": "^3.1.0"
      },
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/d3-time": {
      "version": "3.1.0",
      "resolved": "https://registry.npmjs.org/d3-time/-/d3-time-3.1.0.tgz",
      "integrity": "sha512-VqKjzBLejbSMT4IgbmVgDjpkYrNWUYJnbCGo874u7MMKIWsILRX+OpX/gTk8MqjpT1A/c6HY2dCA77ZN0lkQ2Q==",
      "license": "ISC",
      "dependencies": {
        "d3-array": "2 - 3"
      },
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/d3-time-format": {
      "version": "4.1.0",
      "resolved": "https://registry.npmjs.org/d3-time-format/-/d3-time-format-4.1.0.tgz",
      "integrity": "sha512-dJxPBlzC7NugB2PDLwo9Q8JiTR3M3e4/XANkreKSUxF8vvXKqm1Yfq4Q5dl8budlunRVlUUaDUgFt7eA8D6NLg==",
      "license": "ISC",
      "dependencies": {
        "d3-time": "1 - 3"
      },
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/d3-timer": {
      "version": "3.0.1",
      "resolved": "https://registry.npmjs.org/d3-timer/-/d3-timer-3.0.1.tgz",
      "integrity": "sha512-ndfJ/JxxMd3nw31uyKoY2naivF+r29V+Lc0svZxe1JvvIRmi8hUsrMvdOwgS1o6uBHmiz91geQ0ylPP0aj1VUA==",
      "license": "ISC",
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/data-urls": {
      "version": "6.0.0",
      "resolved": "https://registry.npmjs.org/data-urls/-/data-urls-6.0.0.tgz",
      "integrity": "sha512-BnBS08aLUM+DKamupXs3w2tJJoqU+AkaE/+6vQxi/G/DPmIZFJJp9Dkb1kM03AZx8ADehDUZgsNxju3mPXZYIA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "whatwg-mimetype": "^4.0.0",
        "whatwg-url": "^15.0.0"
      },
      "engines": {
        "node": ">=20"
      }
    },
    "node_modules/debug": {
      "version": "4.4.3",
      "resolved": "https://registry.npmjs.org/debug/-/debug-4.4.3.tgz",
      "integrity": "sha512-RGwwWnwQvkVfavKVt22FGLw+xYSdzARwm0ru6DhTVA3umU5hZc28V3kO4stgYryrTlLpuvgI9GiijltAjNbcqA==",
      "license": "MIT",
      "dependencies": {
        "ms": "^2.1.3"
      },
      "engines": {
        "node": ">=6.0"
      },
      "peerDependenciesMeta": {
        "supports-color": {
          "optional": true
        }
      }
    },
    "node_modules/decimal.js": {
      "version": "10.6.0",
      "resolved": "https://registry.npmjs.org/decimal.js/-/decimal.js-10.6.0.tgz",
      "integrity": "sha512-YpgQiITW3JXGntzdUmyUR1V812Hn8T1YVXhCu+wO3OpS4eU9l4YdD3qjyiKdV6mvV29zapkMeD390UVEf2lkUg==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/decimal.js-light": {
      "version": "2.5.1",
      "resolved": "https://registry.npmjs.org/decimal.js-light/-/decimal.js-light-2.5.1.tgz",
      "integrity": "sha512-qIMFpTMZmny+MMIitAB6D7iVPEorVw6YQRWkvarTkT4tBeSLLiHzcwj6q0MmYSFCiVpiqPJTJEYIrpcPzVEIvg==",
      "license": "MIT"
    },
    "node_modules/deep-is": {
      "version": "0.1.4",
      "resolved": "https://registry.npmjs.org/deep-is/-/deep-is-0.1.4.tgz",
      "integrity": "sha512-oIPzksmTg4/MriiaYGO+okXDT7ztn/w3Eptv/+gSIdMdKsJo0u4CfYNFJPy+4SKMuCqGw2wxnA+URMg3t8a/bQ==",
      "license": "MIT"
    },
    "node_modules/dequal": {
      "version": "2.0.3",
      "resolved": "https://registry.npmjs.org/dequal/-/dequal-2.0.3.tgz",
      "integrity": "sha512-0je+qPKHEMohvfRTCEo3CrPG6cAzAYgmzKyxRiYSSDkS6eGJdyVJm7WaYA5ECaAD9wLB2T4EEeymA5aFVcYXCA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/detect-libc": {
      "version": "2.1.2",
      "resolved": "https://registry.npmjs.org/detect-libc/-/detect-libc-2.1.2.tgz",
      "integrity": "sha512-Btj2BOOO83o3WyH59e8MgXsxEQVcarkUOpEYrubB0urwnN10yQ364rsiByU11nZlqWYZm05i/of7io4mzihBtQ==",
      "dev": true,
      "license": "Apache-2.0",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/dom-accessibility-api": {
      "version": "0.5.16",
      "resolved": "https://registry.npmjs.org/dom-accessibility-api/-/dom-accessibility-api-0.5.16.tgz",
      "integrity": "sha512-X7BJ2yElsnOJ30pZF4uIIDfBEVgF4XEBxL9Bxhy6dnrm5hkzqmsWHGTiHqRiITNhMyFLyAiWndIJP7Z1NTteDg==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/electron-to-chromium": {
      "version": "1.5.261",
      "resolved": "https://registry.npmjs.org/electron-to-chromium/-/electron-to-chromium-1.5.261.tgz",
      "integrity": "sha512-cmyHEWFqEt3ICUNF93ShneOF47DHoSDbLb7E/AonsWcbzg95N+kPXeLNfkdzgTT/vEUcoW76fxbLBkeYtfoM8A==",
      "dev": true,
      "license": "ISC"
    },
    "node_modules/enhanced-resolve": {
      "version": "5.18.3",
      "resolved": "https://registry.npmjs.org/enhanced-resolve/-/enhanced-resolve-5.18.3.tgz",
      "integrity": "sha512-d4lC8xfavMeBjzGr2vECC3fsGXziXZQyJxD868h2M/mBI3PwAuODxAkLkq5HYuvrPYcUtiLzsTo8U3PgX3Ocww==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "graceful-fs": "^4.2.4",
        "tapable": "^2.2.0"
      },
      "engines": {
        "node": ">=10.13.0"
      }
    },
    "node_modules/entities": {
      "version": "6.0.1",
      "resolved": "https://registry.npmjs.org/entities/-/entities-6.0.1.tgz",
      "integrity": "sha512-aN97NXWF6AWBTahfVOIrB/NShkzi5H7F9r1s9mD3cDj4Ko5f2qhhVoYMibXF7GlLveb/D2ioWay8lxI97Ven3g==",
      "dev": true,
      "license": "BSD-2-Clause",
      "engines": {
        "node": ">=0.12"
      },
      "funding": {
        "url": "https://github.com/fb55/entities?sponsor=1"
      }
    },
    "node_modules/es-module-lexer": {
      "version": "1.7.0",
      "resolved": "https://registry.npmjs.org/es-module-lexer/-/es-module-lexer-1.7.0.tgz",
      "integrity": "sha512-jEQoCwk8hyb2AZziIOLhDqpm5+2ww5uIE6lkO/6jcOCusfk6LhMHpXXfBLXTZ7Ydyt0j4VoUQv6uGNYbdW+kBA==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/es-toolkit": {
      "version": "1.42.0",
      "resolved": "https://registry.npmjs.org/es-toolkit/-/es-toolkit-1.42.0.tgz",
      "integrity": "sha512-SLHIyY7VfDJBM8clz4+T2oquwTQxEzu263AyhVK4jREOAwJ+8eebaa4wM3nlvnAqhDrMm2EsA6hWHaQsMPQ1nA==",
      "license": "MIT",
      "workspaces": [
        "docs",
        "benchmarks"
      ]
    },
    "node_modules/esbuild": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/esbuild/-/esbuild-0.25.12.tgz",
      "integrity": "sha512-bbPBYYrtZbkt6Os6FiTLCTFxvq4tt3JKall1vRwshA3fdVztsLAatFaZobhkBC8/BrPetoa0oksYoKXoG4ryJg==",
      "dev": true,
      "hasInstallScript": true,
      "license": "MIT",
      "bin": {
        "esbuild": "bin/esbuild"
      },
      "engines": {
        "node": ">=18"
      },
      "optionalDependencies": {
        "@esbuild/aix-ppc64": "0.25.12",
        "@esbuild/android-arm": "0.25.12",
        "@esbuild/android-arm64": "0.25.12",
        "@esbuild/android-x64": "0.25.12",
        "@esbuild/darwin-arm64": "0.25.12",
        "@esbuild/darwin-x64": "0.25.12",
        "@esbuild/freebsd-arm64": "0.25.12",
        "@esbuild/freebsd-x64": "0.25.12",
        "@esbuild/linux-arm": "0.25.12",
        "@esbuild/linux-arm64": "0.25.12",
        "@esbuild/linux-ia32": "0.25.12",
        "@esbuild/linux-loong64": "0.25.12",
        "@esbuild/linux-mips64el": "0.25.12",
        "@esbuild/linux-ppc64": "0.25.12",
        "@esbuild/linux-riscv64": "0.25.12",
        "@esbuild/linux-s390x": "0.25.12",
        "@esbuild/linux-x64": "0.25.12",
        "@esbuild/netbsd-arm64": "0.25.12",
        "@esbuild/netbsd-x64": "0.25.12",
        "@esbuild/openbsd-arm64": "0.25.12",
        "@esbuild/openbsd-x64": "0.25.12",
        "@esbuild/openharmony-arm64": "0.25.12",
        "@esbuild/sunos-x64": "0.25.12",
        "@esbuild/win32-arm64": "0.25.12",
        "@esbuild/win32-ia32": "0.25.12",
        "@esbuild/win32-x64": "0.25.12"
      }
    },
    "node_modules/escalade": {
      "version": "3.2.0",
      "resolved": "https://registry.npmjs.org/escalade/-/escalade-3.2.0.tgz",
      "integrity": "sha512-WUj2qlxaQtO4g6Pq5c29GTcWGDyd8itL8zTlipgECz3JesAiiOKotd8JU6otB3PACgG6xkJUyVhboMS+bje/jA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/escape-string-regexp": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/escape-string-regexp/-/escape-string-regexp-4.0.0.tgz",
      "integrity": "sha512-TtpcNJ3XAzx3Gq8sWRzJaVajRs0uVxA2YAkdb1jm2YkPz4G6egUFAyA3n5vtEIZefPk5Wa4UXbKuS5fKkJWdgA==",
      "license": "MIT",
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/eslint": {
      "version": "9.39.1",
      "resolved": "https://registry.npmjs.org/eslint/-/eslint-9.39.1.tgz",
      "integrity": "sha512-BhHmn2yNOFA9H9JmmIVKJmd288g9hrVRDkdoIgRCRuSySRUHH7r/DI6aAXW9T1WwUuY3DFgrcaqB+deURBLR5g==",
      "license": "MIT",
      "peer": true,
      "dependencies": {
        "@eslint-community/eslint-utils": "^4.8.0",
        "@eslint-community/regexpp": "^4.12.1",
        "@eslint/config-array": "^0.21.1",
        "@eslint/config-helpers": "^0.4.2",
        "@eslint/core": "^0.17.0",
        "@eslint/eslintrc": "^3.3.1",
        "@eslint/js": "9.39.1",
        "@eslint/plugin-kit": "^0.4.1",
        "@humanfs/node": "^0.16.6",
        "@humanwhocodes/module-importer": "^1.0.1",
        "@humanwhocodes/retry": "^0.4.2",
        "@types/estree": "^1.0.6",
        "ajv": "^6.12.4",
        "chalk": "^4.0.0",
        "cross-spawn": "^7.0.6",
        "debug": "^4.3.2",
        "escape-string-regexp": "^4.0.0",
        "eslint-scope": "^8.4.0",
        "eslint-visitor-keys": "^4.2.1",
        "espree": "^10.4.0",
        "esquery": "^1.5.0",
        "esutils": "^2.0.2",
        "fast-deep-equal": "^3.1.3",
        "file-entry-cache": "^8.0.0",
        "find-up": "^5.0.0",
        "glob-parent": "^6.0.2",
        "ignore": "^5.2.0",
        "imurmurhash": "^0.1.4",
        "is-glob": "^4.0.0",
        "json-stable-stringify-without-jsonify": "^1.0.1",
        "lodash.merge": "^4.6.2",
        "minimatch": "^3.1.2",
        "natural-compare": "^1.4.0",
        "optionator": "^0.9.3"
      },
      "bin": {
        "eslint": "bin/eslint.js"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      },
      "funding": {
        "url": "https://eslint.org/donate"
      },
      "peerDependencies": {
        "jiti": "*"
      },
      "peerDependenciesMeta": {
        "jiti": {
          "optional": true
        }
      }
    },
    "node_modules/eslint-plugin-react-hooks": {
      "version": "7.0.1",
      "resolved": "https://registry.npmjs.org/eslint-plugin-react-hooks/-/eslint-plugin-react-hooks-7.0.1.tgz",
      "integrity": "sha512-O0d0m04evaNzEPoSW+59Mezf8Qt0InfgGIBJnpC0h3NH/WjUAR7BIKUfysC6todmtiZ/A0oUVS8Gce0WhBrHsA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/core": "^7.24.4",
        "@babel/parser": "^7.24.4",
        "hermes-parser": "^0.25.1",
        "zod": "^3.25.0 || ^4.0.0",
        "zod-validation-error": "^3.5.0 || ^4.0.0"
      },
      "engines": {
        "node": ">=18"
      },
      "peerDependencies": {
        "eslint": "^3.0.0 || ^4.0.0 || ^5.0.0 || ^6.0.0 || ^7.0.0 || ^8.0.0-0 || ^9.0.0"
      }
    },
    "node_modules/eslint-plugin-react-perf": {
      "version": "3.3.3",
      "resolved": "https://registry.npmjs.org/eslint-plugin-react-perf/-/eslint-plugin-react-perf-3.3.3.tgz",
      "integrity": "sha512-EzPdxsRJg5IllCAH9ny/3nK7sv9251tvKmi/d3Ouv5KzI8TB3zNhzScxL9wnh9Hvv8GYC5LEtzTauynfOEYiAw==",
      "license": "MIT",
      "engines": {
        "node": ">=6.9.1"
      },
      "peerDependencies": {
        "eslint": "^3.0.0 || ^4.0.0 || ^5.0.0 || ^6.0.0 || ^7.0.0 || ^8.0.0 || ^9.0.0"
      }
    },
    "node_modules/eslint-plugin-react-refresh": {
      "version": "0.4.24",
      "resolved": "https://registry.npmjs.org/eslint-plugin-react-refresh/-/eslint-plugin-react-refresh-0.4.24.tgz",
      "integrity": "sha512-nLHIW7TEq3aLrEYWpVaJ1dRgFR+wLDPN8e8FpYAql/bMV2oBEfC37K0gLEGgv9fy66juNShSMV8OkTqzltcG/w==",
      "dev": true,
      "license": "MIT",
      "peerDependencies": {
        "eslint": ">=8.40"
      }
    },
    "node_modules/eslint-scope": {
      "version": "8.4.0",
      "resolved": "https://registry.npmjs.org/eslint-scope/-/eslint-scope-8.4.0.tgz",
      "integrity": "sha512-sNXOfKCn74rt8RICKMvJS7XKV/Xk9kA7DyJr8mJik3S7Cwgy3qlkkmyS2uQB3jiJg6VNdZd/pDBJu0nvG2NlTg==",
      "license": "BSD-2-Clause",
      "dependencies": {
        "esrecurse": "^4.3.0",
        "estraverse": "^5.2.0"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      },
      "funding": {
        "url": "https://opencollective.com/eslint"
      }
    },
    "node_modules/eslint-visitor-keys": {
      "version": "4.2.1",
      "resolved": "https://registry.npmjs.org/eslint-visitor-keys/-/eslint-visitor-keys-4.2.1.tgz",
      "integrity": "sha512-Uhdk5sfqcee/9H/rCOJikYz67o0a2Tw2hGRPOG2Y1R2dg7brRe1uG0yaNQDHu+TO/uQPF/5eCapvYSmHUjt7JQ==",
      "license": "Apache-2.0",
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      },
      "funding": {
        "url": "https://opencollective.com/eslint"
      }
    },
    "node_modules/espree": {
      "version": "10.4.0",
      "resolved": "https://registry.npmjs.org/espree/-/espree-10.4.0.tgz",
      "integrity": "sha512-j6PAQ2uUr79PZhBjP5C5fhl8e39FmRnOjsD5lGnWrFU8i2G776tBK7+nP8KuQUTTyAZUwfQqXAgrVH5MbH9CYQ==",
      "license": "BSD-2-Clause",
      "dependencies": {
        "acorn": "^8.15.0",
        "acorn-jsx": "^5.3.2",
        "eslint-visitor-keys": "^4.2.1"
      },
      "engines": {
        "node": "^18.18.0 || ^20.9.0 || >=21.1.0"
      },
      "funding": {
        "url": "https://opencollective.com/eslint"
      }
    },
    "node_modules/esquery": {
      "version": "1.6.0",
      "resolved": "https://registry.npmjs.org/esquery/-/esquery-1.6.0.tgz",
      "integrity": "sha512-ca9pw9fomFcKPvFLXhBKUK90ZvGibiGOvRJNbjljY7s7uq/5YO4BOzcYtJqExdx99rF6aAcnRxHmcUHcz6sQsg==",
      "license": "BSD-3-Clause",
      "dependencies": {
        "estraverse": "^5.1.0"
      },
      "engines": {
        "node": ">=0.10"
      }
    },
    "node_modules/esrecurse": {
      "version": "4.3.0",
      "resolved": "https://registry.npmjs.org/esrecurse/-/esrecurse-4.3.0.tgz",
      "integrity": "sha512-KmfKL3b6G+RXvP8N1vr3Tq1kL/oCFgn2NYXEtqP8/L3pKapUA4G8cFVaoF3SU323CD4XypR/ffioHmkti6/Tag==",
      "license": "BSD-2-Clause",
      "dependencies": {
        "estraverse": "^5.2.0"
      },
      "engines": {
        "node": ">=4.0"
      }
    },
    "node_modules/estraverse": {
      "version": "5.3.0",
      "resolved": "https://registry.npmjs.org/estraverse/-/estraverse-5.3.0.tgz",
      "integrity": "sha512-MMdARuVEQziNTeJD8DgMqmhwR11BRQ/cBP+pLtYdSTnf3MIO8fFeiINEbX36ZdNlfU/7A9f3gUw49B3oQsvwBA==",
      "license": "BSD-2-Clause",
      "engines": {
        "node": ">=4.0"
      }
    },
    "node_modules/estree-walker": {
      "version": "3.0.3",
      "resolved": "https://registry.npmjs.org/estree-walker/-/estree-walker-3.0.3.tgz",
      "integrity": "sha512-7RUKfXgSMMkzt6ZuXmqapOurLGPPfgj6l9uRZ7lRGolvk0y2yocc35LdcxKC5PQZdn2DMqioAQ2NoWcrTKmm6g==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@types/estree": "^1.0.0"
      }
    },
    "node_modules/esutils": {
      "version": "2.0.3",
      "resolved": "https://registry.npmjs.org/esutils/-/esutils-2.0.3.tgz",
      "integrity": "sha512-kVscqXk4OCp68SZ0dkgEKVi6/8ij300KBWTJq32P/dYeWTSwK41WyTxalN1eRmA5Z9UU/LX9D7FWSmV9SAYx6g==",
      "license": "BSD-2-Clause",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/eventemitter3": {
      "version": "5.0.1",
      "resolved": "https://registry.npmjs.org/eventemitter3/-/eventemitter3-5.0.1.tgz",
      "integrity": "sha512-GWkBvjiSZK87ELrYOSESUYeVIc9mvLLf/nXalMOS5dYrgZq9o5OVkbZAVM06CVxYsCwH9BDZFPlQTlPA1j4ahA==",
      "license": "MIT"
    },
    "node_modules/expect-type": {
      "version": "1.2.2",
      "resolved": "https://registry.npmjs.org/expect-type/-/expect-type-1.2.2.tgz",
      "integrity": "sha512-JhFGDVJ7tmDJItKhYgJCGLOWjuK9vPxiXoUFLwLDc99NlmklilbiQJwoctZtt13+xMw91MCk/REan6MWHqDjyA==",
      "dev": true,
      "license": "Apache-2.0",
      "engines": {
        "node": ">=12.0.0"
      }
    },
    "node_modules/fast-deep-equal": {
      "version": "3.1.3",
      "resolved": "https://registry.npmjs.org/fast-deep-equal/-/fast-deep-equal-3.1.3.tgz",
      "integrity": "sha512-f3qQ9oQy9j2AhBe/H9VC91wLmKBCCU/gDOnKNAYG5hswO7BLKj09Hc5HYNz9cGI++xlpDCIgDaitVs03ATR84Q==",
      "license": "MIT"
    },
    "node_modules/fast-equals": {
      "version": "4.0.3",
      "resolved": "https://registry.npmjs.org/fast-equals/-/fast-equals-4.0.3.tgz",
      "integrity": "sha512-G3BSX9cfKttjr+2o1O22tYMLq0DPluZnYtq1rXumE1SpL/F/SLIfHx08WYQoWSIpeMYf8sRbJ8++71+v6Pnxfg==",
      "license": "MIT"
    },
    "node_modules/fast-json-stable-stringify": {
      "version": "2.1.0",
      "resolved": "https://registry.npmjs.org/fast-json-stable-stringify/-/fast-json-stable-stringify-2.1.0.tgz",
      "integrity": "sha512-lhd/wF+Lk98HZoTCtlVraHtfh5XYijIjalXck7saUtuanSDyLMxnHhSXEDJqHxD7msR8D0uCmqlkwjCV8xvwHw==",
      "license": "MIT"
    },
    "node_modules/fast-levenshtein": {
      "version": "2.0.6",
      "resolved": "https://registry.npmjs.org/fast-levenshtein/-/fast-levenshtein-2.0.6.tgz",
      "integrity": "sha512-DCXu6Ifhqcks7TZKY3Hxp3y6qphY5SJZmrWMDrKcERSOXWQdMhU9Ig/PYrzyw/ul9jOIyh0N4M0tbC5hodg8dw==",
      "license": "MIT"
    },
    "node_modules/fdir": {
      "version": "6.5.0",
      "resolved": "https://registry.npmjs.org/fdir/-/fdir-6.5.0.tgz",
      "integrity": "sha512-tIbYtZbucOs0BRGqPJkshJUYdL+SDH7dVM8gjy+ERp3WAUjLEFJE+02kanyHtwjWOnwrKYBiwAmM0p4kLJAnXg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=12.0.0"
      },
      "peerDependencies": {
        "picomatch": "^3 || ^4"
      },
      "peerDependenciesMeta": {
        "picomatch": {
          "optional": true
        }
      }
    },
    "node_modules/file-entry-cache": {
      "version": "8.0.0",
      "resolved": "https://registry.npmjs.org/file-entry-cache/-/file-entry-cache-8.0.0.tgz",
      "integrity": "sha512-XXTUwCvisa5oacNGRP9SfNtYBNAMi+RPwBFmblZEF7N7swHYQS6/Zfk7SRwx4D5j3CH211YNRco1DEMNVfZCnQ==",
      "license": "MIT",
      "dependencies": {
        "flat-cache": "^4.0.0"
      },
      "engines": {
        "node": ">=16.0.0"
      }
    },
    "node_modules/find-up": {
      "version": "5.0.0",
      "resolved": "https://registry.npmjs.org/find-up/-/find-up-5.0.0.tgz",
      "integrity": "sha512-78/PXT1wlLLDgTzDs7sjq9hzz0vXD+zn+7wypEe4fXQxCmdmqfGsEPQxmiCSQI3ajFV91bVSsvNtrJRiW6nGng==",
      "license": "MIT",
      "dependencies": {
        "locate-path": "^6.0.0",
        "path-exists": "^4.0.0"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/flat-cache": {
      "version": "4.0.1",
      "resolved": "https://registry.npmjs.org/flat-cache/-/flat-cache-4.0.1.tgz",
      "integrity": "sha512-f7ccFPK3SXFHpx15UIGyRJ/FJQctuKZ0zVuN3frBo4HnK3cay9VEW0R6yPYFHC0AgqhukPzKjq22t5DmAyqGyw==",
      "license": "MIT",
      "dependencies": {
        "flatted": "^3.2.9",
        "keyv": "^4.5.4"
      },
      "engines": {
        "node": ">=16"
      }
    },
    "node_modules/flatted": {
      "version": "3.3.3",
      "resolved": "https://registry.npmjs.org/flatted/-/flatted-3.3.3.tgz",
      "integrity": "sha512-GX+ysw4PBCz0PzosHDepZGANEuFCMLrnRTiEy9McGjmkCQYwRq4A/X786G/fjM/+OjsWSU1ZrY5qyARZmO/uwg==",
      "license": "ISC"
    },
    "node_modules/fraction.js": {
      "version": "5.3.4",
      "resolved": "https://registry.npmjs.org/fraction.js/-/fraction.js-5.3.4.tgz",
      "integrity": "sha512-1X1NTtiJphryn/uLQz3whtY6jK3fTqoE3ohKs0tT+Ujr1W59oopxmoEh7Lu5p6vBaPbgoM0bzveAW4Qi5RyWDQ==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": "*"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/rawify"
      }
    },
    "node_modules/fsevents": {
      "version": "2.3.3",
      "resolved": "https://registry.npmjs.org/fsevents/-/fsevents-2.3.3.tgz",
      "integrity": "sha512-5xoDfX+fL7faATnagmWPpbFtwh/R77WmMMqqHGS65C3vvB0YHrgF+B1YmZ3441tMj5n63k0212XNoJwzlhffQw==",
      "dev": true,
      "hasInstallScript": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": "^8.16.0 || ^10.6.0 || >=11.0.0"
      }
    },
    "node_modules/gensync": {
      "version": "1.0.0-beta.2",
      "resolved": "https://registry.npmjs.org/gensync/-/gensync-1.0.0-beta.2.tgz",
      "integrity": "sha512-3hN7NaskYvMDLQY55gnW3NQ+mesEAepTqlg+VEbj7zzqEMBVNhzcGYYeqFo/TlYz6eQiFcp1HcsCZO+nGgS8zg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/glob-parent": {
      "version": "6.0.2",
      "resolved": "https://registry.npmjs.org/glob-parent/-/glob-parent-6.0.2.tgz",
      "integrity": "sha512-XxwI8EOhVQgWp6iDL+3b0r86f4d6AX6zSU55HfB4ydCEuXLXc5FcYeOu+nnGftS4TEju/11rt4KJPTMgbfmv4A==",
      "license": "ISC",
      "dependencies": {
        "is-glob": "^4.0.3"
      },
      "engines": {
        "node": ">=10.13.0"
      }
    },
    "node_modules/globals": {
      "version": "16.5.0",
      "resolved": "https://registry.npmjs.org/globals/-/globals-16.5.0.tgz",
      "integrity": "sha512-c/c15i26VrJ4IRt5Z89DnIzCGDn9EcebibhAOjw5ibqEHsE1wLUgkPn9RDmNcUKyU87GeaL633nyJ+pplFR2ZQ==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=18"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/graceful-fs": {
      "version": "4.2.11",
      "resolved": "https://registry.npmjs.org/graceful-fs/-/graceful-fs-4.2.11.tgz",
      "integrity": "sha512-RbJ5/jmFcNNCcDV5o9eTnBLJ/HszWV0P73bc+Ff4nS/rJj+YaS6IGyiOL0VoBYX+l1Wrl3k63h/KrH+nhJ0XvQ==",
      "dev": true,
      "license": "ISC"
    },
    "node_modules/has-flag": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/has-flag/-/has-flag-4.0.0.tgz",
      "integrity": "sha512-EykJT/Q1KjTWctppgIAgfSO0tKVuZUjhgMr17kqTumMl6Afv3EISleU7qZUzoXDFTAHTDC4NOoG/ZxU3EvlMPQ==",
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/hermes-estree": {
      "version": "0.25.1",
      "resolved": "https://registry.npmjs.org/hermes-estree/-/hermes-estree-0.25.1.tgz",
      "integrity": "sha512-0wUoCcLp+5Ev5pDW2OriHC2MJCbwLwuRx+gAqMTOkGKJJiBCLjtrvy4PWUGn6MIVefecRpzoOZ/UV6iGdOr+Cw==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/hermes-parser": {
      "version": "0.25.1",
      "resolved": "https://registry.npmjs.org/hermes-parser/-/hermes-parser-0.25.1.tgz",
      "integrity": "sha512-6pEjquH3rqaI6cYAXYPcz9MS4rY6R4ngRgrgfDshRptUZIc3lw0MCIJIGDj9++mfySOuPTHB4nrSW99BCvOPIA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "hermes-estree": "0.25.1"
      }
    },
    "node_modules/html-encoding-sniffer": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/html-encoding-sniffer/-/html-encoding-sniffer-4.0.0.tgz",
      "integrity": "sha512-Y22oTqIU4uuPgEemfz7NDJz6OeKf12Lsu+QC+s3BVpda64lTiMYCyGwg5ki4vFxkMwQdeZDl2adZoqUgdFuTgQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "whatwg-encoding": "^3.1.1"
      },
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/http-proxy-agent": {
      "version": "7.0.2",
      "resolved": "https://registry.npmjs.org/http-proxy-agent/-/http-proxy-agent-7.0.2.tgz",
      "integrity": "sha512-T1gkAiYYDWYx3V5Bmyu7HcfcvL7mUrTWiM6yOfa3PIphViJ/gFPbvidQ+veqSOHci/PxBcDabeUNCzpOODJZig==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "agent-base": "^7.1.0",
        "debug": "^4.3.4"
      },
      "engines": {
        "node": ">= 14"
      }
    },
    "node_modules/https-proxy-agent": {
      "version": "7.0.6",
      "resolved": "https://registry.npmjs.org/https-proxy-agent/-/https-proxy-agent-7.0.6.tgz",
      "integrity": "sha512-vK9P5/iUfdl95AI+JVyUuIcVtd4ofvtrOr3HNtM2yxC9bnMbEdp3x01OhQNnjb8IJYi38VlTE3mBXwcfvywuSw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "agent-base": "^7.1.2",
        "debug": "4"
      },
      "engines": {
        "node": ">= 14"
      }
    },
    "node_modules/iconv-lite": {
      "version": "0.6.3",
      "resolved": "https://registry.npmjs.org/iconv-lite/-/iconv-lite-0.6.3.tgz",
      "integrity": "sha512-4fCk79wshMdzMp2rH06qWrJE4iolqLhCUH+OiuIgU++RB0+94NlDL81atO7GX55uUKueo0txHNtvEyI6D7WdMw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "safer-buffer": ">= 2.1.2 < 3.0.0"
      },
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/ignore": {
      "version": "5.3.2",
      "resolved": "https://registry.npmjs.org/ignore/-/ignore-5.3.2.tgz",
      "integrity": "sha512-hsBTNUqQTDwkWtcdYI2i06Y/nUBEsNEDJKjWdigLvegy8kDuJAS8uRlpkkcQpyEXL0Z/pjDy5HBmMjRCJ2gq+g==",
      "license": "MIT",
      "engines": {
        "node": ">= 4"
      }
    },
    "node_modules/immer": {
      "version": "10.2.0",
      "resolved": "https://registry.npmjs.org/immer/-/immer-10.2.0.tgz",
      "integrity": "sha512-d/+XTN3zfODyjr89gM3mPq1WNX2B8pYsu7eORitdwyA2sBubnTl3laYlBk4sXY5FUa5qTZGBDPJICVbvqzjlbw==",
      "license": "MIT",
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/immer"
      }
    },
    "node_modules/import-fresh": {
      "version": "3.3.1",
      "resolved": "https://registry.npmjs.org/import-fresh/-/import-fresh-3.3.1.tgz",
      "integrity": "sha512-TR3KfrTZTYLPB6jUjfx6MF9WcWrHL9su5TObK4ZkYgBdWKPOFoSoQIdEuTuR82pmtxH2spWG9h6etwfr1pLBqQ==",
      "license": "MIT",
      "dependencies": {
        "parent-module": "^1.0.0",
        "resolve-from": "^4.0.0"
      },
      "engines": {
        "node": ">=6"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/imurmurhash": {
      "version": "0.1.4",
      "resolved": "https://registry.npmjs.org/imurmurhash/-/imurmurhash-0.1.4.tgz",
      "integrity": "sha512-JmXMZ6wuvDmLiHEml9ykzqO6lwFbof0GG4IkcGaENdCRDDmMVnny7s5HsIgHCbaq0w2MyPhDqkhTUgS2LU2PHA==",
      "license": "MIT",
      "engines": {
        "node": ">=0.8.19"
      }
    },
    "node_modules/indent-string": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/indent-string/-/indent-string-4.0.0.tgz",
      "integrity": "sha512-EdDDZu4A2OyIK7Lr/2zG+w5jmbuk1DVBnEwREQvBzspBJkCEbRa8GxU1lghYcaGJCnRWibjDXlq779X1/y5xwg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/internmap": {
      "version": "2.0.3",
      "resolved": "https://registry.npmjs.org/internmap/-/internmap-2.0.3.tgz",
      "integrity": "sha512-5Hh7Y1wQbvY5ooGgPbDaL5iYLAPzMTUrjMulskHLH6wnv/A+1q5rgEaiuqEjB+oxGXIVZs1FF+R/KPN3ZSQYYg==",
      "license": "ISC",
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/is-extglob": {
      "version": "2.1.1",
      "resolved": "https://registry.npmjs.org/is-extglob/-/is-extglob-2.1.1.tgz",
      "integrity": "sha512-SbKbANkN603Vi4jEZv49LeVJMn4yGwsbzZworEoyEiutsN3nJYdbO36zfhGJ6QEDpOZIFkDtnq5JRxmvl3jsoQ==",
      "license": "MIT",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/is-glob": {
      "version": "4.0.3",
      "resolved": "https://registry.npmjs.org/is-glob/-/is-glob-4.0.3.tgz",
      "integrity": "sha512-xelSayHH36ZgE7ZWhli7pW34hNbNl8Ojv5KVmkJD4hBdD3th8Tfk9vYasLM+mXWOZhFkgZfxhLSnrwRr4elSSg==",
      "license": "MIT",
      "dependencies": {
        "is-extglob": "^2.1.1"
      },
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/is-potential-custom-element-name": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/is-potential-custom-element-name/-/is-potential-custom-element-name-1.0.1.tgz",
      "integrity": "sha512-bCYeRA2rVibKZd+s2625gGnGF/t7DSqDs4dP7CrLA1m7jKWz6pps0LpYLJN8Q64HtmPKJ1hrN3nzPNKFEKOUiQ==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/isexe": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/isexe/-/isexe-2.0.0.tgz",
      "integrity": "sha512-RHxMLp9lnKHGHRng9QFhRCMbYAcVpn69smSGcq3f36xjgVVWThj4qqLbTLlq7Ssj8B+fIQ1EuCEGI2lKsyQeIw==",
      "license": "ISC"
    },
    "node_modules/jiti": {
      "version": "2.6.1",
      "resolved": "https://registry.npmjs.org/jiti/-/jiti-2.6.1.tgz",
      "integrity": "sha512-ekilCSN1jwRvIbgeg/57YFh8qQDNbwDb9xT/qu2DAHbFFZUicIl4ygVaAvzveMhMVr3LnpSKTNnwt8PoOfmKhQ==",
      "devOptional": true,
      "license": "MIT",
      "bin": {
        "jiti": "lib/jiti-cli.mjs"
      }
    },
    "node_modules/js-tokens": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/js-tokens/-/js-tokens-4.0.0.tgz",
      "integrity": "sha512-RdJUflcE3cUzKiMqQgsCu06FPu9UdIJO0beYbPhHN4k6apgJtifcoCtT9bcxOpYBtpD2kCM6Sbzg4CausW/PKQ==",
      "license": "MIT"
    },
    "node_modules/js-yaml": {
      "version": "4.1.1",
      "resolved": "https://registry.npmjs.org/js-yaml/-/js-yaml-4.1.1.tgz",
      "integrity": "sha512-qQKT4zQxXl8lLwBtHMWwaTcGfFOZviOJet3Oy/xmGk2gZH677CJM9EvtfdSkgWcATZhj/55JZ0rmy3myCT5lsA==",
      "license": "MIT",
      "dependencies": {
        "argparse": "^2.0.1"
      },
      "bin": {
        "js-yaml": "bin/js-yaml.js"
      }
    },
    "node_modules/jsdom": {
      "version": "27.2.0",
      "resolved": "https://registry.npmjs.org/jsdom/-/jsdom-27.2.0.tgz",
      "integrity": "sha512-454TI39PeRDW1LgpyLPyURtB4Zx1tklSr6+OFOipsxGUH1WMTvk6C65JQdrj455+DP2uJ1+veBEHTGFKWVLFoA==",
      "dev": true,
      "license": "MIT",
      "peer": true,
      "dependencies": {
        "@acemir/cssom": "^0.9.23",
        "@asamuzakjp/dom-selector": "^6.7.4",
        "cssstyle": "^5.3.3",
        "data-urls": "^6.0.0",
        "decimal.js": "^10.6.0",
        "html-encoding-sniffer": "^4.0.0",
        "http-proxy-agent": "^7.0.2",
        "https-proxy-agent": "^7.0.6",
        "is-potential-custom-element-name": "^1.0.1",
        "parse5": "^8.0.0",
        "saxes": "^6.0.0",
        "symbol-tree": "^3.2.4",
        "tough-cookie": "^6.0.0",
        "w3c-xmlserializer": "^5.0.0",
        "webidl-conversions": "^8.0.0",
        "whatwg-encoding": "^3.1.1",
        "whatwg-mimetype": "^4.0.0",
        "whatwg-url": "^15.1.0",
        "ws": "^8.18.3",
        "xml-name-validator": "^5.0.0"
      },
      "engines": {
        "node": "^20.19.0 || ^22.12.0 || >=24.0.0"
      },
      "peerDependencies": {
        "canvas": "^3.0.0"
      },
      "peerDependenciesMeta": {
        "canvas": {
          "optional": true
        }
      }
    },
    "node_modules/jsesc": {
      "version": "3.1.0",
      "resolved": "https://registry.npmjs.org/jsesc/-/jsesc-3.1.0.tgz",
      "integrity": "sha512-/sM3dO2FOzXjKQhJuo0Q173wf2KOo8t4I8vHy6lF9poUp7bKT0/NHE8fPX23PwfhnykfqnC2xRxOnVw5XuGIaA==",
      "dev": true,
      "license": "MIT",
      "bin": {
        "jsesc": "bin/jsesc"
      },
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/json-buffer": {
      "version": "3.0.1",
      "resolved": "https://registry.npmjs.org/json-buffer/-/json-buffer-3.0.1.tgz",
      "integrity": "sha512-4bV5BfR2mqfQTJm+V5tPPdf+ZpuhiIvTuAB5g8kcrXOZpTT/QwwVRWBywX1ozr6lEuPdbHxwaJlm9G6mI2sfSQ==",
      "license": "MIT"
    },
    "node_modules/json-schema-traverse": {
      "version": "0.4.1",
      "resolved": "https://registry.npmjs.org/json-schema-traverse/-/json-schema-traverse-0.4.1.tgz",
      "integrity": "sha512-xbbCH5dCYU5T8LcEhhuh7HJ88HXuW3qsI3Y0zOZFKfZEHcpWiHU/Jxzk629Brsab/mMiHQti9wMP+845RPe3Vg==",
      "license": "MIT"
    },
    "node_modules/json-stable-stringify-without-jsonify": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/json-stable-stringify-without-jsonify/-/json-stable-stringify-without-jsonify-1.0.1.tgz",
      "integrity": "sha512-Bdboy+l7tA3OGW6FjyFHWkP5LuByj1Tk33Ljyq0axyzdk9//JSi2u3fP1QSmd1KNwq6VOKYGlAu87CisVir6Pw==",
      "license": "MIT"
    },
    "node_modules/json5": {
      "version": "2.2.3",
      "resolved": "https://registry.npmjs.org/json5/-/json5-2.2.3.tgz",
      "integrity": "sha512-XmOWe7eyHYH14cLdVPoyg+GOH3rYX++KpzrylJwSW98t3Nk+U8XOl8FWKOgwtzdb8lXGf6zYwDUzeHMWfxasyg==",
      "dev": true,
      "license": "MIT",
      "bin": {
        "json5": "lib/cli.js"
      },
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/keyv": {
      "version": "4.5.4",
      "resolved": "https://registry.npmjs.org/keyv/-/keyv-4.5.4.tgz",
      "integrity": "sha512-oxVHkHR/EJf2CNXnWxRLW6mg7JyCCUcG0DtEGmL2ctUo1PNTin1PUil+r/+4r5MpVgC/fn1kjsx7mjSujKqIpw==",
      "license": "MIT",
      "dependencies": {
        "json-buffer": "3.0.1"
      }
    },
    "node_modules/levn": {
      "version": "0.4.1",
      "resolved": "https://registry.npmjs.org/levn/-/levn-0.4.1.tgz",
      "integrity": "sha512-+bT2uH4E5LGE7h/n3evcS/sQlJXCpIp6ym8OWJ5eV6+67Dsql/LaaT7qJBAt2rzfoa/5QBGBhxDix1dMt2kQKQ==",
      "license": "MIT",
      "dependencies": {
        "prelude-ls": "^1.2.1",
        "type-check": "~0.4.0"
      },
      "engines": {
        "node": ">= 0.8.0"
      }
    },
    "node_modules/lightningcss": {
      "version": "1.30.2",
      "resolved": "https://registry.npmjs.org/lightningcss/-/lightningcss-1.30.2.tgz",
      "integrity": "sha512-utfs7Pr5uJyyvDETitgsaqSyjCb2qNRAtuqUeWIAKztsOYdcACf2KtARYXg2pSvhkt+9NfoaNY7fxjl6nuMjIQ==",
      "dev": true,
      "license": "MPL-2.0",
      "dependencies": {
        "detect-libc": "^2.0.3"
      },
      "engines": {
        "node": ">= 12.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/parcel"
      },
      "optionalDependencies": {
        "lightningcss-android-arm64": "1.30.2",
        "lightningcss-darwin-arm64": "1.30.2",
        "lightningcss-darwin-x64": "1.30.2",
        "lightningcss-freebsd-x64": "1.30.2",
        "lightningcss-linux-arm-gnueabihf": "1.30.2",
        "lightningcss-linux-arm64-gnu": "1.30.2",
        "lightningcss-linux-arm64-musl": "1.30.2",
        "lightningcss-linux-x64-gnu": "1.30.2",
        "lightningcss-linux-x64-musl": "1.30.2",
        "lightningcss-win32-arm64-msvc": "1.30.2",
        "lightningcss-win32-x64-msvc": "1.30.2"
      }
    },
    "node_modules/lightningcss-android-arm64": {
      "version": "1.30.2",
      "resolved": "https://registry.npmjs.org/lightningcss-android-arm64/-/lightningcss-android-arm64-1.30.2.tgz",
      "integrity": "sha512-BH9sEdOCahSgmkVhBLeU7Hc9DWeZ1Eb6wNS6Da8igvUwAe0sqROHddIlvU06q3WyXVEOYDZ6ykBZQnjTbmo4+A==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MPL-2.0",
      "optional": true,
      "os": [
        "android"
      ],
      "engines": {
        "node": ">= 12.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/parcel"
      }
    },
    "node_modules/lightningcss-darwin-arm64": {
      "version": "1.30.2",
      "resolved": "https://registry.npmjs.org/lightningcss-darwin-arm64/-/lightningcss-darwin-arm64-1.30.2.tgz",
      "integrity": "sha512-ylTcDJBN3Hp21TdhRT5zBOIi73P6/W0qwvlFEk22fkdXchtNTOU4Qc37SkzV+EKYxLouZ6M4LG9NfZ1qkhhBWA==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MPL-2.0",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": ">= 12.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/parcel"
      }
    },
    "node_modules/lightningcss-darwin-x64": {
      "version": "1.30.2",
      "resolved": "https://registry.npmjs.org/lightningcss-darwin-x64/-/lightningcss-darwin-x64-1.30.2.tgz",
      "integrity": "sha512-oBZgKchomuDYxr7ilwLcyms6BCyLn0z8J0+ZZmfpjwg9fRVZIR5/GMXd7r9RH94iDhld3UmSjBM6nXWM2TfZTQ==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MPL-2.0",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": ">= 12.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/parcel"
      }
    },
    "node_modules/lightningcss-freebsd-x64": {
      "version": "1.30.2",
      "resolved": "https://registry.npmjs.org/lightningcss-freebsd-x64/-/lightningcss-freebsd-x64-1.30.2.tgz",
      "integrity": "sha512-c2bH6xTrf4BDpK8MoGG4Bd6zAMZDAXS569UxCAGcA7IKbHNMlhGQ89eRmvpIUGfKWNVdbhSbkQaWhEoMGmGslA==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MPL-2.0",
      "optional": true,
      "os": [
        "freebsd"
      ],
      "engines": {
        "node": ">= 12.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/parcel"
      }
    },
    "node_modules/lightningcss-linux-arm-gnueabihf": {
      "version": "1.30.2",
      "resolved": "https://registry.npmjs.org/lightningcss-linux-arm-gnueabihf/-/lightningcss-linux-arm-gnueabihf-1.30.2.tgz",
      "integrity": "sha512-eVdpxh4wYcm0PofJIZVuYuLiqBIakQ9uFZmipf6LF/HRj5Bgm0eb3qL/mr1smyXIS1twwOxNWndd8z0E374hiA==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MPL-2.0",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">= 12.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/parcel"
      }
    },
    "node_modules/lightningcss-linux-arm64-gnu": {
      "version": "1.30.2",
      "resolved": "https://registry.npmjs.org/lightningcss-linux-arm64-gnu/-/lightningcss-linux-arm64-gnu-1.30.2.tgz",
      "integrity": "sha512-UK65WJAbwIJbiBFXpxrbTNArtfuznvxAJw4Q2ZGlU8kPeDIWEX1dg3rn2veBVUylA2Ezg89ktszWbaQnxD/e3A==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MPL-2.0",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">= 12.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/parcel"
      }
    },
    "node_modules/lightningcss-linux-arm64-musl": {
      "version": "1.30.2",
      "resolved": "https://registry.npmjs.org/lightningcss-linux-arm64-musl/-/lightningcss-linux-arm64-musl-1.30.2.tgz",
      "integrity": "sha512-5Vh9dGeblpTxWHpOx8iauV02popZDsCYMPIgiuw97OJ5uaDsL86cnqSFs5LZkG3ghHoX5isLgWzMs+eD1YzrnA==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MPL-2.0",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">= 12.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/parcel"
      }
    },
    "node_modules/lightningcss-linux-x64-gnu": {
      "version": "1.30.2",
      "resolved": "https://registry.npmjs.org/lightningcss-linux-x64-gnu/-/lightningcss-linux-x64-gnu-1.30.2.tgz",
      "integrity": "sha512-Cfd46gdmj1vQ+lR6VRTTadNHu6ALuw2pKR9lYq4FnhvgBc4zWY1EtZcAc6EffShbb1MFrIPfLDXD6Xprbnni4w==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MPL-2.0",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">= 12.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/parcel"
      }
    },
    "node_modules/lightningcss-linux-x64-musl": {
      "version": "1.30.2",
      "resolved": "https://registry.npmjs.org/lightningcss-linux-x64-musl/-/lightningcss-linux-x64-musl-1.30.2.tgz",
      "integrity": "sha512-XJaLUUFXb6/QG2lGIW6aIk6jKdtjtcffUT0NKvIqhSBY3hh9Ch+1LCeH80dR9q9LBjG3ewbDjnumefsLsP6aiA==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MPL-2.0",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">= 12.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/parcel"
      }
    },
    "node_modules/lightningcss-win32-arm64-msvc": {
      "version": "1.30.2",
      "resolved": "https://registry.npmjs.org/lightningcss-win32-arm64-msvc/-/lightningcss-win32-arm64-msvc-1.30.2.tgz",
      "integrity": "sha512-FZn+vaj7zLv//D/192WFFVA0RgHawIcHqLX9xuWiQt7P0PtdFEVaxgF9rjM/IRYHQXNnk61/H/gb2Ei+kUQ4xQ==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MPL-2.0",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">= 12.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/parcel"
      }
    },
    "node_modules/lightningcss-win32-x64-msvc": {
      "version": "1.30.2",
      "resolved": "https://registry.npmjs.org/lightningcss-win32-x64-msvc/-/lightningcss-win32-x64-msvc-1.30.2.tgz",
      "integrity": "sha512-5g1yc73p+iAkid5phb4oVFMB45417DkRevRbt/El/gKXJk4jid+vPFF/AXbxn05Aky8PapwzZrdJShv5C0avjw==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MPL-2.0",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">= 12.0.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/parcel"
      }
    },
    "node_modules/locate-path": {
      "version": "6.0.0",
      "resolved": "https://registry.npmjs.org/locate-path/-/locate-path-6.0.0.tgz",
      "integrity": "sha512-iPZK6eYjbxRu3uB4/WZ3EsEIMJFMqAoopl3R+zuq0UjcAm/MO6KCweDgPfP3elTztoKP3KtnVHxTn2NHBSDVUw==",
      "license": "MIT",
      "dependencies": {
        "p-locate": "^5.0.0"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/lodash": {
      "version": "4.17.21",
      "resolved": "https://registry.npmjs.org/lodash/-/lodash-4.17.21.tgz",
      "integrity": "sha512-v2kDEe57lecTulaDIuNTPy3Ry4gLGJ6Z1O3vE1krgXZNrsQ+LFTGHVxVjcXPs17LhbZVGedAJv8XZ1tvj5FvSg==",
      "license": "MIT"
    },
    "node_modules/lodash.merge": {
      "version": "4.6.2",
      "resolved": "https://registry.npmjs.org/lodash.merge/-/lodash.merge-4.6.2.tgz",
      "integrity": "sha512-0KpjqXRVvrYyCsX1swR/XTK0va6VQkQM6MNo7PqW77ByjAhoARA8EfrP1N4+KlKj8YS0ZUCtRT/YUuhyYDujIQ==",
      "license": "MIT"
    },
    "node_modules/loose-envify": {
      "version": "1.4.0",
      "resolved": "https://registry.npmjs.org/loose-envify/-/loose-envify-1.4.0.tgz",
      "integrity": "sha512-lyuxPGr/Wfhrlem2CL/UcnUc1zcqKAImBDzukY7Y5F/yQiNdko6+fRLevlw1HgMySw7f611UIY408EtxRSoK3Q==",
      "license": "MIT",
      "dependencies": {
        "js-tokens": "^3.0.0 || ^4.0.0"
      },
      "bin": {
        "loose-envify": "cli.js"
      }
    },
    "node_modules/lru-cache": {
      "version": "5.1.1",
      "resolved": "https://registry.npmjs.org/lru-cache/-/lru-cache-5.1.1.tgz",
      "integrity": "sha512-KpNARQA3Iwv+jTA0utUVVbrh+Jlrr1Fv0e56GGzAFOXN7dk/FviaDW8LHmK52DlcH4WP2n6gI8vN1aesBFgo9w==",
      "dev": true,
      "license": "ISC",
      "dependencies": {
        "yallist": "^3.0.2"
      }
    },
    "node_modules/lucide-react": {
      "version": "0.555.0",
      "resolved": "https://registry.npmjs.org/lucide-react/-/lucide-react-0.555.0.tgz",
      "integrity": "sha512-D8FvHUGbxWBRQM90NZeIyhAvkFfsh3u9ekrMvJ30Z6gnpBHS6HC6ldLg7tL45hwiIz/u66eKDtdA23gwwGsAHA==",
      "license": "ISC",
      "peerDependencies": {
        "react": "^16.5.1 || ^17.0.0 || ^18.0.0 || ^19.0.0"
      }
    },
    "node_modules/lz-string": {
      "version": "1.5.0",
      "resolved": "https://registry.npmjs.org/lz-string/-/lz-string-1.5.0.tgz",
      "integrity": "sha512-h5bgJWpxJNswbU7qCrV0tIKQCaS3blPDrqKWx+QxzuzL1zGUzij9XCWLrSLsJPu5t+eWA/ycetzYAO5IOMcWAQ==",
      "dev": true,
      "license": "MIT",
      "bin": {
        "lz-string": "bin/bin.js"
      }
    },
    "node_modules/magic-string": {
      "version": "0.30.21",
      "resolved": "https://registry.npmjs.org/magic-string/-/magic-string-0.30.21.tgz",
      "integrity": "sha512-vd2F4YUyEXKGcLHoq+TEyCjxueSeHnFxyyjNp80yg0XV4vUhnDer/lvvlqM/arB5bXQN5K2/3oinyCRyx8T2CQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jridgewell/sourcemap-codec": "^1.5.5"
      }
    },
    "node_modules/mdn-data": {
      "version": "2.12.2",
      "resolved": "https://registry.npmjs.org/mdn-data/-/mdn-data-2.12.2.tgz",
      "integrity": "sha512-IEn+pegP1aManZuckezWCO+XZQDplx1366JoVhTpMpBB1sPey/SbveZQUosKiKiGYjg1wH4pMlNgXbCiYgihQA==",
      "dev": true,
      "license": "CC0-1.0"
    },
    "node_modules/min-indent": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/min-indent/-/min-indent-1.0.1.tgz",
      "integrity": "sha512-I9jwMn07Sy/IwOj3zVkVik2JTvgpaykDZEigL6Rx6N9LbMywwUSMtxET+7lVoDLLd3O3IXwJwvuuns8UB/HeAg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=4"
      }
    },
    "node_modules/minimatch": {
      "version": "3.1.2",
      "resolved": "https://registry.npmjs.org/minimatch/-/minimatch-3.1.2.tgz",
      "integrity": "sha512-J7p63hRiAjw1NDEww1W7i37+ByIrOWO5XQQAzZ3VOcL0PNybwpfmV/N05zFAzwQ9USyEcX6t3UO+K5aqBQOIHw==",
      "license": "ISC",
      "dependencies": {
        "brace-expansion": "^1.1.7"
      },
      "engines": {
        "node": "*"
      }
    },
    "node_modules/ms": {
      "version": "2.1.3",
      "resolved": "https://registry.npmjs.org/ms/-/ms-2.1.3.tgz",
      "integrity": "sha512-6FlzubTLZG3J2a/NVCAleEhjzq5oxgHyaCU9yYXvcLsvoVaHJq/s5xXI6/XXP6tz7R9xAOtHnSO/tXtF3WRTlA==",
      "license": "MIT"
    },
    "node_modules/nanoid": {
      "version": "3.3.11",
      "resolved": "https://registry.npmjs.org/nanoid/-/nanoid-3.3.11.tgz",
      "integrity": "sha512-N8SpfPUnUp1bK+PMYW8qSWdl9U+wwNWI4QKxOYDy9JAro3WMX7p2OeVRF9v+347pnakNevPmiHhNmZ2HbFA76w==",
      "dev": true,
      "funding": [
        {
          "type": "github",
          "url": "https://github.com/sponsors/ai"
        }
      ],
      "license": "MIT",
      "bin": {
        "nanoid": "bin/nanoid.cjs"
      },
      "engines": {
        "node": "^10 || ^12 || ^13.7 || ^14 || >=15.0.1"
      }
    },
    "node_modules/natural-compare": {
      "version": "1.4.0",
      "resolved": "https://registry.npmjs.org/natural-compare/-/natural-compare-1.4.0.tgz",
      "integrity": "sha512-OWND8ei3VtNC9h7V60qff3SVobHr996CTwgxubgyQYEpg290h9J0buyECNNJexkFm5sOajh5G116RYA1c8ZMSw==",
      "license": "MIT"
    },
    "node_modules/node-releases": {
      "version": "2.0.27",
      "resolved": "https://registry.npmjs.org/node-releases/-/node-releases-2.0.27.tgz",
      "integrity": "sha512-nmh3lCkYZ3grZvqcCH+fjmQ7X+H0OeZgP40OierEaAptX4XofMh5kwNbWh7lBduUzCcV/8kZ+NDLCwm2iorIlA==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/normalize-range": {
      "version": "0.1.2",
      "resolved": "https://registry.npmjs.org/normalize-range/-/normalize-range-0.1.2.tgz",
      "integrity": "sha512-bdok/XvKII3nUpklnV6P2hxtMNrCboOjAcyBuQnWEhO665FwrSNRxU+AqpsyvO6LgGYPspN+lu5CLtw4jPRKNA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/object-assign": {
      "version": "4.1.1",
      "resolved": "https://registry.npmjs.org/object-assign/-/object-assign-4.1.1.tgz",
      "integrity": "sha512-rJgTQnkUnH1sFw8yT6VSU3zD3sWmu6sZhIseY8VX+GRu3P6F7Fu+JNDoXfklElbLJSnc3FUQHVe4cU5hj+BcUg==",
      "license": "MIT",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/obug": {
      "version": "2.1.1",
      "resolved": "https://registry.npmjs.org/obug/-/obug-2.1.1.tgz",
      "integrity": "sha512-uTqF9MuPraAQ+IsnPf366RG4cP9RtUi7MLO1N3KEc+wb0a6yKpeL0lmk2IB1jY5KHPAlTc6T/JRdC/YqxHNwkQ==",
      "dev": true,
      "funding": [
        "https://github.com/sponsors/sxzz",
        "https://opencollective.com/debug"
      ],
      "license": "MIT"
    },
    "node_modules/optionator": {
      "version": "0.9.4",
      "resolved": "https://registry.npmjs.org/optionator/-/optionator-0.9.4.tgz",
      "integrity": "sha512-6IpQ7mKUxRcZNLIObR0hz7lxsapSSIYNZJwXPGeF0mTVqGKFIXj1DQcMoT22S3ROcLyY/rz0PWaWZ9ayWmad9g==",
      "license": "MIT",
      "dependencies": {
        "deep-is": "^0.1.3",
        "fast-levenshtein": "^2.0.6",
        "levn": "^0.4.1",
        "prelude-ls": "^1.2.1",
        "type-check": "^0.4.0",
        "word-wrap": "^1.2.5"
      },
      "engines": {
        "node": ">= 0.8.0"
      }
    },
    "node_modules/p-limit": {
      "version": "3.1.0",
      "resolved": "https://registry.npmjs.org/p-limit/-/p-limit-3.1.0.tgz",
      "integrity": "sha512-TYOanM3wGwNGsZN2cVTYPArw454xnXj5qmWF1bEoAc4+cU/ol7GVh7odevjp1FNHduHc3KZMcFduxU5Xc6uJRQ==",
      "license": "MIT",
      "dependencies": {
        "yocto-queue": "^0.1.0"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/p-locate": {
      "version": "5.0.0",
      "resolved": "https://registry.npmjs.org/p-locate/-/p-locate-5.0.0.tgz",
      "integrity": "sha512-LaNjtRWUBY++zB5nE/NwcaoMylSPk+S+ZHNB1TzdbMJMny6dynpAGt7X/tl/QYq3TIeE6nxHppbo2LGymrG5Pw==",
      "license": "MIT",
      "dependencies": {
        "p-limit": "^3.0.2"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/parent-module": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/parent-module/-/parent-module-1.0.1.tgz",
      "integrity": "sha512-GQ2EWRpQV8/o+Aw8YqtfZZPfNRWZYkbidE9k5rpl/hC3vtHHBfGm2Ifi6qWV+coDGkrUKZAxE3Lot5kcsRlh+g==",
      "license": "MIT",
      "dependencies": {
        "callsites": "^3.0.0"
      },
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/parse5": {
      "version": "8.0.0",
      "resolved": "https://registry.npmjs.org/parse5/-/parse5-8.0.0.tgz",
      "integrity": "sha512-9m4m5GSgXjL4AjumKzq1Fgfp3Z8rsvjRNbnkVwfu2ImRqE5D0LnY2QfDen18FSY9C573YU5XxSapdHZTZ2WolA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "entities": "^6.0.0"
      },
      "funding": {
        "url": "https://github.com/inikulin/parse5?sponsor=1"
      }
    },
    "node_modules/path-exists": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/path-exists/-/path-exists-4.0.0.tgz",
      "integrity": "sha512-ak9Qy5Q7jYb2Wwcey5Fpvg2KoAc/ZIhLSLOSBmRmygPsGwkVVt0fZa0qrtMz+m6tJTAHfZQ8FnmB4MG4LWy7/w==",
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/path-key": {
      "version": "3.1.1",
      "resolved": "https://registry.npmjs.org/path-key/-/path-key-3.1.1.tgz",
      "integrity": "sha512-ojmeN0qd+y0jszEtoY48r0Peq5dwMEkIlCOu6Q5f41lfkswXuKtYrhgoTpLnyIcHm24Uhqx+5Tqm2InSwLhE6Q==",
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/pathe": {
      "version": "2.0.3",
      "resolved": "https://registry.npmjs.org/pathe/-/pathe-2.0.3.tgz",
      "integrity": "sha512-WUjGcAqP1gQacoQe+OBJsFA7Ld4DyXuUIjZ5cc75cLHvJ7dtNsTugphxIADwspS+AraAUePCKrSVtPLFj/F88w==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/picocolors": {
      "version": "1.1.1",
      "resolved": "https://registry.npmjs.org/picocolors/-/picocolors-1.1.1.tgz",
      "integrity": "sha512-xceH2snhtb5M9liqDsmEw56le376mTZkEX/jEb/RxNFyegNul7eNslCXP9FDj/Lcu0X8KEyMceP2ntpaHrDEVA==",
      "dev": true,
      "license": "ISC"
    },
    "node_modules/picomatch": {
      "version": "4.0.3",
      "resolved": "https://registry.npmjs.org/picomatch/-/picomatch-4.0.3.tgz",
      "integrity": "sha512-5gTmgEY/sqK6gFXLIsQNH19lWb4ebPDLA4SdLP7dsWkIXHWlG66oPuVvXSGFPppYZz8ZDZq0dYYrbHfBCVUb1Q==",
      "dev": true,
      "license": "MIT",
      "peer": true,
      "engines": {
        "node": ">=12"
      },
      "funding": {
        "url": "https://github.com/sponsors/jonschlinkert"
      }
    },
    "node_modules/postcss": {
      "version": "8.5.6",
      "resolved": "https://registry.npmjs.org/postcss/-/postcss-8.5.6.tgz",
      "integrity": "sha512-3Ybi1tAuwAP9s0r1UQ2J4n5Y0G05bJkpUIO0/bI9MhwmD70S5aTWbXGBwxHrelT+XM1k6dM0pk+SwNkpTRN7Pg==",
      "dev": true,
      "funding": [
        {
          "type": "opencollective",
          "url": "https://opencollective.com/postcss/"
        },
        {
          "type": "tidelift",
          "url": "https://tidelift.com/funding/github/npm/postcss"
        },
        {
          "type": "github",
          "url": "https://github.com/sponsors/ai"
        }
      ],
      "license": "MIT",
      "peer": true,
      "dependencies": {
        "nanoid": "^3.3.11",
        "picocolors": "^1.1.1",
        "source-map-js": "^1.2.1"
      },
      "engines": {
        "node": "^10 || ^12 || >=14"
      }
    },
    "node_modules/postcss-value-parser": {
      "version": "4.2.0",
      "resolved": "https://registry.npmjs.org/postcss-value-parser/-/postcss-value-parser-4.2.0.tgz",
      "integrity": "sha512-1NNCs6uurfkVbeXG4S8JFT9t19m45ICnif8zWLd5oPSZ50QnwMfK+H3jv408d4jw/7Bttv5axS5IiHoLaVNHeQ==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/prelude-ls": {
      "version": "1.2.1",
      "resolved": "https://registry.npmjs.org/prelude-ls/-/prelude-ls-1.2.1.tgz",
      "integrity": "sha512-vkcDPrRZo1QZLbn5RLGPpg/WmIQ65qoWWhcGKf/b5eplkkarX0m9z8ppCat4mlOqUsWpyNuYgO3VRyrYHSzX5g==",
      "license": "MIT",
      "engines": {
        "node": ">= 0.8.0"
      }
    },
    "node_modules/pretty-format": {
      "version": "27.5.1",
      "resolved": "https://registry.npmjs.org/pretty-format/-/pretty-format-27.5.1.tgz",
      "integrity": "sha512-Qb1gy5OrP5+zDf2Bvnzdl3jsTf1qXVMazbvCoKhtKqVs4/YK4ozX4gKQJJVyNe+cajNPn0KoC0MC3FUmaHWEmQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "ansi-regex": "^5.0.1",
        "ansi-styles": "^5.0.0",
        "react-is": "^17.0.1"
      },
      "engines": {
        "node": "^10.13.0 || ^12.13.0 || ^14.15.0 || >=15.0.0"
      }
    },
    "node_modules/pretty-format/node_modules/ansi-styles": {
      "version": "5.2.0",
      "resolved": "https://registry.npmjs.org/ansi-styles/-/ansi-styles-5.2.0.tgz",
      "integrity": "sha512-Cxwpt2SfTzTtXcfOlzGEee8O+c+MmUgGrNiBcXnuWxuFJHe6a5Hz7qwhwe5OgaSYI0IJvkLqWX1ASG+cJOkEiA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/chalk/ansi-styles?sponsor=1"
      }
    },
    "node_modules/pretty-format/node_modules/react-is": {
      "version": "17.0.2",
      "resolved": "https://registry.npmjs.org/react-is/-/react-is-17.0.2.tgz",
      "integrity": "sha512-w2GsyukL62IJnlaff/nRegPQR94C/XXamvMWmSHRJ4y7Ts/4ocGRmTHvOs8PSE6pB3dWOrD/nueuU5sduBsQ4w==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/prop-types": {
      "version": "15.8.1",
      "resolved": "https://registry.npmjs.org/prop-types/-/prop-types-15.8.1.tgz",
      "integrity": "sha512-oj87CgZICdulUohogVAR7AjlC0327U4el4L6eAvOqCeudMDVU0NThNaV+b9Df4dXgSP1gXMTnPdhfe/2qDH5cg==",
      "license": "MIT",
      "dependencies": {
        "loose-envify": "^1.4.0",
        "object-assign": "^4.1.1",
        "react-is": "^16.13.1"
      }
    },
    "node_modules/prop-types/node_modules/react-is": {
      "version": "16.13.1",
      "resolved": "https://registry.npmjs.org/react-is/-/react-is-16.13.1.tgz",
      "integrity": "sha512-24e6ynE2H+OKt4kqsOvNd8kBpV65zoxbA4BVsEOB3ARVWQki/DHzaUoC5KuON/BiccDaCCTZBuOcfZs70kR8bQ==",
      "license": "MIT"
    },
    "node_modules/punycode": {
      "version": "2.3.1",
      "resolved": "https://registry.npmjs.org/punycode/-/punycode-2.3.1.tgz",
      "integrity": "sha512-vYt7UD1U9Wg6138shLtLOvdAu+8DsC/ilFtEVHcH+wydcSpNE20AfSOduf6MkRFahL5FY7X1oU7nKVZFtfq8Fg==",
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/react": {
      "version": "19.2.0",
      "resolved": "https://registry.npmjs.org/react/-/react-19.2.0.tgz",
      "integrity": "sha512-tmbWg6W31tQLeB5cdIBOicJDJRR2KzXsV7uSK9iNfLWQ5bIZfxuPEHp7M8wiHyHnn0DD1i7w3Zmin0FtkrwoCQ==",
      "license": "MIT",
      "peer": true,
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/react-dom": {
      "version": "19.2.0",
      "resolved": "https://registry.npmjs.org/react-dom/-/react-dom-19.2.0.tgz",
      "integrity": "sha512-UlbRu4cAiGaIewkPyiRGJk0imDN2T3JjieT6spoL2UeSf5od4n5LB/mQ4ejmxhCFT1tYe8IvaFulzynWovsEFQ==",
      "license": "MIT",
      "peer": true,
      "dependencies": {
        "scheduler": "^0.27.0"
      },
      "peerDependencies": {
        "react": "^19.2.0"
      }
    },
    "node_modules/react-draggable": {
      "version": "4.5.0",
      "resolved": "https://registry.npmjs.org/react-draggable/-/react-draggable-4.5.0.tgz",
      "integrity": "sha512-VC+HBLEZ0XJxnOxVAZsdRi8rD04Iz3SiiKOoYzamjylUcju/hP9np/aZdLHf/7WOD268WMoNJMvYfB5yAK45cw==",
      "license": "MIT",
      "dependencies": {
        "clsx": "^2.1.1",
        "prop-types": "^15.8.1"
      },
      "peerDependencies": {
        "react": ">= 16.3.0",
        "react-dom": ">= 16.3.0"
      }
    },
    "node_modules/react-grid-layout": {
      "version": "1.5.2",
      "resolved": "https://registry.npmjs.org/react-grid-layout/-/react-grid-layout-1.5.2.tgz",
      "integrity": "sha512-vT7xmQqszTT+sQw/LfisrEO4le1EPNnSEMVHy6sBZyzS3yGkMywdOd+5iEFFwQwt0NSaGkxuRmYwa1JsP6OJdw==",
      "license": "MIT",
      "dependencies": {
        "clsx": "^2.1.1",
        "fast-equals": "^4.0.3",
        "prop-types": "^15.8.1",
        "react-draggable": "^4.4.6",
        "react-resizable": "^3.0.5",
        "resize-observer-polyfill": "^1.5.1"
      },
      "peerDependencies": {
        "react": ">= 16.3.0",
        "react-dom": ">= 16.3.0"
      }
    },
    "node_modules/react-is": {
      "version": "19.2.0",
      "resolved": "https://registry.npmjs.org/react-is/-/react-is-19.2.0.tgz",
      "integrity": "sha512-x3Ax3kNSMIIkyVYhWPyO09bu0uttcAIoecO/um/rKGQ4EltYWVYtyiGkS/3xMynrbVQdS69Jhlv8FXUEZehlzA==",
      "license": "MIT",
      "peer": true
    },
    "node_modules/react-redux": {
      "version": "9.2.0",
      "resolved": "https://registry.npmjs.org/react-redux/-/react-redux-9.2.0.tgz",
      "integrity": "sha512-ROY9fvHhwOD9ySfrF0wmvu//bKCQ6AeZZq1nJNtbDC+kk5DuSuNX/n6YWYF/SYy7bSba4D4FSz8DJeKY/S/r+g==",
      "license": "MIT",
      "peer": true,
      "dependencies": {
        "@types/use-sync-external-store": "^0.0.6",
        "use-sync-external-store": "^1.4.0"
      },
      "peerDependencies": {
        "@types/react": "^18.2.25 || ^19",
        "react": "^18.0 || ^19",
        "redux": "^5.0.0"
      },
      "peerDependenciesMeta": {
        "@types/react": {
          "optional": true
        },
        "redux": {
          "optional": true
        }
      }
    },
    "node_modules/react-refresh": {
      "version": "0.18.0",
      "resolved": "https://registry.npmjs.org/react-refresh/-/react-refresh-0.18.0.tgz",
      "integrity": "sha512-QgT5//D3jfjJb6Gsjxv0Slpj23ip+HtOpnNgnb2S5zU3CB26G/IDPGoy4RJB42wzFE46DRsstbW6tKHoKbhAxw==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/react-resizable": {
      "version": "3.0.5",
      "resolved": "https://registry.npmjs.org/react-resizable/-/react-resizable-3.0.5.tgz",
      "integrity": "sha512-vKpeHhI5OZvYn82kXOs1bC8aOXktGU5AmKAgaZS4F5JPburCtbmDPqE7Pzp+1kN4+Wb81LlF33VpGwWwtXem+w==",
      "license": "MIT",
      "dependencies": {
        "prop-types": "15.x",
        "react-draggable": "^4.0.3"
      },
      "peerDependencies": {
        "react": ">= 16.3"
      }
    },
    "node_modules/recharts": {
      "version": "3.5.0",
      "resolved": "https://registry.npmjs.org/recharts/-/recharts-3.5.0.tgz",
      "integrity": "sha512-jWqBtu8L3VICXWa3g/y+bKjL8DDHSRme7DHD/70LQ/Tk0di1h11Y0kKC0nPh6YJ2oaa0k6anIFNhg6SfzHWdEA==",
      "license": "MIT",
      "workspaces": [
        "www"
      ],
      "dependencies": {
        "@reduxjs/toolkit": "1.x.x || 2.x.x",
        "clsx": "^2.1.1",
        "decimal.js-light": "^2.5.1",
        "es-toolkit": "^1.39.3",
        "eslint-plugin-react-perf": "^3.3.3",
        "eventemitter3": "^5.0.1",
        "immer": "^10.1.1",
        "react-redux": "8.x.x || 9.x.x",
        "reselect": "5.1.1",
        "tiny-invariant": "^1.3.3",
        "use-sync-external-store": "^1.2.2",
        "victory-vendor": "^37.0.2"
      },
      "engines": {
        "node": ">=18"
      },
      "peerDependencies": {
        "react": "^16.8.0 || ^17.0.0 || ^18.0.0 || ^19.0.0",
        "react-dom": "^16.0.0 || ^17.0.0 || ^18.0.0 || ^19.0.0",
        "react-is": "^16.8.0 || ^17.0.0 || ^18.0.0 || ^19.0.0"
      }
    },
    "node_modules/redent": {
      "version": "3.0.0",
      "resolved": "https://registry.npmjs.org/redent/-/redent-3.0.0.tgz",
      "integrity": "sha512-6tDA8g98We0zd0GvVeMT9arEOnTw9qM03L9cJXaCjrip1OO764RDBLBfrB4cwzNGDj5OA5ioymC9GkizgWJDUg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "indent-string": "^4.0.0",
        "strip-indent": "^3.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/redux": {
      "version": "5.0.1",
      "resolved": "https://registry.npmjs.org/redux/-/redux-5.0.1.tgz",
      "integrity": "sha512-M9/ELqF6fy8FwmkpnF0S3YKOqMyoWJ4+CS5Efg2ct3oY9daQvd/Pc71FpGZsVsbl3Cpb+IIcjBDUnnyBdQbq4w==",
      "license": "MIT",
      "peer": true
    },
    "node_modules/redux-thunk": {
      "version": "3.1.0",
      "resolved": "https://registry.npmjs.org/redux-thunk/-/redux-thunk-3.1.0.tgz",
      "integrity": "sha512-NW2r5T6ksUKXCabzhL9z+h206HQw/NJkcLm1GPImRQ8IzfXwRGqjVhKJGauHirT0DAuyy6hjdnMZaRoAcy0Klw==",
      "license": "MIT",
      "peerDependencies": {
        "redux": "^5.0.0"
      }
    },
    "node_modules/require-from-string": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/require-from-string/-/require-from-string-2.0.2.tgz",
      "integrity": "sha512-Xf0nWe6RseziFMu+Ap9biiUbmplq6S9/p+7w7YXP/JBHhrUDDUhwa+vANyubuqfZWTveU//DYVGsDG7RKL/vEw==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/reselect": {
      "version": "5.1.1",
      "resolved": "https://registry.npmjs.org/reselect/-/reselect-5.1.1.tgz",
      "integrity": "sha512-K/BG6eIky/SBpzfHZv/dd+9JBFiS4SWV7FIujVyJRux6e45+73RaUHXLmIR1f7WOMaQ0U1km6qwklRQxpJJY0w==",
      "license": "MIT"
    },
    "node_modules/resize-observer-polyfill": {
      "version": "1.5.1",
      "resolved": "https://registry.npmjs.org/resize-observer-polyfill/-/resize-observer-polyfill-1.5.1.tgz",
      "integrity": "sha512-LwZrotdHOo12nQuZlHEmtuXdqGoOD0OhaxopaNFxWzInpEgaLWoVuAMbTzixuosCx2nEG58ngzW3vxdWoxIgdg==",
      "license": "MIT"
    },
    "node_modules/resolve-from": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/resolve-from/-/resolve-from-4.0.0.tgz",
      "integrity": "sha512-pb/MYmXstAkysRFx8piNI1tGFNQIFA3vkE3Gq4EuA1dF6gHp/+vgZqsCGJapvy8N3Q+4o7FwvquPJcnZ7RYy4g==",
      "license": "MIT",
      "engines": {
        "node": ">=4"
      }
    },
    "node_modules/rollup": {
      "version": "4.53.3",
      "resolved": "https://registry.npmjs.org/rollup/-/rollup-4.53.3.tgz",
      "integrity": "sha512-w8GmOxZfBmKknvdXU1sdM9NHcoQejwF/4mNgj2JuEEdRaHwwF12K7e9eXn1nLZ07ad+du76mkVsyeb2rKGllsA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@types/estree": "1.0.8"
      },
      "bin": {
        "rollup": "dist/bin/rollup"
      },
      "engines": {
        "node": ">=18.0.0",
        "npm": ">=8.0.0"
      },
      "optionalDependencies": {
        "@rollup/rollup-android-arm-eabi": "4.53.3",
        "@rollup/rollup-android-arm64": "4.53.3",
        "@rollup/rollup-darwin-arm64": "4.53.3",
        "@rollup/rollup-darwin-x64": "4.53.3",
        "@rollup/rollup-freebsd-arm64": "4.53.3",
        "@rollup/rollup-freebsd-x64": "4.53.3",
        "@rollup/rollup-linux-arm-gnueabihf": "4.53.3",
        "@rollup/rollup-linux-arm-musleabihf": "4.53.3",
        "@rollup/rollup-linux-arm64-gnu": "4.53.3",
        "@rollup/rollup-linux-arm64-musl": "4.53.3",
        "@rollup/rollup-linux-loong64-gnu": "4.53.3",
        "@rollup/rollup-linux-ppc64-gnu": "4.53.3",
        "@rollup/rollup-linux-riscv64-gnu": "4.53.3",
        "@rollup/rollup-linux-riscv64-musl": "4.53.3",
        "@rollup/rollup-linux-s390x-gnu": "4.53.3",
        "@rollup/rollup-linux-x64-gnu": "4.53.3",
        "@rollup/rollup-linux-x64-musl": "4.53.3",
        "@rollup/rollup-openharmony-arm64": "4.53.3",
        "@rollup/rollup-win32-arm64-msvc": "4.53.3",
        "@rollup/rollup-win32-ia32-msvc": "4.53.3",
        "@rollup/rollup-win32-x64-gnu": "4.53.3",
        "@rollup/rollup-win32-x64-msvc": "4.53.3",
        "fsevents": "~2.3.2"
      }
    },
    "node_modules/safer-buffer": {
      "version": "2.1.2",
      "resolved": "https://registry.npmjs.org/safer-buffer/-/safer-buffer-2.1.2.tgz",
      "integrity": "sha512-YZo3K82SD7Riyi0E1EQPojLz7kpepnSQI9IyPbHHg1XXXevb5dJI7tpyN2ADxGcQbHG7vcyRHk0cbwqcQriUtg==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/saxes": {
      "version": "6.0.0",
      "resolved": "https://registry.npmjs.org/saxes/-/saxes-6.0.0.tgz",
      "integrity": "sha512-xAg7SOnEhrm5zI3puOOKyy1OMcMlIJZYNJY7xLBwSze0UjhPLnWfj2GF2EpT0jmzaJKIWKHLsaSSajf35bcYnA==",
      "dev": true,
      "license": "ISC",
      "dependencies": {
        "xmlchars": "^2.2.0"
      },
      "engines": {
        "node": ">=v12.22.7"
      }
    },
    "node_modules/scheduler": {
      "version": "0.27.0",
      "resolved": "https://registry.npmjs.org/scheduler/-/scheduler-0.27.0.tgz",
      "integrity": "sha512-eNv+WrVbKu1f3vbYJT/xtiF5syA5HPIMtf9IgY/nKg0sWqzAUEvqY/xm7OcZc/qafLx/iO9FgOmeSAp4v5ti/Q==",
      "license": "MIT"
    },
    "node_modules/semver": {
      "version": "6.3.1",
      "resolved": "https://registry.npmjs.org/semver/-/semver-6.3.1.tgz",
      "integrity": "sha512-BR7VvDCVHO+q2xBEWskxS6DJE1qRnb7DxzUrogb71CWoSficBxYsiAGd+Kl0mmq/MprG9yArRkyrQxTO6XjMzA==",
      "dev": true,
      "license": "ISC",
      "bin": {
        "semver": "bin/semver.js"
      }
    },
    "node_modules/shebang-command": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/shebang-command/-/shebang-command-2.0.0.tgz",
      "integrity": "sha512-kHxr2zZpYtdmrN1qDjrrX/Z1rR1kG8Dx+gkpK1G4eXmvXswmcE1hTWBWYUzlraYw1/yZp6YuDY77YtvbN0dmDA==",
      "license": "MIT",
      "dependencies": {
        "shebang-regex": "^3.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/shebang-regex": {
      "version": "3.0.0",
      "resolved": "https://registry.npmjs.org/shebang-regex/-/shebang-regex-3.0.0.tgz",
      "integrity": "sha512-7++dFhtcx3353uBaq8DDR4NuxBetBzC7ZQOhmTQInHEd6bSrXdiEyzCvG07Z44UYdLShWUyXt5M/yhz8ekcb1A==",
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/siginfo": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/siginfo/-/siginfo-2.0.0.tgz",
      "integrity": "sha512-ybx0WO1/8bSBLEWXZvEd7gMW3Sn3JFlW3TvX1nREbDLRNQNaeNN8WK0meBwPdAaOI7TtRRRJn/Es1zhrrCHu7g==",
      "dev": true,
      "license": "ISC"
    },
    "node_modules/source-map-js": {
      "version": "1.2.1",
      "resolved": "https://registry.npmjs.org/source-map-js/-/source-map-js-1.2.1.tgz",
      "integrity": "sha512-UXWMKhLOwVKb728IUtQPXxfYU+usdybtUrK/8uGE8CQMvrhOpwvzDBwj0QhSL7MQc7vIsISBG8VQ8+IDQxpfQA==",
      "dev": true,
      "license": "BSD-3-Clause",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/stackback": {
      "version": "0.0.2",
      "resolved": "https://registry.npmjs.org/stackback/-/stackback-0.0.2.tgz",
      "integrity": "sha512-1XMJE5fQo1jGH6Y/7ebnwPOBEkIEnT4QF32d5R1+VXdXveM0IBMJt8zfaxX1P3QhVwrYe+576+jkANtSS2mBbw==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/std-env": {
      "version": "3.10.0",
      "resolved": "https://registry.npmjs.org/std-env/-/std-env-3.10.0.tgz",
      "integrity": "sha512-5GS12FdOZNliM5mAOxFRg7Ir0pWz8MdpYm6AY6VPkGpbA7ZzmbzNcBJQ0GPvvyWgcY7QAhCgf9Uy89I03faLkg==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/strip-indent": {
      "version": "3.0.0",
      "resolved": "https://registry.npmjs.org/strip-indent/-/strip-indent-3.0.0.tgz",
      "integrity": "sha512-laJTa3Jb+VQpaC6DseHhF7dXVqHTfJPCRDaEbid/drOhgitgYku/letMUqOXFoWV0zIIUbjpdH2t+tYj4bQMRQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "min-indent": "^1.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/strip-json-comments": {
      "version": "3.1.1",
      "resolved": "https://registry.npmjs.org/strip-json-comments/-/strip-json-comments-3.1.1.tgz",
      "integrity": "sha512-6fPc+R4ihwqP6N/aIv2f1gMH8lOVtWQHoqC4yK6oSDVVocumAsfCqjkXnqiYMhmMwS/mEHLp7Vehlt3ql6lEig==",
      "license": "MIT",
      "engines": {
        "node": ">=8"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/supports-color": {
      "version": "7.2.0",
      "resolved": "https://registry.npmjs.org/supports-color/-/supports-color-7.2.0.tgz",
      "integrity": "sha512-qpCAvRl9stuOHveKsn7HncJRvv501qIacKzQlO/+Lwxc9+0q2wLyv4Dfvt80/DPn2pqOBsJdDiogXGR9+OvwRw==",
      "license": "MIT",
      "dependencies": {
        "has-flag": "^4.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/symbol-tree": {
      "version": "3.2.4",
      "resolved": "https://registry.npmjs.org/symbol-tree/-/symbol-tree-3.2.4.tgz",
      "integrity": "sha512-9QNk5KwDF+Bvz+PyObkmSYjI5ksVUYtjW7AU22r2NKcfLJcXp96hkDWU3+XndOsUb+AQ9QhfzfCT2O+CNWT5Tw==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/tailwindcss": {
      "version": "4.1.17",
      "resolved": "https://registry.npmjs.org/tailwindcss/-/tailwindcss-4.1.17.tgz",
      "integrity": "sha512-j9Ee2YjuQqYT9bbRTfTZht9W/ytp5H+jJpZKiYdP/bpnXARAuELt9ofP0lPnmHjbga7SNQIxdTAXCmtKVYjN+Q==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/tapable": {
      "version": "2.3.0",
      "resolved": "https://registry.npmjs.org/tapable/-/tapable-2.3.0.tgz",
      "integrity": "sha512-g9ljZiwki/LfxmQADO3dEY1CbpmXT5Hm2fJ+QaGKwSXUylMybePR7/67YW7jOrrvjEgL1Fmz5kzyAjWVWLlucg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/webpack"
      }
    },
    "node_modules/tiny-invariant": {
      "version": "1.3.3",
      "resolved": "https://registry.npmjs.org/tiny-invariant/-/tiny-invariant-1.3.3.tgz",
      "integrity": "sha512-+FbBPE1o9QAYvviau/qC5SE3caw21q3xkvWKBtja5vgqOWIHHJ3ioaq1VPfn/Szqctz2bU/oYeKd9/z5BL+PVg==",
      "license": "MIT"
    },
    "node_modules/tinybench": {
      "version": "2.9.0",
      "resolved": "https://registry.npmjs.org/tinybench/-/tinybench-2.9.0.tgz",
      "integrity": "sha512-0+DUvqWMValLmha6lr4kD8iAMK1HzV0/aKnCtWb9v9641TnP/MFb7Pc2bxoxQjTXAErryXVgUOfv2YqNllqGeg==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/tinyexec": {
      "version": "0.3.2",
      "resolved": "https://registry.npmjs.org/tinyexec/-/tinyexec-0.3.2.tgz",
      "integrity": "sha512-KQQR9yN7R5+OSwaK0XQoj22pwHoTlgYqmUscPYoknOoWCWfj/5/ABTMRi69FrKU5ffPVh5QcFikpWJI/P1ocHA==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/tinyglobby": {
      "version": "0.2.15",
      "resolved": "https://registry.npmjs.org/tinyglobby/-/tinyglobby-0.2.15.tgz",
      "integrity": "sha512-j2Zq4NyQYG5XMST4cbs02Ak8iJUdxRM0XI5QyxXuZOzKOINmWurp3smXu3y5wDcJrptwpSjgXHzIQxR0omXljQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "fdir": "^6.5.0",
        "picomatch": "^4.0.3"
      },
      "engines": {
        "node": ">=12.0.0"
      },
      "funding": {
        "url": "https://github.com/sponsors/SuperchupuDev"
      }
    },
    "node_modules/tinyrainbow": {
      "version": "3.0.3",
      "resolved": "https://registry.npmjs.org/tinyrainbow/-/tinyrainbow-3.0.3.tgz",
      "integrity": "sha512-PSkbLUoxOFRzJYjjxHJt9xro7D+iilgMX/C9lawzVuYiIdcihh9DXmVibBe8lmcFrRi/VzlPjBxbN7rH24q8/Q==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=14.0.0"
      }
    },
    "node_modules/tldts": {
      "version": "7.0.19",
      "resolved": "https://registry.npmjs.org/tldts/-/tldts-7.0.19.tgz",
      "integrity": "sha512-8PWx8tvC4jDB39BQw1m4x8y5MH1BcQ5xHeL2n7UVFulMPH/3Q0uiamahFJ3lXA0zO2SUyRXuVVbWSDmstlt9YA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "tldts-core": "^7.0.19"
      },
      "bin": {
        "tldts": "bin/cli.js"
      }
    },
    "node_modules/tldts-core": {
      "version": "7.0.19",
      "resolved": "https://registry.npmjs.org/tldts-core/-/tldts-core-7.0.19.tgz",
      "integrity": "sha512-lJX2dEWx0SGH4O6p+7FPwYmJ/bu1JbcGJ8RLaG9b7liIgZ85itUVEPbMtWRVrde/0fnDPEPHW10ZsKW3kVsE9A==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/tough-cookie": {
      "version": "6.0.0",
      "resolved": "https://registry.npmjs.org/tough-cookie/-/tough-cookie-6.0.0.tgz",
      "integrity": "sha512-kXuRi1mtaKMrsLUxz3sQYvVl37B0Ns6MzfrtV5DvJceE9bPyspOqk9xxv7XbZWcfLWbFmm997vl83qUWVJA64w==",
      "dev": true,
      "license": "BSD-3-Clause",
      "dependencies": {
        "tldts": "^7.0.5"
      },
      "engines": {
        "node": ">=16"
      }
    },
    "node_modules/tr46": {
      "version": "6.0.0",
      "resolved": "https://registry.npmjs.org/tr46/-/tr46-6.0.0.tgz",
      "integrity": "sha512-bLVMLPtstlZ4iMQHpFHTR7GAGj2jxi8Dg0s2h2MafAE4uSWF98FC/3MomU51iQAMf8/qDUbKWf5GxuvvVcXEhw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "punycode": "^2.3.1"
      },
      "engines": {
        "node": ">=20"
      }
    },
    "node_modules/type-check": {
      "version": "0.4.0",
      "resolved": "https://registry.npmjs.org/type-check/-/type-check-0.4.0.tgz",
      "integrity": "sha512-XleUoc9uwGXqjWwXaUTZAmzMcFZ5858QA2vvx1Ur5xIcixXIP+8LnFDgRplU30us6teqdlskFfu+ae4K79Ooew==",
      "license": "MIT",
      "dependencies": {
        "prelude-ls": "^1.2.1"
      },
      "engines": {
        "node": ">= 0.8.0"
      }
    },
    "node_modules/update-browserslist-db": {
      "version": "1.1.4",
      "resolved": "https://registry.npmjs.org/update-browserslist-db/-/update-browserslist-db-1.1.4.tgz",
      "integrity": "sha512-q0SPT4xyU84saUX+tomz1WLkxUbuaJnR1xWt17M7fJtEJigJeWUNGUqrauFXsHnqev9y9JTRGwk13tFBuKby4A==",
      "dev": true,
      "funding": [
        {
          "type": "opencollective",
          "url": "https://opencollective.com/browserslist"
        },
        {
          "type": "tidelift",
          "url": "https://tidelift.com/funding/github/npm/browserslist"
        },
        {
          "type": "github",
          "url": "https://github.com/sponsors/ai"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "escalade": "^3.2.0",
        "picocolors": "^1.1.1"
      },
      "bin": {
        "update-browserslist-db": "cli.js"
      },
      "peerDependencies": {
        "browserslist": ">= 4.21.0"
      }
    },
    "node_modules/uri-js": {
      "version": "4.4.1",
      "resolved": "https://registry.npmjs.org/uri-js/-/uri-js-4.4.1.tgz",
      "integrity": "sha512-7rKUyy33Q1yc98pQ1DAmLtwX109F7TIfWlW1Ydo8Wl1ii1SeHieeh0HHfPeL2fMXK6z0s8ecKs9frCuLJvndBg==",
      "license": "BSD-2-Clause",
      "dependencies": {
        "punycode": "^2.1.0"
      }
    },
    "node_modules/use-sync-external-store": {
      "version": "1.6.0",
      "resolved": "https://registry.npmjs.org/use-sync-external-store/-/use-sync-external-store-1.6.0.tgz",
      "integrity": "sha512-Pp6GSwGP/NrPIrxVFAIkOQeyw8lFenOHijQWkUTrDvrF4ALqylP2C/KCkeS9dpUM3KvYRQhna5vt7IL95+ZQ9w==",
      "license": "MIT",
      "peerDependencies": {
        "react": "^16.8.0 || ^17.0.0 || ^18.0.0 || ^19.0.0"
      }
    },
    "node_modules/victory-vendor": {
      "version": "37.3.6",
      "resolved": "https://registry.npmjs.org/victory-vendor/-/victory-vendor-37.3.6.tgz",
      "integrity": "sha512-SbPDPdDBYp+5MJHhBCAyI7wKM3d5ivekigc2Dk2s7pgbZ9wIgIBYGVw4zGHBml/qTFbexrofXW6Gu4noGxrOwQ==",
      "license": "MIT AND ISC",
      "dependencies": {
        "@types/d3-array": "^3.0.3",
        "@types/d3-ease": "^3.0.0",
        "@types/d3-interpolate": "^3.0.1",
        "@types/d3-scale": "^4.0.2",
        "@types/d3-shape": "^3.1.0",
        "@types/d3-time": "^3.0.0",
        "@types/d3-timer": "^3.0.0",
        "d3-array": "^3.1.6",
        "d3-ease": "^3.0.1",
        "d3-interpolate": "^3.0.1",
        "d3-scale": "^4.0.2",
        "d3-shape": "^3.1.0",
        "d3-time": "^3.0.0",
        "d3-timer": "^3.0.1"
      }
    },
    "node_modules/vite": {
      "version": "7.2.4",
      "resolved": "https://registry.npmjs.org/vite/-/vite-7.2.4.tgz",
      "integrity": "sha512-NL8jTlbo0Tn4dUEXEsUg8KeyG/Lkmc4Fnzb8JXN/Ykm9G4HNImjtABMJgkQoVjOBN/j2WAwDTRytdqJbZsah7w==",
      "dev": true,
      "license": "MIT",
      "peer": true,
      "dependencies": {
        "esbuild": "^0.25.0",
        "fdir": "^6.5.0",
        "picomatch": "^4.0.3",
        "postcss": "^8.5.6",
        "rollup": "^4.43.0",
        "tinyglobby": "^0.2.15"
      },
      "bin": {
        "vite": "bin/vite.js"
      },
      "engines": {
        "node": "^20.19.0 || >=22.12.0"
      },
      "funding": {
        "url": "https://github.com/vitejs/vite?sponsor=1"
      },
      "optionalDependencies": {
        "fsevents": "~2.3.3"
      },
      "peerDependencies": {
        "@types/node": "^20.19.0 || >=22.12.0",
        "jiti": ">=1.21.0",
        "less": "^4.0.0",
        "lightningcss": "^1.21.0",
        "sass": "^1.70.0",
        "sass-embedded": "^1.70.0",
        "stylus": ">=0.54.8",
        "sugarss": "^5.0.0",
        "terser": "^5.16.0",
        "tsx": "^4.8.1",
        "yaml": "^2.4.2"
      },
      "peerDependenciesMeta": {
        "@types/node": {
          "optional": true
        },
        "jiti": {
          "optional": true
        },
        "less": {
          "optional": true
        },
        "lightningcss": {
          "optional": true
        },
        "sass": {
          "optional": true
        },
        "sass-embedded": {
          "optional": true
        },
        "stylus": {
          "optional": true
        },
        "sugarss": {
          "optional": true
        },
        "terser": {
          "optional": true
        },
        "tsx": {
          "optional": true
        },
        "yaml": {
          "optional": true
        }
      }
    },
    "node_modules/vitest": {
      "version": "4.0.14",
      "resolved": "https://registry.npmjs.org/vitest/-/vitest-4.0.14.tgz",
      "integrity": "sha512-d9B2J9Cm9dN9+6nxMnnNJKJCtcyKfnHj15N6YNJfaFHRLua/d3sRKU9RuKmO9mB0XdFtUizlxfz/VPbd3OxGhw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@vitest/expect": "4.0.14",
        "@vitest/mocker": "4.0.14",
        "@vitest/pretty-format": "4.0.14",
        "@vitest/runner": "4.0.14",
        "@vitest/snapshot": "4.0.14",
        "@vitest/spy": "4.0.14",
        "@vitest/utils": "4.0.14",
        "es-module-lexer": "^1.7.0",
        "expect-type": "^1.2.2",
        "magic-string": "^0.30.21",
        "obug": "^2.1.1",
        "pathe": "^2.0.3",
        "picomatch": "^4.0.3",
        "std-env": "^3.10.0",
        "tinybench": "^2.9.0",
        "tinyexec": "^0.3.2",
        "tinyglobby": "^0.2.15",
        "tinyrainbow": "^3.0.3",
        "vite": "^6.0.0 || ^7.0.0",
        "why-is-node-running": "^2.3.0"
      },
      "bin": {
        "vitest": "vitest.mjs"
      },
      "engines": {
        "node": "^20.0.0 || ^22.0.0 || >=24.0.0"
      },
      "funding": {
        "url": "https://opencollective.com/vitest"
      },
      "peerDependencies": {
        "@edge-runtime/vm": "*",
        "@opentelemetry/api": "^1.9.0",
        "@types/node": "^20.0.0 || ^22.0.0 || >=24.0.0",
        "@vitest/browser-playwright": "4.0.14",
        "@vitest/browser-preview": "4.0.14",
        "@vitest/browser-webdriverio": "4.0.14",
        "@vitest/ui": "4.0.14",
        "happy-dom": "*",
        "jsdom": "*"
      },
      "peerDependenciesMeta": {
        "@edge-runtime/vm": {
          "optional": true
        },
        "@opentelemetry/api": {
          "optional": true
        },
        "@types/node": {
          "optional": true
        },
        "@vitest/browser-playwright": {
          "optional": true
        },
        "@vitest/browser-preview": {
          "optional": true
        },
        "@vitest/browser-webdriverio": {
          "optional": true
        },
        "@vitest/ui": {
          "optional": true
        },
        "happy-dom": {
          "optional": true
        },
        "jsdom": {
          "optional": true
        }
      }
    },
    "node_modules/w3c-xmlserializer": {
      "version": "5.0.0",
      "resolved": "https://registry.npmjs.org/w3c-xmlserializer/-/w3c-xmlserializer-5.0.0.tgz",
      "integrity": "sha512-o8qghlI8NZHU1lLPrpi2+Uq7abh4GGPpYANlalzWxyWteJOCsr/P+oPBA49TOLu5FTZO4d3F9MnWJfiMo4BkmA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "xml-name-validator": "^5.0.0"
      },
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/webidl-conversions": {
      "version": "8.0.0",
      "resolved": "https://registry.npmjs.org/webidl-conversions/-/webidl-conversions-8.0.0.tgz",
      "integrity": "sha512-n4W4YFyz5JzOfQeA8oN7dUYpR+MBP3PIUsn2jLjWXwK5ASUzt0Jc/A5sAUZoCYFJRGF0FBKJ+1JjN43rNdsQzA==",
      "dev": true,
      "license": "BSD-2-Clause",
      "engines": {
        "node": ">=20"
      }
    },
    "node_modules/whatwg-encoding": {
      "version": "3.1.1",
      "resolved": "https://registry.npmjs.org/whatwg-encoding/-/whatwg-encoding-3.1.1.tgz",
      "integrity": "sha512-6qN4hJdMwfYBtE3YBTTHhoeuUrDBPZmbQaxWAqSALV/MeEnR5z1xd8UKud2RAkFoPkmB+hli1TZSnyi84xz1vQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "iconv-lite": "0.6.3"
      },
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/whatwg-mimetype": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/whatwg-mimetype/-/whatwg-mimetype-4.0.0.tgz",
      "integrity": "sha512-QaKxh0eNIi2mE9p2vEdzfagOKHCcj1pJ56EEHGQOVxp8r9/iszLUUV7v89x9O1p/T+NlTM5W7jW6+cz4Fq1YVg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/whatwg-url": {
      "version": "15.1.0",
      "resolved": "https://registry.npmjs.org/whatwg-url/-/whatwg-url-15.1.0.tgz",
      "integrity": "sha512-2ytDk0kiEj/yu90JOAp44PVPUkO9+jVhyf+SybKlRHSDlvOOZhdPIrr7xTH64l4WixO2cP+wQIcgujkGBPPz6g==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "tr46": "^6.0.0",
        "webidl-conversions": "^8.0.0"
      },
      "engines": {
        "node": ">=20"
      }
    },
    "node_modules/which": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/which/-/which-2.0.2.tgz",
      "integrity": "sha512-BLI3Tl1TW3Pvl70l3yq3Y64i+awpwXqsGBYWkkqMtnbXgrMD+yj7rhW0kuEDxzJaYXGjEW5ogapKNMEKNMjibA==",
      "license": "ISC",
      "dependencies": {
        "isexe": "^2.0.0"
      },
      "bin": {
        "node-which": "bin/node-which"
      },
      "engines": {
        "node": ">= 8"
      }
    },
    "node_modules/why-is-node-running": {
      "version": "2.3.0",
      "resolved": "https://registry.npmjs.org/why-is-node-running/-/why-is-node-running-2.3.0.tgz",
      "integrity": "sha512-hUrmaWBdVDcxvYqnyh09zunKzROWjbZTiNy8dBEjkS7ehEDQibXJ7XvlmtbwuTclUiIyN+CyXQD4Vmko8fNm8w==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "siginfo": "^2.0.0",
        "stackback": "0.0.2"
      },
      "bin": {
        "why-is-node-running": "cli.js"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/word-wrap": {
      "version": "1.2.5",
      "resolved": "https://registry.npmjs.org/word-wrap/-/word-wrap-1.2.5.tgz",
      "integrity": "sha512-BN22B5eaMMI9UMtjrGd5g5eCYPpCPDUy0FJXbYsaT5zYxjFOckS53SQDE3pWkVoWpHXVb3BrYcEN4Twa55B5cA==",
      "license": "MIT",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/ws": {
      "version": "8.18.3",
      "resolved": "https://registry.npmjs.org/ws/-/ws-8.18.3.tgz",
      "integrity": "sha512-PEIGCY5tSlUt50cqyMXfCzX+oOPqN0vuGqWzbcJ2xvnkzkq46oOpz7dQaTDBdfICb4N14+GARUDw2XV2N4tvzg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=10.0.0"
      },
      "peerDependencies": {
        "bufferutil": "^4.0.1",
        "utf-8-validate": ">=5.0.2"
      },
      "peerDependenciesMeta": {
        "bufferutil": {
          "optional": true
        },
        "utf-8-validate": {
          "optional": true
        }
      }
    },
    "node_modules/xml-name-validator": {
      "version": "5.0.0",
      "resolved": "https://registry.npmjs.org/xml-name-validator/-/xml-name-validator-5.0.0.tgz",
      "integrity": "sha512-EvGK8EJ3DhaHfbRlETOWAS5pO9MZITeauHKJyb8wyajUfQUenkIg2MvLDTZ4T/TgIcm3HU0TFBgWWboAZ30UHg==",
      "dev": true,
      "license": "Apache-2.0",
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/xmlchars": {
      "version": "2.2.0",
      "resolved": "https://registry.npmjs.org/xmlchars/-/xmlchars-2.2.0.tgz",
      "integrity": "sha512-JZnDKK8B0RCDw84FNdDAIpZK+JuJw+s7Lz8nksI7SIuU3UXJJslUthsi+uWBUYOwPFwW7W7PRLRfUKpxjtjFCw==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/yallist": {
      "version": "3.1.1",
      "resolved": "https://registry.npmjs.org/yallist/-/yallist-3.1.1.tgz",
      "integrity": "sha512-a4UGQaWPH59mOXUYnAG2ewncQS4i4F43Tv3JoAM+s2VDAmS9NsK8GpDMLrCHPksFT7h3K6TOoUNn2pb7RoXx4g==",
      "dev": true,
      "license": "ISC"
    },
    "node_modules/yocto-queue": {
      "version": "0.1.0",
      "resolved": "https://registry.npmjs.org/yocto-queue/-/yocto-queue-0.1.0.tgz",
      "integrity": "sha512-rVksvsnNCdJ/ohGc6xgPwyN8eheCxsiLM8mxuE/t/mOVqJewPuO1miLpTHQiRgTKCLexL4MeAFVagts7HmNZ2Q==",
      "license": "MIT",
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/zod": {
      "version": "4.1.13",
      "resolved": "https://registry.npmjs.org/zod/-/zod-4.1.13.tgz",
      "integrity": "sha512-AvvthqfqrAhNH9dnfmrfKzX5upOdjUVJYFqNSlkmGf64gRaTzlPwz99IHYnVs28qYAybvAlBV+H7pn0saFY4Ig==",
      "dev": true,
      "license": "MIT",
      "peer": true,
      "funding": {
        "url": "https://github.com/sponsors/colinhacks"
      }
    },
    "node_modules/zod-validation-error": {
      "version": "4.0.2",
      "resolved": "https://registry.npmjs.org/zod-validation-error/-/zod-validation-error-4.0.2.tgz",
      "integrity": "sha512-Q6/nZLe6jxuU80qb/4uJ4t5v2VEZ44lzQjPDhYJNztRQ4wyWc6VF3D3Kb/fAuPetZQnhS3hnajCf9CsWesghLQ==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=18.0.0"
      },
      "peerDependencies": {
        "zod": "^3.25.0 || ^4.0.0"
      }
    }
  }
}


======= FILE: dashboard/package.json =======

{
  "name": "dashboard",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "lint": "eslint .",
    "preview": "vite preview"
  },
  "dependencies": {
    "lodash": "^4.17.21",
    "lucide-react": "^0.555.0",
    "react": "^19.2.0",
    "react-dom": "^19.2.0",
    "react-grid-layout": "^1.5.2",
    "recharts": "^3.5.0"
  },
  "devDependencies": {
    "@eslint/js": "^9.39.1",
    "@tailwindcss/postcss": "^4.1.17",
    "@testing-library/jest-dom": "^6.9.1",
    "@testing-library/react": "^16.3.0",
    "@types/react": "^19.2.5",
    "@types/react-dom": "^19.2.3",
    "@vitejs/plugin-react": "^5.1.1",
    "autoprefixer": "^10.4.22",
    "eslint": "^9.39.1",
    "eslint-plugin-react-hooks": "^7.0.1",
    "eslint-plugin-react-refresh": "^0.4.24",
    "globals": "^16.5.0",
    "jsdom": "^27.2.0",
    "postcss": "^8.5.6",
    "tailwindcss": "^4.1.17",
    "vite": "^7.2.4",
    "vitest": "^4.0.14"
  }
}


======= FILE: dashboard/eslint.config.js =======

import js from '@eslint/js'
import globals from 'globals'
import reactHooks from 'eslint-plugin-react-hooks'
import reactRefresh from 'eslint-plugin-react-refresh'
import { defineConfig, globalIgnores } from 'eslint/config'

export default defineConfig([
  globalIgnores(['dist']),
  {
    files: ['**/*.{js,jsx}'],
    extends: [
      js.configs.recommended,
      reactHooks.configs.flat.recommended,
      reactRefresh.configs.vite,
    ],
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
      parserOptions: {
        ecmaVersion: 'latest',
        ecmaFeatures: { jsx: true },
        sourceType: 'module',
      },
    },
    rules: {
      'no-unused-vars': ['error', { varsIgnorePattern: '^[A-Z_]' }],
    },
  },
])


======= FILE: dashboard/postcss.config.js =======

export default {
    plugins: {
        '@tailwindcss/postcss': {},
        autoprefixer: {},
    },
}


======= FILE: dashboard/src/App.css =======

#root {
  max-width: 1280px;
  margin: 0 auto;
  padding: 2rem;
  text-align: center;
}

.logo {
  height: 6em;
  padding: 1.5em;
  will-change: filter;
  transition: filter 300ms;
}
.logo:hover {
  filter: drop-shadow(0 0 2em #646cffaa);
}
.logo.react:hover {
  filter: drop-shadow(0 0 2em #61dafbaa);
}

@keyframes logo-spin {
  from {
    transform: rotate(0deg);
  }
  to {
    transform: rotate(360deg);
  }
}

@media (prefers-reduced-motion: no-preference) {
  a:nth-of-type(2) .logo {
    animation: logo-spin infinite 20s linear;
  }
}

.card {
  padding: 2em;
}

.read-the-docs {
  color: #888;
}


======= FILE: dashboard/src/index.css =======

@import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;700&family=Rajdhani:wght@400;500;600;700&display=swap');

@tailwind base;
@tailwind components;
@tailwind utilities;

:root {
  --bg-dark: #050505;
  --primary: #00f3ff;
  --secondary: #bc13fe;
}

body {
  background-color: #050505;
  color: #f3f4f6;
  font-family: 'Rajdhani', sans-serif;
  overflow-x: hidden;
  background-image:
    linear-gradient(rgba(0, 243, 255, 0.03) 1px, transparent 1px),
    linear-gradient(90deg, rgba(0, 243, 255, 0.03) 1px, transparent 1px);
  background-size: 30px 30px;
}

/* Glassmorphism Panel */
.glass-panel {
  background-color: rgba(0, 0, 0, 0.4);
  backdrop-filter: blur(12px);
  -webkit-backdrop-filter: blur(12px);
  border: 1px solid rgba(255, 255, 255, 0.1);
  border-radius: 0.75rem;
  box-shadow: 0 4px 30px rgba(0, 0, 0, 0.5);
}

/* Neon Text */
.text-neon-cyan {
  color: #00f3ff;
  text-shadow: 0 0 5px rgba(0, 243, 255, 0.7);
}

.text-neon-purple {
  color: #bc13fe;
  text-shadow: 0 0 5px rgba(188, 19, 254, 0.7);
}

/* Scrollbar */
::-webkit-scrollbar {
  width: 6px;
  height: 6px;
}

::-webkit-scrollbar-track {
  background-color: #000000;
}

::-webkit-scrollbar-thumb {
  background-color: #1f2937;
  border-radius: 9999px;
}

::-webkit-scrollbar-thumb:hover {
  background-color: rgba(0, 243, 255, 0.5);
}

/* Chart Tooltip Customization */
.recharts-tooltip-wrapper {
  z-index: 50 !important;
}

.custom-tooltip {
  background-color: rgba(0, 0, 0, 0.9);
  border: 1px solid rgba(0, 243, 255, 0.3);
  padding: 0.75rem;
  border-radius: 0.25rem;
  box-shadow: 0 0 10px rgba(0, 243, 255, 0.5);
  backdrop-filter: blur(24px);
}

======= FILE: dashboard/src/main.jsx =======

import { StrictMode } from 'react'
import { createRoot } from 'react-dom/client'
import './index.css'
import App from './App.jsx'

createRoot(document.getElementById('root')).render(
  <StrictMode>
    <App />
  </StrictMode>,
)


======= FILE: dashboard/src/App.jsx =======


import React, { useState, useEffect, useRef } from 'react';
import {
  AreaChart, Area, XAxis, YAxis, CartesianGrid, Tooltip, ResponsiveContainer,
  BarChart, Bar, Cell, RadarChart, PolarGrid, PolarAngleAxis, PolarRadiusAxis, Radar,
  ScatterChart, Scatter, PieChart, Pie, Legend, LineChart, Line
} from 'recharts';
import { Play, Pause, Activity, Cpu, Zap, Layers, Terminal, Wifi, BarChart2, Database, PieChart as PieIcon, Target, History, Trash2, LayoutDashboard, Server, Brain, Shuffle, MousePointer2, GripHorizontal, TrendingUp } from 'lucide-react';
import { Responsive, WidthProvider } from 'react-grid-layout';
import EnhancedVisualizationsDemo from './components/EnhancedVisualizationsDemo';
import HistoricalPerformanceTable from './components/HistoricalPerformanceTable';
import 'react-grid-layout/css/styles.css';
import 'react-resizable/css/styles.css';

const ResponsiveGridLayout = WidthProvider(Responsive);

const Dashboard = () => {
  const [data, setData] = useState([]);
  const [history, setHistory] = useState([]);
  const [fullHistory, setFullHistory] = useState([]);
  const [comparisonData, setComparisonData] = useState([]);
  const [currentTask, setCurrentTask] = useState(null);
  const [latestResults, setLatestResults] = useState(null);
  const [isConnected, setIsConnected] = useState(false);
  const [isPaused, setIsPaused] = useState(false);
  const [activeView, setActiveView] = useState('global'); // 'global', 'history', 'enhanced', or scheduler_id
  const [notification, setNotification] = useState(null);

  const wsRef = useRef(null);

  const SCHEDULERS = [
    { id: 'hybrid_ml', name: 'Hybrid ML', icon: Brain, color: '#06b6d4' },
    { id: 'rl_agent', name: 'RL Agent', icon: Zap, color: '#8b5cf6' },
    { id: 'oracle', name: 'Oracle', icon: Target, color: '#10b981' },
    { id: 'round_robin', name: 'Round Robin', icon: Layers, color: '#f59e0b' },
    { id: 'random', name: 'Random', icon: Shuffle, color: '#ef4444' },
    { id: 'greedy', name: 'Greedy', icon: MousePointer2, color: '#ec4899' },
  ];

  // Default Layouts for Scheduler View
  const defaultLayouts = {
    lg: [
      { i: 'metrics', x: 0, y: 0, w: 12, h: 2, static: true },
      { i: 'cluster_load', x: 0, y: 2, w: 8, h: 8 },
      { i: 'oracle_vs', x: 8, y: 2, w: 4, h: 6 },
      { i: 'resource_split', x: 8, y: 8, w: 4, h: 6 },
      { i: 'logs', x: 0, y: 10, w: 8, h: 6 }
    ]
  };

  const [layouts, setLayouts] = useState(defaultLayouts);

  // WebSocket Connection
  useEffect(() => {
    const connect = () => {
      const ws = new WebSocket('ws://localhost:8000/ws');
      wsRef.current = ws;

      ws.onopen = () => {
        console.log('âœ… WebSocket connected');
        setIsConnected(true);
      };
      ws.onclose = () => {
        console.log('âŒ WebSocket disconnected');
        setIsConnected(false);
        setTimeout(connect, 3000);
      };

      ws.onmessage = (event) => {
        const message = JSON.parse(event.data);
        console.log('ğŸ“¨ WebSocket message received:', message.type);

        if (message.type === 'notification') {
          setNotification(message.message);
          setTimeout(() => setNotification(null), 5000);
          return;
        }

        if (message.type === 'simulation_update') {
          console.log('âœ… Simulation update - latestResults:', message.latest_results ? 'YES' : 'NO');
          const newData = transformData(message);
          if (newData) {
            setData(prev => {
              const updated = [...prev, newData].slice(-50);
              console.log('ğŸ“Š Data array length:', updated.length);
              return updated;
            });
          }
          setHistory(prev => [message, ...prev].slice(0, 50));
          setComparisonData(message.comparison);
          setCurrentTask(message.task);
          setLatestResults(message.latest_results);
        }
      };
    };
    connect();
    return () => wsRef.current?.close();
  }, []);

  // Fetch Full History when view is history
  useEffect(() => {
    if (activeView === 'history' || activeView === 'enhanced') {
      fetch('http://localhost:8000/api/full_history')
        .then(res => res.json())
        .then(data => {
          console.log('Fetched full history, length:', data.length);
          setFullHistory(data);
        })
        .catch(err => console.error("Failed to fetch full history", err));
    }
  }, [activeView]);

  const togglePause = async () => {
    const newState = !isPaused;
    setIsPaused(newState);
    try {
      await fetch(`http://localhost:8000/api/${newState ? 'pause' : 'resume'}`, { method: 'POST' });
    } catch (e) {
      console.error("Failed to toggle simulation", e);
    }
  };

  const clearHistory = async () => {
    if (confirm("Are you sure you want to clear all historical data?")) {
      await fetch('http://localhost:8000/api/history', { method: 'DELETE' });
      setFullHistory([]);
      setNotification("History Cleared");
    }
  };

  const transformData = (message) => {
    try {
      const util = message.utilization;
      if (!util) {
        console.error('âŒ No utilization data in message');
        return null;
      }
      const transformed = {
        time: new Date().toLocaleTimeString(),
        avgUtil: (util.average_utilization * 100).toFixed(1),
        gpu0: (util.gpu_0?.utilization || 0) * 100,
        gpu1: (util.gpu_1?.utilization || 0) * 100,
        gpu2: (util.gpu_2?.utilization || 0) * 100,
        gpu3: (util.gpu_3?.utilization || 0) * 100,
        raw: message
      };
      console.log('âœ… Transformed data:', transformed);
      return transformed;
    } catch (error) {
      console.error('âŒ Error in transformData:', error);
      return null;
    }
  };

  // --- Derived Metrics for Selected Scheduler ---
  const getSelectedMetrics = () => {
    if (!latestResults || !latestResults[activeView]) return null;
    const res = latestResults[activeView];
    return {
      time: res.time,
      energy: res.energy,
      cost: res.cost,
      efficiency: (1.0 / (res.cost + 0.00001)).toFixed(0) // Tasks per $
    };
  };

  const selectedMetrics = getSelectedMetrics();

  // Filter history for selected scheduler log
  const selectedHistory = history.map(h => ({
    time: new Date().toLocaleTimeString(),
    task_id: h.task.id,
    result: h.latest_results[activeView]
  }));

  // Radar Data: Selected vs Oracle
  const radarData = latestResults ? [
    { subject: 'Time', A: latestResults[activeView]?.time || 0, B: latestResults.oracle?.time || 0, fullMark: 10 },
    // Energy is usually 100-300J. Divide by 200 to map to ~0.5-1.5 range
    { subject: 'Energy', A: (latestResults[activeView]?.energy || 0) / 200, B: (latestResults.oracle?.energy || 0) / 200, fullMark: 10 },
    // Cost is tiny (~5e-6). Multiply by 200,000 to map to ~1.0 range
    { subject: 'Cost', A: (latestResults[activeView]?.cost || 0) * 200000, B: (latestResults.oracle?.cost || 0) * 200000, fullMark: 10 },
  ] : [];

  // Pie Data: GPU vs CPU Split (Mocked based on scheduler logic)
  const getSplitData = () => {
    if (activeView === 'rl_agent') return [{ name: 'GPU', value: 90, fill: '#8b5cf6' }, { name: 'CPU', value: 10, fill: '#4b5563' }];
    if (activeView === 'round_robin') return [{ name: 'GPU', value: 50, fill: '#f59e0b' }, { name: 'CPU', value: 50, fill: '#4b5563' }];
    if (activeView === 'random') return [{ name: 'GPU', value: 40, fill: '#ef4444' }, { name: 'CPU', value: 60, fill: '#4b5563' }];
    const gpuVal = currentTask?.intensity > 0.5 ? 80 : 20;
    return [{ name: 'GPU', value: gpuVal, fill: '#06b6d4' }, { name: 'CPU', value: 100 - gpuVal, fill: '#4b5563' }];
  };

  // Historical Trend Data
  const trendData = fullHistory.map((item, idx) => ({
    id: idx,
    optimal_time: item.optimal_time,
    size: item.size
  }));

  // Render Content based on Active View
  const renderContent = () => {
    if (activeView === 'global') {
      return (
        <div className="grid grid-cols-12 gap-6 p-6">
          <div className="col-span-12 md:col-span-6 glass-panel p-6 relative overflow-hidden group" style={{ height: 450 }}>
            <div className="absolute inset-0 bg-gradient-to-br from-cyan-500/5 to-transparent opacity-0 group-hover:opacity-100 transition-opacity duration-500" />
            <h2 className="text-sm font-bold text-gray-300 mb-4 flex items-center gap-2 relative z-10">
              <BarChart2 size={16} className="text-cyan-400" /> PERFORMANCE RACE
            </h2>
            <div className="w-full relative z-10" style={{ height: 380 }}>
              <ResponsiveContainer width="100%" height="100%">
                <BarChart data={comparisonData} layout="vertical" margin={{ left: 40, right: 30, top: 10, bottom: 10 }}>
                  <CartesianGrid strokeDasharray="3 3" stroke="#374151" horizontal={false} />
                  <XAxis type="number" stroke="#6b7280" tick={{ fontSize: 10 }} />
                  <YAxis dataKey="name" type="category" stroke="#9ca3af" width={80} tick={{ fontSize: 11, fontWeight: 500 }} />
                  <Tooltip cursor={{ fill: 'rgba(255,255,255,0.05)' }} contentStyle={{ backgroundColor: '#18181b', border: '1px solid #3f3f46', borderRadius: '8px' }} />
                  <Bar dataKey="avg_time" radius={[0, 4, 4, 0]} barSize={24}>
                    {comparisonData.map((entry, index) => (
                      <Cell key={`cell-${index}`} fill={
                        entry.name === 'oracle' ? '#10b981' :
                          entry.name === 'hybrid_ml' ? '#06b6d4' :
                            entry.name === 'rl_agent' ? '#8b5cf6' : '#4b5563'
                      } />
                    ))}
                  </Bar>
                </BarChart>
              </ResponsiveContainer>
            </div>
          </div>
          <div className="col-span-12 md:col-span-6 glass-panel p-6 relative overflow-hidden group" style={{ height: 450 }}>
            <div className="absolute inset-0 bg-gradient-to-bl from-pink-500/5 to-transparent opacity-0 group-hover:opacity-100 transition-opacity duration-500" />
            <h2 className="text-sm font-bold text-gray-300 mb-4 flex items-center gap-2 relative z-10">
              <Database size={16} className="text-pink-400" /> WORKLOAD DISTRIBUTION
            </h2>
            <div className="w-full relative z-10" style={{ height: 380 }}>
              <ResponsiveContainer width="100%" height="100%">
                <ScatterChart margin={{ top: 10, right: 10, bottom: 10, left: 0 }}>
                  <CartesianGrid strokeDasharray="3 3" stroke="#374151" />
                  <XAxis type="number" dataKey="x" name="Size" stroke="#6b7280" tick={{ fontSize: 10 }} label={{ value: 'Task Size', position: 'insideBottom', offset: -5, fill: '#6b7280', fontSize: 10 }} />
                  <YAxis type="number" dataKey="y" name="Time" stroke="#6b7280" tick={{ fontSize: 10 }} label={{ value: 'Execution Time (s)', angle: -90, position: 'insideLeft', fill: '#6b7280', fontSize: 10 }} />
                  <Tooltip cursor={{ strokeDasharray: '3 3' }} contentStyle={{ backgroundColor: '#18181b', border: '1px solid #3f3f46', borderRadius: '8px' }} />
                  <Scatter name="Tasks" data={history.map(h => ({ x: h.task?.size || 0, y: h.latest_results?.hybrid_ml?.time || 0 }))} fill="#f472b6" shape="circle" />
                </ScatterChart>
              </ResponsiveContainer>
            </div>
          </div>

          {/* Historical Data Section */}
          <div className="col-span-12 glass-panel p-6 relative overflow-hidden group">
            <HistoricalPerformanceTable />
          </div>
        </div>
      );
    }

    if (activeView === 'history') {
      return (
        <div className="p-6 h-full flex flex-col">
          <div className="glass-panel p-8 text-center flex-1 flex flex-col relative overflow-hidden">
            <div className="absolute inset-0 bg-[radial-gradient(ellipse_at_center,_var(--tw-gradient-stops))] from-purple-900/20 via-black to-black -z-10" />
            <div className="flex justify-between items-center mb-6">
              <h2 className="text-2xl font-bold text-white flex items-center gap-3">
                <History className="text-purple-400" /> Historical Analysis Mode
              </h2>
              <div className="flex items-center gap-4">
                <span className="text-sm text-gray-400 font-mono">
                  DATA POINTS: <span className="text-white font-bold">{fullHistory.length}</span>
                </span>
                <button onClick={clearHistory} className="text-red-400 hover:text-red-300 hover:bg-red-500/10 transition-colors flex items-center gap-2 text-xs px-4 py-2 border border-red-500/30 rounded-md">
                  <Trash2 size={14} /> Clear Data
                </button>
              </div>
            </div>

            <div className="flex-1 min-h-[400px] w-full bg-black/20 rounded-xl border border-white/5 p-4">
              {fullHistory.length > 0 ? (
                <ResponsiveContainer width="100%" height="100%">
                  <LineChart data={trendData}>
                    <CartesianGrid strokeDasharray="3 3" stroke="#374151" vertical={false} />
                    <XAxis dataKey="id" stroke="#6b7280" tick={{ fontSize: 10 }} label={{ value: 'Task ID', position: 'insideBottom', offset: -5, fill: '#6b7280', fontSize: 10 }} />
                    <YAxis stroke="#6b7280" tick={{ fontSize: 10 }} label={{ value: 'Optimal Time (s)', angle: -90, position: 'insideLeft', fill: '#6b7280', fontSize: 10 }} />
                    <Tooltip contentStyle={{ backgroundColor: '#18181b', border: '1px solid #3f3f46', borderRadius: '8px' }} />
                    <Legend />
                    <Line type="monotone" dataKey="optimal_time" name="Optimal Execution Time" stroke="#10b981" strokeWidth={2} dot={false} activeDot={{ r: 4 }} />
                  </LineChart>
                </ResponsiveContainer>
              ) : (
                <div className="flex flex-col items-center justify-center h-full text-gray-500">
                  <Database size={48} className="mb-4 opacity-20" />
                  <p className="text-lg font-semibold">No historical data available yet</p>
                  <p className="text-sm mt-2 text-gray-600">The simulation is collecting data points...</p>
                  <p className="text-xs mt-4 text-gray-700">Current data points: {fullHistory.length}</p>
                </div>
              )}
            </div>
          </div>
        </div>
      );
    }

    if (activeView === 'enhanced') {
      return (
        <EnhancedVisualizationsDemo
          history={history}
          fullHistory={fullHistory}
          comparisonData={comparisonData}
          latestResults={latestResults}
        />
      );
    }

    // Scheduler View (Grid Layout)
    return (
      <ResponsiveGridLayout
        className="layout p-6"
        layouts={layouts}
        breakpoints={{ lg: 1200, md: 996, sm: 768, xs: 480, xxs: 0 }}
        cols={{ lg: 12, md: 10, sm: 6, xs: 4, xxs: 2 }}
        rowHeight={30}
        onLayoutChange={(layout, layouts) => setLayouts(layouts)}
        draggableHandle=".drag-handle"
        margin={[24, 24]}
      >
        <div key="metrics" className="glass-panel p-0 flex justify-between items-center overflow-hidden">
          <div className="absolute inset-0 bg-gradient-to-r from-cyan-500/10 via-transparent to-transparent pointer-events-none" />
          <div className="flex gap-0 w-full h-full relative z-10 divide-x divide-white/5">
            <div className="flex-1 p-4 flex flex-col justify-center">
              <div className="text-[10px] font-bold text-gray-500 tracking-wider mb-1">AVG LATENCY</div>
              <div className="text-2xl font-bold text-neon-cyan font-mono">{selectedMetrics?.time.toFixed(4) || '-'}s</div>
            </div>
            <div className="flex-1 p-4 flex flex-col justify-center">
              <div className="text-[10px] font-bold text-gray-500 tracking-wider mb-1">ENERGY / TASK</div>
              <div className="text-2xl font-bold text-neon-purple font-mono">{selectedMetrics?.energy.toFixed(2) || '-'}J</div>
            </div>
            <div className="flex-1 p-4 flex flex-col justify-center">
              <div className="text-[10px] font-bold text-gray-500 tracking-wider mb-1">COST EFFICIENCY</div>
              <div className="text-2xl font-bold text-white font-mono">{selectedMetrics?.efficiency || '-'} T/$</div>
            </div>
          </div>
          <div className="h-full w-8 bg-white/5 flex items-center justify-center cursor-move drag-handle hover:bg-white/10 transition-colors">
            <GripHorizontal className="text-gray-500" size={16} />
          </div>
        </div>

        <div key="cluster_load" className="glass-panel p-0 flex flex-col overflow-hidden">
          <div className="p-3 border-b border-white/5 bg-black/20 flex justify-between items-center cursor-move drag-handle">
            <h3 className="text-xs font-bold text-gray-300 flex items-center gap-2">
              <Activity size={14} className="text-cyan-400" /> VIRTUAL CLUSTER LOAD
            </h3>
            <GripHorizontal className="text-gray-600" size={14} />
          </div>
          <div className="flex-1 min-h-0 w-full p-4">
            {data.length > 0 ? (
              <ResponsiveContainer width="100%" height="100%">
                <AreaChart data={data}>
                  <defs>
                    <linearGradient id="colorUtil" x1="0" y1="0" x2="0" y2="1">
                      <stop offset="5%" stopColor={SCHEDULERS.find(s => s.id === activeView)?.color} stopOpacity={0.4} />
                      <stop offset="95%" stopColor={SCHEDULERS.find(s => s.id === activeView)?.color} stopOpacity={0} />
                    </linearGradient>
                  </defs>
                  <CartesianGrid strokeDasharray="3 3" stroke="#374151" vertical={false} />
                  <XAxis dataKey="time" stroke="#6b7280" tick={{ fontSize: 9 }} interval={10} />
                  <YAxis stroke="#6b7280" tick={{ fontSize: 9 }} domain={[0, 100]} />
                  <Tooltip contentStyle={{ backgroundColor: '#18181b', border: '1px solid #3f3f46', borderRadius: '6px' }} />
                  <Area type="monotone" dataKey="avgUtil" stroke={SCHEDULERS.find(s => s.id === activeView)?.color} strokeWidth={2} fill="url(#colorUtil)" />
                </AreaChart>
              </ResponsiveContainer>
            ) : <div className="absolute inset-0 flex items-center justify-center text-gray-500 text-xs">Waiting for simulation data...</div>}
          </div>
        </div>


        <div key="oracle_vs" className="glass-panel p-0 flex flex-col overflow-hidden">
          <div className="p-3 border-b border-white/5 bg-black/20 flex justify-between items-center cursor-move drag-handle">
            <h3 className="text-xs font-bold text-gray-300 flex items-center gap-2">
              <Target size={14} className="text-green-400" /> VS ORACLE BASELINE
            </h3>
            <GripHorizontal className="text-gray-600" size={14} />
          </div>
          <div className="flex-1 min-h-0 w-full p-2">
            {latestResults ? (
              <ResponsiveContainer width="100%" height="100%">
                <RadarChart cx="50%" cy="50%" outerRadius="65%" data={radarData}>
                  <PolarGrid stroke="#374151" />
                  <PolarAngleAxis dataKey="subject" tick={{ fill: '#9ca3af', fontSize: 10, fontWeight: 500 }} />
                  <PolarRadiusAxis angle={30} domain={[0, 'auto']} tick={false} axisLine={false} />
                  <Radar name={SCHEDULERS.find(s => s.id === activeView)?.name} dataKey="A" stroke={SCHEDULERS.find(s => s.id === activeView)?.color} fill={SCHEDULERS.find(s => s.id === activeView)?.color} fillOpacity={0.4} />
                  <Radar name="Oracle" dataKey="B" stroke="#10b981" fill="#10b981" fillOpacity={0.1} />
                  <Legend wrapperStyle={{ fontSize: '10px', bottom: 0 }} />
                  <Tooltip contentStyle={{ backgroundColor: '#18181b', border: '1px solid #3f3f46', borderRadius: '6px' }} />
                </RadarChart>
              </ResponsiveContainer>
            ) : <div className="flex items-center justify-center h-full text-gray-500 text-xs">Waiting for simulation data...</div>}
          </div>

        </div>
        <div key="resource_split" className="glass-panel p-0 flex flex-col overflow-hidden">
          <div className="p-3 border-b border-white/5 bg-black/20 flex justify-between items-center cursor-move drag-handle">
            <h3 className="text-xs font-bold text-gray-300 flex items-center gap-2">
              <Server size={14} className="text-yellow-400" /> RESOURCE SPLIT
            </h3>
            <GripHorizontal className="text-gray-600" size={14} />
          </div>
          <div className="flex-1 min-h-0 w-full p-2">
            <ResponsiveContainer width="100%" height="100%">
              <PieChart>
                <Pie data={getSplitData()} cx="50%" cy="50%" innerRadius={45} outerRadius={65} paddingAngle={5} dataKey="value">
                  {getSplitData().map((entry, index) => (
                    <Cell key={`cell-${index}`} fill={entry.fill} stroke="rgba(0,0,0,0.5)" />
                  ))}
                </Pie>
                <Tooltip contentStyle={{ backgroundColor: '#18181b', border: '1px solid #3f3f46', borderRadius: '6px' }} />
                <Legend wrapperStyle={{ fontSize: '10px', bottom: 0 }} />
              </PieChart>
            </ResponsiveContainer>
          </div>
        </div>

        <div key="logs" className="glass-panel p-0 flex flex-col overflow-hidden">
          <div className="p-3 border-b border-white/5 bg-black/20 flex justify-between items-center cursor-move drag-handle">
            <h3 className="text-xs font-bold text-gray-300 flex items-center gap-2">
              <Terminal size={14} className="text-gray-400" /> {SCHEDULERS.find(s => s.id === activeView)?.name.toUpperCase()} LOGS
            </h3>
            <GripHorizontal className="text-gray-600" size={14} />
          </div>
          <div className="flex-1 overflow-y-auto p-3 space-y-1 font-mono text-[10px] bg-black/10">
            {selectedHistory.map((item, idx) => (
              <div key={idx} className="flex items-center justify-between p-2 rounded bg-white/5 border border-white/5 hover:bg-white/10 transition-colors">
                <span className="text-gray-500 w-16">{item.time}</span>
                <span className="text-cyan-400 w-16">TASK-{item.task_id}</span>
                <div className="flex-1 flex justify-end gap-4">
                  <span className="text-gray-300">T: <span className="text-white">{item.result?.time.toFixed(3)}s</span></span>
                  <span className="text-yellow-500/80">E: <span className="text-yellow-400">{item.result?.energy.toFixed(1)}J</span></span>
                </div>
              </div>
            ))}
          </div>
        </div>
      </ResponsiveGridLayout >
    );
  };

  return (
    <div className="min-h-screen grid-bg flex">
      {/* Sidebar */}
      <div className="w-64 glass-panel border-r border-white/5 flex flex-col z-20">
        <div className="p-4 border-b border-white/5">
          <h1 className="text-lg font-bold tracking-tight text-white glow-text flex items-center gap-2">
            <LayoutDashboard size={20} className="text-neon-cyan" />
            <span className="text-neon-cyan">SCHEDULER LAB</span>
          </h1>
          <p className="text-[10px] text-cyan-400/70 font-mono tracking-wider mt-1">VIRTUAL CLUSTER MANAGER</p>
        </div>

        <div className="flex-1 overflow-y-auto p-2 space-y-1">
          <div className="text-xs font-semibold text-gray-500 px-2 py-2">VIEWS</div>
          <button
            onClick={() => setActiveView('global')}
            className={`w-full flex items-center gap-3 p-3 rounded-lg transition-all ${activeView === 'global' ? 'bg-white/10 border border-white/10 text-white shadow-lg' : 'text-gray-400 hover:bg-white/5'}`}
          >
            <BarChart2 size={18} className="text-blue-400" />
            <span className="text-sm font-medium">Global Comparison</span>
          </button>
          <button
            onClick={() => setActiveView('history')}
            className={`w-full flex items-center gap-3 p-3 rounded-lg transition-all ${activeView === 'history' ? 'bg-white/10 border border-white/10 text-white shadow-lg' : 'text-gray-400 hover:bg-white/5'}`}
          >
            <History size={18} className="text-purple-400" />
            <span className="text-sm font-medium">Historical Analysis</span>
          </button>
          <button
            onClick={() => setActiveView('enhanced')}
            className={`w-full flex items-center gap-3 p-3 rounded-lg transition-all ${activeView === 'enhanced' ? 'bg-white/10 border border-white/10 text-white shadow-lg' : 'text-gray-400 hover:bg-white/5'}`}
          >
            <TrendingUp size={18} className="text-cyan-400" />
            <span className="text-sm font-medium">Enhanced Analytics</span>
          </button>

          <div className="text-xs font-semibold text-gray-500 px-2 py-2 mt-4">SCHEDULERS</div>
          <div className="space-y-1">
            {SCHEDULERS.map(s => (
              <button
                key={s.id}
                onClick={() => setActiveView(s.id)}
                className={`w-full flex items-center gap-3 p-3 rounded-lg transition-all ${activeView === s.id ? 'bg-white/10 border border-white/10 text-white shadow-lg' : 'text-gray-400 hover:bg-white/5'}`}
              >
                <s.icon size={18} style={{ color: s.color }} />
                <span className="text-sm font-medium">{s.name}</span>
              </button>
            ))}
          </div>
        </div>

        <div className="p-4 border-t border-white/5">
          <div className="flex items-center gap-2 px-3 py-2 rounded-lg bg-black/40 border border-white/5 mb-2">
            <Wifi size={14} className={isConnected ? "text-green-400" : "text-red-400"} />
            <span className="text-xs font-medium text-gray-300">{isConnected ? "ONLINE" : "OFFLINE"}</span>
          </div>
          <button
            onClick={togglePause}
            className={`w-full flex items-center justify-center gap-2 p-2 rounded-lg transition-colors border ${isPaused ? 'bg-green-500/20 border-green-500/50 text-green-300' : 'bg-yellow-500/20 border-yellow-500/50 text-yellow-300'}`}
          >
            {isPaused ? <Play size={16} /> : <Pause size={16} />}
            <span className="text-xs font-bold">{isPaused ? "RESUME SIM" : "PAUSE SIM"}</span>
          </button>
        </div>
      </div>

      {/* Main Content */}
      <div className="flex-1 flex flex-col h-screen overflow-hidden relative">
        {/* Notification Toast */}
        {notification && (
          <div className="absolute top-6 right-6 z-50 glass-panel p-4 border-l-4 border-l-green-500 animate-bounce">
            <p className="text-green-400 font-bold">SYSTEM UPDATE</p>
            <p className="text-sm">{notification}</p>
          </div>
        )}

        {/* Top Bar */}
        <header className="h-16 border-b border-white/5 flex justify-between items-center px-6 bg-black/20 backdrop-blur-sm">
          <div className="flex items-center gap-4">
            <h2 className="text-xl font-bold text-white flex items-center gap-2">
              {activeView === 'global' ? 'Global Comparison' :
                activeView === 'history' ? 'Historical Analysis' :
                  activeView === 'enhanced' ? 'Enhanced Analytics' :
                    SCHEDULERS.find(s => s.id === activeView)?.name}
            </h2>
          </div>
        </header>

        {/* Dashboard Content */}
        <div className="flex-1 overflow-y-auto">
          {renderContent()}
        </div>
      </div>
    </div>
  );
};

export default Dashboard;


======= FILE: dashboard/src/setupTests.js =======

import '@testing-library/jest-dom';


======= FILE: dashboard/src/App.test.jsx =======

import { render, screen, fireEvent, waitFor } from '@testing-library/react';
import { describe, it, expect, vi } from 'vitest';
import App from './App';

// Mock Recharts to avoid rendering issues in JSDOM
vi.mock('recharts', () => {
    const OriginalModule = vi.importActual('recharts');
    return {
        ...OriginalModule,
        ResponsiveContainer: ({ children }) => <div className="recharts-responsive-container" style={{ width: 800, height: 800 }}>{children}</div>,
        AreaChart: () => <div data-testid="area-chart">AreaChart</div>,
        Area: () => null,
        XAxis: () => null,
        YAxis: () => null,
        CartesianGrid: () => null,
        Tooltip: () => null,
        BarChart: () => <div data-testid="bar-chart">BarChart</div>,
        Bar: () => null,
        Cell: () => null,
        RadarChart: () => <div data-testid="radar-chart">RadarChart</div>,
        PolarGrid: () => null,
        PolarAngleAxis: () => null,
        PolarRadiusAxis: () => null,
        Radar: () => null,
        ScatterChart: () => <div data-testid="scatter-chart">ScatterChart</div>,
        Scatter: () => null,
        PieChart: () => <div data-testid="pie-chart">PieChart</div>,
        Pie: () => null,
        Legend: () => null,
        LineChart: () => <div data-testid="line-chart">LineChart</div>,
        Line: () => null,
    };
});

// Mock react-grid-layout
vi.mock('react-grid-layout', () => ({
    Responsive: ({ children }) => <div data-testid="grid-layout">{children}</div>,
    WidthProvider: (Component) => Component,
}));

describe('Dashboard Component', () => {
    it('renders the dashboard title', () => {
        render(<App />);
        expect(screen.getByText(/SCHEDULER LAB/i)).toBeInTheDocument();
        expect(screen.getByText(/VIRTUAL CLUSTER MANAGER/i)).toBeInTheDocument();
    });

    it('renders the sidebar with navigation options', () => {
        render(<App />);
        expect(screen.getAllByText(/Global Comparison/i)[0]).toBeInTheDocument();
        expect(screen.getByText(/Historical Analysis/i)).toBeInTheDocument();
        expect(screen.getByText(/Hybrid ML/i)).toBeInTheDocument();
        expect(screen.getByText(/RL Agent/i)).toBeInTheDocument();
    });

    it('switches to Global Comparison view by default', () => {
        render(<App />);
        expect(screen.getByText(/PERFORMANCE RACE/i)).toBeInTheDocument();
        expect(screen.getByText(/WORKLOAD DISTRIBUTION/i)).toBeInTheDocument();
        expect(screen.getByTestId('bar-chart')).toBeInTheDocument();
        expect(screen.getByTestId('scatter-chart')).toBeInTheDocument();
    });

    it('switches to Historical Analysis view when clicked', async () => {
        render(<App />);
        const historyButton = screen.getByText(/Historical Analysis/i);
        fireEvent.click(historyButton);

        await waitFor(() => {
            expect(screen.getByText(/Historical Analysis Mode/i)).toBeInTheDocument();
        });
        expect(screen.getByText(/Clear Data/i)).toBeInTheDocument();
    });

    it('switches to Scheduler view (Hybrid ML) when clicked', async () => {
        render(<App />);
        const hybridMLButton = screen.getByText(/Hybrid ML/i);
        fireEvent.click(hybridMLButton);

        await waitFor(() => {
            expect(screen.getByText(/VIRTUAL CLUSTER LOAD/i)).toBeInTheDocument();
        });
        expect(screen.getByText(/VS ORACLE BASELINE/i)).toBeInTheDocument();
        expect(screen.getByText(/RESOURCE SPLIT/i)).toBeInTheDocument();
        expect(screen.getByText(/HYBRID ML LOGS/i)).toBeInTheDocument();
    });

    it('toggles simulation pause/resume', () => {
        render(<App />);
        const pauseButton = screen.getByText(/PAUSE SIM/i);
        expect(pauseButton).toBeInTheDocument();

        // Mock fetch for API call
        global.fetch = vi.fn(() => Promise.resolve({ json: () => Promise.resolve({}) }));

        fireEvent.click(pauseButton);
        expect(screen.getByText(/RESUME SIM/i)).toBeInTheDocument();
        expect(global.fetch).toHaveBeenCalledWith(expect.stringContaining('/api/pause'), expect.anything());

        fireEvent.click(screen.getByText(/RESUME SIM/i));
        expect(screen.getByText(/PAUSE SIM/i)).toBeInTheDocument();
        expect(global.fetch).toHaveBeenCalledWith(expect.stringContaining('/api/resume'), expect.anything());
    });
});


======= FILE: dashboard/src/components/EnhancedVisualizationsDemo.jsx =======

import React, { useState, useEffect, useMemo } from 'react';
import { Activity, Gauge, TrendingUp, Target, Database, Layers } from 'lucide-react';
import GaugeChart from './GaugeChart.jsx';
import HeatmapChart from './HeatmapChart.jsx';
import StatsTable from './StatsTable.jsx';
import WinLossMatrix from './WinLossMatrix.jsx';
import CorrelationMatrix from './CorrelationMatrix.jsx';
import PercentileChart from './PercentileChart.jsx';
import TaskDistributionHistogram from './TaskDistributionHistogram.jsx';
import CumulativeChart from './CumulativeChart.jsx';

const EnhancedVisualizationsDemo = ({ history, fullHistory, comparisonData, latestResults }) => {
    const SCHEDULERS = [
        { id: 'hybrid_ml', name: 'Hybrid ML', icon: null, color: '#06b6d4' },
        { id: 'rl_agent', name: 'RL Agent', icon: null, color: '#a855f7' },
        { id: 'oracle', name: 'Oracle', icon: null, color: '#10b981' },
        { id: 'round_robin', name: 'Round Robin', icon: null, color: '#fbbf24' },
        { id: 'random', name: 'Random', icon: null, color: '#f87171' },
        { id: 'greedy', name: 'Greedy', icon: null, color: '#f472b6' },
    ];

    // Calculate all the enhanced metrics
    const schedulerStats = useMemo(() => {
        if (!history || history.length === 0) return {};

        const stats = {};
        SCHEDULERS.forEach(scheduler => {
            const times = history
                .map(h => h.latest_results?.[scheduler.id]?.time)
                .filter(t => t !== undefined && t !== null);

            if (times.length === 0) return;

            const sorted = [...times].sort((a, b) => a - b);
            const mean = times.reduce((a, b) => a + b, 0) / times.length;
            const median = sorted[Math.floor(sorted.length / 2)];
            const variance = times.reduce((sum, t) => sum + Math.pow(t - mean, 2), 0) / times.length;
            const std = Math.sqrt(variance);

            stats[scheduler.id] = {
                mean,
                median,
                std,
                min: Math.min(...times),
                max: Math.max(...times),
                count: times.length
            };
        });

        return stats;
    }, [history]);

    const winLossMatrix = useMemo(() => {
        if (!history || history.length < 2) return {};

        const matrix = {};
        SCHEDULERS.forEach(s1 => {
            matrix[s1.id] = {};
            SCHEDULERS.forEach(s2 => {
                if (s1.id === s2.id) return;

                let wins = 0;
                let total = 0;

                history.forEach(h => {
                    const time1 = h.latest_results?.[s1.id]?.time;
                    const time2 = h.latest_results?.[s2.id]?.time;

                    if (time1 !== undefined && time2 !== undefined) {
                        total++;
                        if (time1 < time2) wins++;
                    }
                });

                matrix[s1.id][s2.id] = total > 0 ? wins / total : null;
            });
        });

        return matrix;
    }, [history]);

    const correlationData = useMemo(() => {
        console.log('Computing correlationData, fullHistory length:', fullHistory?.length);
        if (!fullHistory || fullHistory.length < 5) return { correlations: {}, features: [] };

        const features = ['size', 'compute_intensity', 'memory_required', 'optimal_time'];
        const correlations = {};

        features.forEach(f1 => {
            correlations[f1] = {};
            features.forEach(f2 => {
                const values1 = fullHistory.map(h => h[f1] || 0);
                const values2 = fullHistory.map(h => h[f2] || 0);

                const mean1 = values1.reduce((a, b) => a + b, 0) / values1.length;
                const mean2 = values2.reduce((a, b) => a + b, 0) / values2.length;

                let num = 0, den1 = 0, den2 = 0;
                for (let i = 0; i < values1.length; i++) {
                    const diff1 = values1[i] - mean1;
                    const diff2 = values2[i] - mean2;
                    num += diff1 * diff2;
                    den1 += diff1 * diff1;
                    den2 += diff2 * diff2;
                }

                const correlation = den1 === 0 || den2 === 0 ? 0 : num / Math.sqrt(den1 * den2);
                correlations[f1][f2] = correlation;
            });
        });

        return { correlations, features };
    }, [fullHistory]);

    const heatmapData = useMemo(() => {
        console.log('Computing heatmapData, fullHistory length:', fullHistory?.length);
        if (!fullHistory || fullHistory.length < 5) return [];

        const bins = 5;
        const sizes = fullHistory.map(h => h.size || 0);
        const intensities = fullHistory.map(h => h.compute_intensity || 0);

        const minSize = Math.min(...sizes);
        const maxSize = Math.max(...sizes);
        const minInt = Math.min(...intensities);
        const maxInt = Math.max(...intensities);

        const sizeBinWidth = (maxSize - minSize) / bins || 1;
        const intBinWidth = (maxInt - minInt) / bins || 1;

        const grid = {};

        fullHistory.forEach(item => {
            const size = item.size || 0;
            const intensity = item.compute_intensity || 0;
            const time = item.optimal_time || 0;

            const sizeBin = Math.min(Math.floor((size - minSize) / sizeBinWidth), bins - 1);
            const intBin = Math.min(Math.floor((intensity - minInt) / intBinWidth), bins - 1);

            const key = `${sizeBin},${intBin}`;
            if (!grid[key]) grid[key] = { count: 0, totalTime: 0, sizeBin, intBin };
            grid[key].count++;
            grid[key].totalTime += time;
        });

        return Object.values(grid).map(cell => ({
            x: minSize + (cell.sizeBin + 0.5) * sizeBinWidth,
            y: minInt + (cell.intBin + 0.5) * intBinWidth,
            value: cell.totalTime / cell.count
        }));
    }, [fullHistory]);

    const cumulativeData = useMemo(() => {
        if (!history || history.length === 0) return [];

        const cumulative = { index: [] };
        SCHEDULERS.forEach(s => { cumulative[s.id] = []; });

        let running = {};
        SCHEDULERS.forEach(s => running[s.id] = 0);

        history.forEach((h, idx) => {
            cumulative.index.push(idx);
            SCHEDULERS.forEach(s => {
                const time = h.latest_results?.[s.id]?.time || 0;
                running[s.id] += time;
                cumulative[s.id].push(running[s.id]);
            });
        });

        return cumulative.index.map((idx, i) => {
            const entry = { index: idx };
            SCHEDULERS.forEach(s => { entry[s.id] = cumulative[s.id][i]; });
            return entry;
        });
    }, [history]);

    const efficiencyGauges = useMemo(() => {
        if (!latestResults || !latestResults.oracle) return {};

        const gauges = {};
        const oracleTime = latestResults.oracle.time;

        SCHEDULERS.forEach(s => {
            if (s.id === 'oracle') {
                gauges[s.id] = 100;
            } else {
                const schedulerTime = latestResults[s.id]?.time || Infinity;
                const efficiency = Math.min(100, (oracleTime / schedulerTime) * 100);
                gauges[s.id] = efficiency;
            }
        });

        return gauges;
    }, [latestResults]);

    return (
        <div className="p-6 grid grid-cols-12 gap-4 overflow-y-auto" style={{ maxHeight: '90vh' }}>

            {/* Efficiency Gauges */}
            <div className="col-span-12 glass-panel p-4">
                <h2 className="text-sm font-bold text-gray-300 mb-3 flex items-center gap-2">
                    <Gauge size={16} className="text-cyan-400" /> SCHEDULER EFFICIENCY (vs Oracle)
                </h2>
                <div className="grid grid-cols-5 gap-4">
                    {SCHEDULERS.filter(s => s.id !== 'oracle').map(s => (
                        <div key={s.id} className="flex flex-col items-center p-2 rounded-lg bg-black/20 border border-white/5">
                            <GaugeChart value={efficiencyGauges[s.id] || 0} max={100} label={s.name} color={s.color} size={100} />
                        </div>
                    ))}
                </div>
            </div>

            {/* Statistical Table */}
            <div className="col-span-12 glass-panel p-6">
                <h2 className="text-sm font-bold text-gray-300 mb-4 flex items-center gap-2">
                    <TrendingUp size={16} className="text-green-400" /> STATISTICAL COMPARISON
                </h2>
                <StatsTable data={schedulerStats} schedulers={SCHEDULERS} />
            </div>

            {/* Win/Loss Matrix */}
            <div className="col-span-12 md:col-span-6 glass-panel p-6" style={{ height: 450 }}>
                <h2 className="text-sm font-bold text-gray-300 mb-4 flex items-center gap-2">
                    <Target size={16} className="text-yellow-400" /> WIN/LOSS MATRIX
                </h2>
                <div className="flex justify-center" style={{ height: 380 }}>
                    <WinLossMatrix matrix={winLossMatrix} schedulers={SCHEDULERS} />
                </div>
            </div>

            {/* Performance Heatmap */}
            <div className="col-span-12 md:col-span-6 glass-panel p-6" style={{ height: 450 }}>
                <h2 className="text-sm font-bold text-gray-300 mb-4 flex items-center gap-2">
                    <Activity size={16} className="text-red-400" /> PERFORMANCE HEATMAP
                </h2>
                <div className="flex justify-center" style={{ height: 380 }}>
                    <HeatmapChart data={heatmapData} xLabel="Task Size" yLabel="Compute Intensity" />
                </div>
            </div>

            {/* Task Distribution Histogram */}
            <div className="col-span-12 md:col-span-6 glass-panel p-6" style={{ height: 400 }}>
                <h2 className="text-sm font-bold text-gray-300 mb-4 flex items-center gap-2">
                    <Database size={16} className="text-purple-400" /> TASK SIZE DISTRIBUTION
                </h2>
                <div style={{ height: 330 }}>
                    <TaskDistributionHistogram tasks={history || []} bins={12} />
                </div>
            </div>

            {/* Correlation Matrix */}
            <div className="col-span-12 md:col-span-6 glass-panel p-6" style={{ height: 400 }}>
                <h2 className="text-sm font-bold text-gray-300 mb-4 flex items-center gap-2">
                    <Activity size={16} className="text-blue-400" /> FEATURE CORRELATION MATRIX
                </h2>
                <div className="flex justify-center" style={{ height: 330 }}>
                    <CorrelationMatrix correlations={correlationData.correlations} features={correlationData.features} />
                </div>
            </div>

            {/* Cumulative Performance */}
            <div className="col-span-12 glass-panel p-6" style={{ height: 450 }}>
                <h2 className="text-sm font-bold text-gray-300 mb-4 flex items-center gap-2">
                    <Layers size={16} className="text-blue-400" /> CUMULATIVE PERFORMANCE
                </h2>
                <div style={{ height: 380 }}>
                    <CumulativeChart
                        data={cumulativeData}
                        dataKeys={SCHEDULERS.map(s => s.id)}
                        colors={Object.fromEntries(SCHEDULERS.map(s => [s.id, s.color]))}
                    />
                </div>
            </div>

        </div>
    );
};

export default EnhancedVisualizationsDemo;


======= FILE: dashboard/src/components/TaskDistributionHistogram.jsx =======

import React from 'react';
import { BarChart, Bar, XAxis, YAxis, CartesianGrid, Tooltip, ResponsiveContainer, Cell } from 'recharts';

const TaskDistributionHistogram = ({ tasks, bins = 10 }) => {
    if (!tasks || tasks.length === 0) {
        return (
            <div className="flex items-center justify-center h-full text-gray-500 text-sm">
                No task data available
            </div>
        );
    }

    // Extract sizes
    const sizes = tasks.map(t => t.task?.size || t.size || 0);
    const minSize = Math.min(...sizes);
    const maxSize = Math.max(...sizes);
    const binWidth = (maxSize - minSize) / bins || 1;

    // Create histogram bins
    const histogram = Array(bins).fill(0).map((_, i) => ({
        range: `${(minSize + i * binWidth).toFixed(0)}-${(minSize + (i + 1) * binWidth).toFixed(0)}`,
        count: 0,
        binStart: minSize + i * binWidth
    }));

    // Fill bins
    sizes.forEach(size => {
        const binIndex = Math.min(Math.floor((size - minSize) / binWidth), bins - 1);
        histogram[binIndex].count++;
    });

    return (
        <ResponsiveContainer width="100%" height="100%">
            <BarChart data={histogram} margin={{ top: 10, right: 20, left: 0, bottom: 20 }}>
                <CartesianGrid strokeDasharray="3 3" stroke="#374151" vertical={false} />
                <XAxis
                    dataKey="range"
                    stroke="#6b7280"
                    tick={{ fontSize: 9, angle: -45, textAnchor: 'end' }}
                    height={60}
                    label={{ value: 'Task Size Range', position: 'insideBottom', offset: -15, fill: '#9ca3af', fontSize: 10 }}
                />
                <YAxis
                    stroke="#6b7280"
                    tick={{ fontSize: 10 }}
                    label={{ value: 'Frequency', angle: -90, position: 'insideLeft', fill: '#9ca3af', fontSize: 10 }}
                />
                <Tooltip
                    contentStyle={{
                        backgroundColor: '#18181b',
                        border: '1px solid #3f3f46',
                        borderRadius: '8px'
                    }}
                />
                <Bar dataKey="count" radius={[4, 4, 0, 0]}>
                    {histogram.map((entry, index) => (
                        <Cell
                            key={`cell-${index}`}
                            fill={`hsl(${180 + (index / bins) * 100}, 70%, 50%)`}
                        />
                    ))}
                </Bar>
            </BarChart>
        </ResponsiveContainer>
    );
};

export default TaskDistributionHistogram;


======= FILE: dashboard/src/components/CumulativeChart.jsx =======

import React from 'react';
import { ComposedChart, Line, Area, XAxis, YAxis, CartesianGrid, Tooltip, Legend, ResponsiveContainer } from 'recharts';

const CumulativeChart = ({ data, dataKeys, colors }) => {
    // data: [{ time: 0, scheduler_a: 10, scheduler_b: 12 }]
    // dataKeys: ['scheduler_a', 'scheduler_b']
    // colors: { scheduler_a: '#06b6d4', scheduler_b: '#8b5cf6' }

    if (!data || data.length === 0) {
        return (
            <div className="flex items-center justify-center h-full text-gray-500 text-sm">
                No cumulative data available
            </div>
        );
    }

    return (
        <ResponsiveContainer width="100%" height="100%">
            <ComposedChart data={data} margin={{ top: 10, right: 30, left: 0, bottom: 10 }}>
                <defs>
                    {dataKeys.map(key => (
                        <linearGradient key={key} id={`gradient-${key}`} x1="0" y1="0" x2="0" y2="1">
                            <stop offset="5%" stopColor={colors[key]} stopOpacity={0.3} />
                            <stop offset="95%" stopColor={colors[key]} stopOpacity={0} />
                        </linearGradient>
                    ))}
                </defs>
                <CartesianGrid strokeDasharray="3 3" stroke="#374151" vertical={false} />
                <XAxis
                    dataKey="index"
                    stroke="#6b7280"
                    tick={{ fontSize: 10 }}
                    label={{ value: 'Task Index', position: 'insideBottom', offset: -5, fill: '#9ca3af', fontSize: 10 }}
                />
                <YAxis
                    stroke="#6b7280"
                    tick={{ fontSize: 10 }}
                    label={{ value: 'Cumulative Time (s)', angle: -90, position: 'insideLeft', fill: '#9ca3af', fontSize: 10 }}
                />
                <Tooltip
                    contentStyle={{
                        backgroundColor: '#18181b',
                        border: '1px solid #3f3f46',
                        borderRadius: '8px',
                        boxShadow: '0 4px 6px rgba(0,0,0,0.3)'
                    }}
                    formatter={(value) => value.toFixed(2) + 's'}
                />
                <Legend
                    wrapperStyle={{ fontSize: '11px', paddingTop: '10px' }}
                    iconType="line"
                />

                {dataKeys.map(key => (
                    <React.Fragment key={key}>
                        <Area
                            type="monotone"
                            dataKey={key}
                            fill={`url(#gradient-${key})`}
                            stroke="none"
                        />
                        <Line
                            type="monotone"
                            dataKey={key}
                            stroke={colors[key]}
                            strokeWidth={2}
                            dot={false}
                            name={key.replace(/_/g, ' ').toUpperCase()}
                        />
                    </React.Fragment>
                ))}
            </ComposedChart>
        </ResponsiveContainer>
    );
};

export default CumulativeChart;


======= FILE: dashboard/src/components/CorrelationMatrix.jsx =======

import React from 'react';

const CorrelationMatrix = ({ correlations, features }) => {
    // correlations: { feature_a: { feature_b: 0.85 } }
    // features: ['size', 'intensity', 'memory', 'optimal_time']

    if (!correlations || !features || features.length === 0) {
        return (
            <div className="flex items-center justify-center h-32 text-gray-500 text-sm">
                No correlation data available
            </div>
        );
    }

    const cellSize = 50;
    const padding = 100;
    const size = features.length * cellSize + padding;
    const legendWidth = 80;
    const totalWidth = size + legendWidth;

    const getColor = (correlation) => {
        if (correlation === null || correlation === undefined) return '#1f2937';

        // Blue (negative) -> White (0) -> Red (positive)
        const abs = Math.abs(correlation);

        if (correlation < 0) {
            // Negative correlation: darker blue
            return `rgb(${Math.round(59 * (1 - abs))}, ${Math.round(130 * (1 - abs))}, ${Math.round(246)})`;
        } else {
            // Positive correlation: darker red
            return `rgb(${Math.round(239)}, ${Math.round(68 * (1 - abs))}, ${Math.round(68 * (1 - abs))})`;
        }
    };

    const featureLabels = {
        size: 'Task Size',
        intensity: 'Compute Int.',
        compute_intensity: 'Compute Int.',
        memory: 'Memory',
        memory_required: 'Memory',
        optimal_time: 'Opt. Time',
        optimal_gpu_fraction: 'GPU Frac.'
    };

    return (
        <div className="overflow-auto">
            <svg width={totalWidth} height={size}>
                {/* Grid */}
                {features.map((rowFeature, ri) => (
                    features.map((colFeature, ci) => {
                        const corr = rowFeature === colFeature ? 1 :
                            (correlations[rowFeature]?.[colFeature] ?? correlations[colFeature]?.[rowFeature] ?? 0);

                        return (
                            <g key={`${rowFeature}-${colFeature}`}>
                                <rect
                                    x={padding + ci * cellSize}
                                    y={padding + ri * cellSize}
                                    width={cellSize - 1}
                                    height={cellSize - 1}
                                    fill={getColor(corr)}
                                    stroke="#000"
                                    strokeWidth={0.5}
                                />
                                <text
                                    x={padding + ci * cellSize + cellSize / 2}
                                    y={padding + ri * cellSize + cellSize / 2 + 4}
                                    textAnchor="middle"
                                    style={{
                                        fontSize: 11,
                                        fill: Math.abs(corr) > 0.5 ? '#fff' : '#9ca3af',
                                        fontWeight: 700
                                    }}
                                >
                                    {corr.toFixed(2)}
                                </text>
                            </g>
                        );
                    })
                ))}

                {/* Row labels */}
                {features.map((feature, i) => (
                    <text
                        key={`row-${feature}`}
                        x={padding - 10}
                        y={padding + i * cellSize + cellSize / 2 + 4}
                        textAnchor="end"
                        style={{ fontSize: 10, fill: '#9ca3af', fontWeight: 600 }}
                    >
                        {featureLabels[feature] || feature}
                    </text>
                ))}

                {/* Column labels */}
                {features.map((feature, i) => (
                    <text
                        key={`col-${feature}`}
                        x={padding + i * cellSize + cellSize / 2}
                        y={padding - 10}
                        textAnchor="end"
                        transform={`rotate(-45, ${padding + i * cellSize + cellSize / 2}, ${padding - 10})`}
                        style={{ fontSize: 10, fill: '#9ca3af', fontWeight: 600 }}
                    >
                        {featureLabels[feature] || feature}
                    </text>
                ))}

                {/* Legend */}
                <g transform={`translate(${size + 10}, ${padding + 20})`}>
                    <text x={0} y={-10} style={{ fontSize: 10, fill: '#6b7280', fontWeight: 600 }}>
                        Correlation
                    </text>
                    {[-1, -0.5, 0, 0.5, 1].map((val, i) => (
                        <g key={i}>
                            <rect
                                x={0}
                                y={i * 18}
                                width={25}
                                height={16}
                                fill={getColor(val)}
                                stroke="#000"
                                strokeWidth={0.5}
                            />
                            <text
                                x={30}
                                y={i * 18 + 11}
                                style={{ fontSize: 9, fill: '#9ca3af' }}
                            >
                                {val.toFixed(1)}
                            </text>
                        </g>
                    ))}
                </g>
            </svg>
        </div>
    );
};

export default CorrelationMatrix;


======= FILE: dashboard/src/components/HeatmapChart.jsx =======

import React from 'react';

const HeatmapChart = ({ data, xLabel, yLabel, title }) => {
    // data format: [{ x: value, y: value, value: intensity }]

    if (!data || data.length === 0) {
        return (
            <div className="flex items-center justify-center h-full text-gray-500 text-sm">
                No data available
            </div>
        );
    }

    // Get unique x and y values
    const xValues = [...new Set(data.map(d => d.x))].sort((a, b) => a - b);
    const yValues = [...new Set(data.map(d => d.y))].sort((a, b) => a - b);

    // Find min/max for color scaling
    const values = data.map(d => d.value);
    const minValue = Math.min(...values);
    const maxValue = Math.max(...values);

    // Color interpolation
    const getColor = (value) => {
        const normalized = (value - minValue) / (maxValue - minValue || 1);

        // Blue (low) -> Cyan -> Green -> Yellow -> Red (high)
        if (normalized < 0.25) {
            const t = normalized / 0.25;
            return `rgb(${Math.round(59 + (6 - 59) * t)}, ${Math.round(130 + (182 - 130) * t)}, ${Math.round(246 + (212 - 246) * t)})`;
        } else if (normalized < 0.5) {
            const t = (normalized - 0.25) / 0.25;
            return `rgb(${Math.round(6 + (16 - 6) * t)}, ${Math.round(182 + (185 - 182) * t)}, ${Math.round(212 + (129 - 212) * t)})`;
        } else if (normalized < 0.75) {
            const t = (normalized - 0.5) / 0.25;
            return `rgb(${Math.round(16 + (251 - 16) * t)}, ${Math.round(185 + (191 - 185) * t)}, ${Math.round(129 + (36 - 129) * t)})`;
        } else {
            const t = (normalized - 0.75) / 0.25;
            return `rgb(${Math.round(251 + (239 - 251) * t)}, ${Math.round(191 + (68 - 191) * t)}, ${Math.round(36 + (68 - 36) * t)})`;
        }
    };

    const cellWidth = 40;
    const cellHeight = 30;
    const paddingLeft = 60;
    const paddingTop = 20;
    const paddingBottom = 40;

    const width = paddingLeft + xValues.length * cellWidth + 80;
    const height = paddingTop + yValues.length * cellHeight + paddingBottom;

    return (
        <div className="overflow-auto">
            {title && <h3 className="text-sm font-bold text-gray-300 mb-2">{title}</h3>}
            <svg width={width} height={height}>
                {/* Heatmap cells */}
                {yValues.map((y, yi) => (
                    xValues.map((x, xi) => {
                        const cell = data.find(d => d.x === x && d.y === y);
                        const value = cell ? cell.value : 0;

                        return (
                            <g key={`${x}-${y}`}>
                                <rect
                                    x={paddingLeft + xi * cellWidth}
                                    y={paddingTop + yi * cellHeight}
                                    width={cellWidth - 1}
                                    height={cellHeight - 1}
                                    fill={cell ? getColor(value) : '#1f2937'}
                                    stroke="#000"
                                    strokeWidth={0.5}
                                />
                                <text
                                    x={paddingLeft + xi * cellWidth + cellWidth / 2}
                                    y={paddingTop + yi * cellHeight + cellHeight / 2 + 4}
                                    textAnchor="middle"
                                    style={{ fontSize: 10, fill: '#fff', fontWeight: 600 }}
                                >
                                    {cell ? value.toFixed(2) : '-'}
                                </text>
                            </g>
                        );
                    })
                ))}

                {/* X-axis labels */}
                {xValues.map((x, xi) => (
                    <text
                        key={`x-${x}`}
                        x={paddingLeft + xi * cellWidth + cellWidth / 2}
                        y={height - paddingBottom + 15}
                        textAnchor="middle"
                        style={{ fontSize: 10, fill: '#9ca3af' }}
                    >
                        {x.toFixed(1)}
                    </text>
                ))}

                {/* Y-axis labels */}
                {yValues.map((y, yi) => (
                    <text
                        key={`y-${y}`}
                        x={paddingLeft - 10}
                        y={paddingTop + yi * cellHeight + cellHeight / 2 + 4}
                        textAnchor="end"
                        style={{ fontSize: 10, fill: '#9ca3af' }}
                    >
                        {y.toFixed(1)}
                    </text>
                ))}

                {/* Axis labels */}
                <text
                    x={width / 2}
                    y={height - 5}
                    textAnchor="middle"
                    style={{ fontSize: 11, fill: '#6b7280', fontWeight: 600 }}
                >
                    {xLabel}
                </text>

                <text
                    x={15}
                    y={height / 2}
                    textAnchor="middle"
                    transform={`rotate(-90, 15, ${height / 2})`}
                    style={{ fontSize: 11, fill: '#6b7280', fontWeight: 600 }}
                >
                    {yLabel}
                </text>

                {/* Legend */}
                <g transform={`translate(${width - 60}, ${paddingTop})`}>
                    {[0, 0.25, 0.5, 0.75, 1].map((normalized, i) => {
                        const value = minValue + normalized * (maxValue - minValue);
                        return (
                            <g key={i}>
                                <rect
                                    x={0}
                                    y={i * 15}
                                    width={20}
                                    height={14}
                                    fill={getColor(value)}
                                    stroke="#000"
                                    strokeWidth={0.5}
                                />
                                <text
                                    x={25}
                                    y={i * 15 + 10}
                                    style={{ fontSize: 8, fill: '#9ca3af' }}
                                >
                                    {value.toFixed(2)}
                                </text>
                            </g>
                        );
                    })}
                </g>
            </svg>
        </div>
    );
};

export default HeatmapChart;


======= FILE: dashboard/src/components/ResourceMonitor.jsx =======

import React from 'react';
import { BarChart, Bar, XAxis, YAxis, Tooltip, ResponsiveContainer, Cell } from 'recharts';

const ResourceMonitor = ({ resources }) => {
    // Transform resources object to array
    const data = Object.keys(resources)
        .filter(k => k.startsWith('gpu_'))
        .map(k => ({
            name: k.toUpperCase().replace('_', ' '),
            load: resources[k].utilization * 100, // Convert to %
            tasks: resources[k].tasks_running,
            memory: resources[k].available_memory
        }));

    return (
        <div style={{ width: '100%', height: '300px' }}>
            {data.length === 0 ? (
                <div className="flex-center" style={{ height: '100%', color: '#64748b' }}>
                    Waiting for resource data...
                </div>
            ) : (
                <ResponsiveContainer width="100%" height="100%">
                    <BarChart data={data} layout="vertical" margin={{ left: 40 }}>
                        <XAxis type="number" domain={[0, 100]} hide />
                        <YAxis type="category" dataKey="name" width={60} tick={{ fill: '#94a3b8' }} />
                        <Tooltip
                            contentStyle={{ backgroundColor: '#1e293b', borderColor: '#334155', color: '#f8fafc' }}
                            cursor={{ fill: 'rgba(255,255,255,0.05)' }}
                        />
                        <Bar dataKey="load" radius={[0, 4, 4, 0]} barSize={30}>
                            {data.map((entry, index) => (
                                <Cell key={`cell-${index}`} fill={entry.load > 80 ? '#ef4444' : entry.load > 50 ? '#eab308' : '#3b82f6'} />
                            ))}
                        </Bar>
                    </BarChart>
                </ResponsiveContainer>
            )}
        </div>
    );
};

export default ResourceMonitor;


======= FILE: dashboard/src/components/Dashboard.jsx =======

import React, { useEffect, useState, useRef } from 'react';
import { Activity, Cpu, Server, Zap } from 'lucide-react';
import ResourceMonitor from './ResourceMonitor';
import LiveLog from './LiveLog';
import MetricsChart from './MetricsChart';

const Dashboard = () => {
    const [connected, setConnected] = useState(false);
    const [resources, setResources] = useState({});
    const [tasks, setTasks] = useState([]);
    const [metrics, setMetrics] = useState([]);
    const ws = useRef(null);

    useEffect(() => {
        const connect = () => {
            ws.current = new WebSocket('ws://localhost:8000/ws');

            ws.current.onopen = () => {
                console.log('Connected to WebSocket');
                setConnected(true);
            };

            ws.current.onclose = () => {
                console.log('Disconnected');
                setConnected(false);
                setTimeout(connect, 3000); // Reconnect
            };

            ws.current.onmessage = (event) => {
                const message = JSON.parse(event.data);
                handleMessage(message);
            };
        };

        connect();

        return () => {
            if (ws.current) ws.current.close();
        };
    }, []);

    const handleMessage = (msg) => {
        if (msg.type === 'resources') {
            setResources(msg.data);
        } else if (msg.type === 'decision') {
            // Update tasks
            setTasks(prev => [msg.data, ...prev].slice(0, 50));

            // Update resources
            if (msg.utilization) {
                setResources(msg.utilization);
            }

            // Update metrics
            if (msg.rl_metrics) {
                setMetrics(prev => [...prev, {
                    time: new Date().toLocaleTimeString(),
                    reward: msg.rl_metrics.reward,
                    epsilon: msg.rl_metrics.epsilon,
                    avg_time: msg.data.estimated_time
                }].slice(-50)); // Keep last 50 points
            }
        }
    };

    return (
        <div style={{ height: '100vh', display: 'flex', flexDirection: 'column' }}>
            {/* Header */}
            <header style={{
                padding: '1rem 2rem',
                borderBottom: '1px solid #334155',
                display: 'flex',
                justifyContent: 'space-between',
                alignItems: 'center',
                backgroundColor: '#0f172a'
            }}>
                <div className="flex-center" style={{ gap: '1rem' }}>
                    <Activity className="text-accent" size={28} />
                    <div>
                        <h1 style={{ fontSize: '1.25rem' }}>Hybrid ML Scheduler</h1>
                        <span style={{ fontSize: '0.875rem', color: '#94a3b8' }}>Live Dashboard</span>
                    </div>
                </div>
                <div className="flex-center" style={{ gap: '0.5rem' }}>
                    <div style={{
                        width: '8px',
                        height: '8px',
                        borderRadius: '50%',
                        backgroundColor: connected ? '#22c55e' : '#ef4444'
                    }} />
                    <span style={{ fontSize: '0.875rem', color: connected ? '#22c55e' : '#ef4444' }}>
                        {connected ? 'System Online' : 'Disconnected'}
                    </span>
                </div>
            </header>

            {/* Main Grid */}
            <main className="grid-layout">
                {/* Resource Monitor */}
                <div className="card col-span-8">
                    <div className="flex-between" style={{ marginBottom: '1rem' }}>
                        <h2 className="flex-center" style={{ gap: '0.5rem' }}>
                            <Server size={20} /> Cluster Resources
                        </h2>
                        <span style={{ color: '#94a3b8', fontSize: '0.9rem' }}>
                            Avg Load: {(resources.average_utilization * 100 || 0).toFixed(1)}%
                        </span>
                    </div>
                    <ResourceMonitor resources={resources} />
                </div>

                {/* Key Metrics */}
                <div className="card col-span-4">
                    <div className="flex-between" style={{ marginBottom: '1rem' }}>
                        <h2 className="flex-center" style={{ gap: '0.5rem' }}>
                            <Zap size={20} /> Performance
                        </h2>
                    </div>
                    <MetricsChart data={metrics} />
                </div>

                {/* Live Log */}
                <div className="card col-span-12" style={{ overflow: 'hidden', display: 'flex', flexDirection: 'column' }}>
                    <div className="flex-between" style={{ marginBottom: '1rem' }}>
                        <h2 className="flex-center" style={{ gap: '0.5rem' }}>
                            <Cpu size={20} /> Recent Scheduling Decisions
                        </h2>
                        <span style={{ color: '#94a3b8', fontSize: '0.9rem' }}>
                            {tasks.length} tasks processed
                        </span>
                    </div>
                    <LiveLog tasks={tasks} />
                </div>
            </main>
        </div>
    );
};

export default Dashboard;


======= FILE: dashboard/src/components/HistoricalPerformanceTable.jsx =======

import React, { useState, useEffect } from 'react';

const HistoricalPerformanceTable = () => {
    const [data, setData] = useState([]);
    const [loading, setLoading] = useState(true);

    const fetchData = async () => {
        try {
            const response = await fetch(`http://localhost:8000/api/history/comparative?limit=50&t=${Date.now()}`);
            const result = await response.json();
            setData(result);
            setLoading(false);
        } catch (error) {
            console.error('Error fetching history:', error);
            setLoading(false);
        }
    };

    useEffect(() => {
        fetchData();
        const interval = setInterval(fetchData, 2000); // Refresh every 2s
        return () => clearInterval(interval);
    }, []);

    if (loading) return <div className="text-cyan-400">Loading history...</div>;

    return (
        <div className="bg-gray-900/80 backdrop-blur border border-cyan-500/30 rounded-lg p-4 shadow-[0_0_15px_rgba(6,182,212,0.1)]">
            <h3 className="text-lg font-bold text-cyan-400 mb-4 flex items-center gap-2">
                <span className="w-2 h-2 rounded-full bg-cyan-400 animate-pulse"></span>
                Historical Performance Analysis
            </h3>

            <div className="overflow-x-auto">
                <table className="w-full text-sm text-left text-gray-300">
                    <thead className="text-xs text-cyan-300 uppercase bg-gray-800/50 border-b border-cyan-500/30">
                        <tr>
                            <th className="px-4 py-3">Task ID</th>
                            <th className="px-4 py-3">Size</th>
                            <th className="px-4 py-3">Intensity</th>
                            <th className="px-4 py-3 text-center">Round Robin</th>
                            <th className="px-4 py-3 text-center">Random</th>
                            <th className="px-4 py-3 text-center">Hybrid ML</th>
                            <th className="px-4 py-3 text-center">RL Agent</th>
                            <th className="px-4 py-3 text-center">Oracle (Best)</th>
                        </tr>
                    </thead>
                    <tbody>
                        {data.map((task) => {
                            const rr = task.results['round_robin']?.time || Infinity;
                            const rnd = task.results['random']?.time || Infinity;
                            const ml = task.results['hybrid_ml']?.time || Infinity;
                            const rl = task.results['rl_agent']?.time || Infinity;
                            const oracle = task.results['oracle']?.time || Infinity;

                            // Find winner (excluding Oracle)
                            const times = { 'Round Robin': rr, 'Random': rnd, 'Hybrid ML': ml, 'RL Agent': rl };
                            const winner = Object.entries(times).reduce((a, b) => a[1] < b[1] ? a : b)[0];

                            return (
                                <tr key={task.task_id} className="border-b border-gray-800 hover:bg-gray-800/30 transition-colors">
                                    <td className="px-4 py-2 font-mono text-cyan-200">#{task.task_id}</td>
                                    <td className="px-4 py-2">{task.size.toFixed(1)}</td>
                                    <td className="px-4 py-2">{task.intensity.toFixed(1)}</td>

                                    <td className={`px-4 py-2 text-center ${winner === 'Round Robin' ? 'text-green-400 font-bold' : ''}`}>
                                        {rr !== Infinity ? rr.toFixed(3) + 's' : '-'}
                                    </td>
                                    <td className={`px-4 py-2 text-center ${winner === 'Random' ? 'text-green-400 font-bold' : ''}`}>
                                        {rnd !== Infinity ? rnd.toFixed(3) + 's' : '-'}
                                    </td>
                                    <td className={`px-4 py-2 text-center ${winner === 'Hybrid ML' ? 'text-green-400 font-bold' : ''}`}>
                                        {ml !== Infinity ? ml.toFixed(3) + 's' : '-'}
                                    </td>
                                    <td className={`px-4 py-2 text-center ${winner === 'RL Agent' ? 'text-green-400 font-bold' : ''}`}>
                                        {rl !== Infinity ? rl.toFixed(3) + 's' : '-'}
                                    </td>
                                    <td className="px-4 py-2 text-center text-yellow-400 font-mono">
                                        {oracle !== Infinity ? oracle.toFixed(3) + 's' : '-'}
                                    </td>
                                </tr>
                            );
                        })}
                    </tbody>
                </table>
            </div>
        </div>
    );
};

export default HistoricalPerformanceTable;


======= FILE: dashboard/src/components/PercentileChart.jsx =======

import React from 'react';
import { LineChart, Line, XAxis, YAxis, CartesianGrid, Tooltip, Legend, ResponsiveContainer } from 'recharts';

const PercentileChart = ({ data, schedulerId, schedulerColor }) => {
    // data: [{ window: 0, p50: 0.5, p75: 0.6, p95: 0.8, p99: 0.9 }]

    if (!data || data.length === 0) {
        return (
            <div className="flex items-center justify-center h-full text-gray-500 text-sm">
                Insufficient data for percentiles
            </div>
        );
    }

    return (
        <ResponsiveContainer width="100%" height="100%">
            <LineChart data={data} margin={{ top: 5, right: 20, left: 0, bottom: 5 }}>
                <defs>
                    <linearGradient id={`p50-${schedulerId}`} x1="0" y1="0" x2="0" y2="1">
                        <stop offset="5%" stopColor={schedulerColor} stopOpacity={0.8} />
                        <stop offset="95%" stopColor={schedulerColor} stopOpacity={0.2} />
                    </linearGradient>
                </defs>
                <CartesianGrid strokeDasharray="3 3" stroke="#374151" vertical={false} />
                <XAxis
                    dataKey="window"
                    stroke="#6b7280"
                    tick={{ fontSize: 10 }}
                    label={{ value: 'Task Window', position: 'insideBottom', offset: -5, fill: '#9ca3af', fontSize: 10 }}
                />
                <YAxis
                    stroke="#6b7280"
                    tick={{ fontSize: 10 }}
                    label={{ value: 'Execution Time (s)', angle: -90, position: 'insideLeft', fill: '#9ca3af', fontSize: 10 }}
                />
                <Tooltip
                    contentStyle={{
                        backgroundColor: '#18181b',
                        border: '1px solid #3f3f46',
                        borderRadius: '8px',
                        boxShadow: '0 4px 6px rgba(0,0,0,0.3)'
                    }}
                    formatter={(value) => value.toFixed(4) + 's'}
                />
                <Legend
                    wrapperStyle={{ fontSize: '11px', paddingTop: '10px' }}
                    iconType="line"
                />

                <Line
                    type="monotone"
                    dataKey="p50"
                    stroke={schedulerColor}
                    strokeWidth={2}
                    dot={false}
                    name="P50 (Median)"
                />
                <Line
                    type="monotone"
                    dataKey="p75"
                    stroke={schedulerColor}
                    strokeWidth={2}
                    dot={false}
                    strokeDasharray="5 5"
                    name="P75"
                    opacity={0.8}
                />
                <Line
                    type="monotone"
                    dataKey="p95"
                    stroke="#f59e0b"
                    strokeWidth={2}
                    dot={false}
                    strokeDasharray="3 3"
                    name="P95"
                />
                <Line
                    type="monotone"
                    dataKey="p99"
                    stroke="#ef4444"
                    strokeWidth={2.5}
                    dot={false}
                    name="P99"
                />
            </LineChart>
        </ResponsiveContainer>
    );
};

export default PercentileChart;


======= FILE: dashboard/src/components/StatsTable.jsx =======

import React, { useState } from 'react';
import { TrendingUp, TrendingDown, Minus, ArrowUpDown } from 'lucide-react';

const StatsTable = ({ data, schedulers }) => {
    // data: { scheduler_id: { mean, median, std, min, max, count } }
    const [sortBy, setSortBy] = useState('mean');
    const [sortAsc, setSortAsc] = useState(true);

    if (!data || Object.keys(data).length === 0) {
        return (
            <div className="flex items-center justify-center h-32 text-gray-500 text-sm">
                No statistics available yet
            </div>
        );
    }

    const metrics = ['mean', 'median', 'std', 'min', 'max', 'count'];
    const metricLabels = {
        mean: 'Mean',
        median: 'Median',
        std: 'Std Dev',
        min: 'Min',
        max: 'Max',
        count: 'Tasks'
    };

    // Sort schedulers
    const sortedSchedulers = Object.keys(data).sort((a, b) => {
        const valA = data[a][sortBy] || 0;
        const valB = data[b][sortBy] || 0;
        return sortAsc ? valA - valB : valB - valA;
    });

    const toggleSort = (metric) => {
        if (sortBy === metric) {
            setSortAsc(!sortAsc);
        } else {
            setSortBy(metric);
            setSortAsc(true);
        }
    };

    // Find best/worst for highlighting
    const getBest = (metric) => {
        if (metric === 'count') return null;
        return Math.min(...Object.values(data).map(d => d[metric] || Infinity));
    };

    const getWorst = (metric) => {
        if (metric === 'count') return null;
        return Math.max(...Object.values(data).map(d => d[metric] || -Infinity));
    };

    return (
        <div className="overflow-x-auto">
            <table className="w-full text-sm">
                <thead>
                    <tr className="border-b border-white/10">
                        <th className="text-left p-2 text-gray-400 font-semibold text-xs">Scheduler</th>
                        {metrics.map(metric => (
                            <th
                                key={metric}
                                className="text-right p-2 text-gray-400 font-semibold text-xs cursor-pointer hover:text-white transition-colors"
                                onClick={() => toggleSort(metric)}
                            >
                                <div className="flex items-center justify-end gap-1">
                                    {metricLabels[metric]}
                                    <ArrowUpDown size={12} className={sortBy === metric ? 'text-cyan-400' : 'text-gray-600'} />
                                </div>
                            </th>
                        ))}
                    </tr>
                </thead>
                <tbody>
                    {sortedSchedulers.map((schedulerId, idx) => {
                        const stats = data[schedulerId];
                        const scheduler = schedulers?.find(s => s.id === schedulerId);

                        return (
                            <tr
                                key={schedulerId}
                                className={`border-b border-white/5 hover:bg-white/5 transition-colors ${idx === 0 ? 'bg-green-500/10' : idx === sortedSchedulers.length - 1 ? 'bg-red-500/10' : ''
                                    }`}
                            >
                                <td className="p-2 font-semibold flex items-center gap-2">
                                    <div
                                        className="w-2 h-2 rounded-full"
                                        style={{ backgroundColor: scheduler?.color || '#6b7280' }}
                                    />
                                    <span style={{ color: scheduler?.color || '#fff' }}>{scheduler?.name || schedulerId}</span>
                                </td>
                                {metrics.map(metric => {
                                    const value = stats[metric];
                                    const best = getBest(metric);
                                    const worst = getWorst(metric);
                                    const isBest = value === best && best !== null;
                                    const isWorst = value === worst && worst !== null;

                                    return (
                                        <td
                                            key={metric}
                                            className={`p-2 text-right font-mono ${isBest ? 'text-green-400 font-bold' :
                                                    isWorst ? 'text-red-400' :
                                                        'text-gray-300'
                                                }`}
                                        >
                                            {metric === 'count' ? value : value?.toFixed(4)}
                                            {isBest && <TrendingDown size={12} className="inline ml-1 text-green-400" />}
                                            {isWorst && <TrendingUp size={12} className="inline ml-1 text-red-400" />}
                                        </td>
                                    );
                                })}
                            </tr>
                        );
                    })}
                </tbody>
            </table>
        </div>
    );
};

export default StatsTable;


======= FILE: dashboard/src/components/MetricsChart.jsx =======

import React from 'react';
import { LineChart, Line, XAxis, YAxis, Tooltip, ResponsiveContainer, CartesianGrid } from 'recharts';

const MetricsChart = ({ data }) => {
    return (
        <div style={{ width: '100%', height: '200px' }}>
            <ResponsiveContainer width="100%" height="100%">
                <LineChart data={data}>
                    <CartesianGrid strokeDasharray="3 3" stroke="#334155" vertical={false} />
                    <XAxis dataKey="time" hide />
                    <YAxis yAxisId="left" stroke="#38bdf8" fontSize={12} />
                    <YAxis yAxisId="right" orientation="right" stroke="#22c55e" fontSize={12} />
                    <Tooltip
                        contentStyle={{ backgroundColor: '#1e293b', borderColor: '#334155', color: '#f8fafc' }}
                    />
                    <Line yAxisId="left" type="monotone" dataKey="avg_time" stroke="#38bdf8" dot={false} strokeWidth={2} />
                    <Line yAxisId="right" type="monotone" dataKey="reward" stroke="#22c55e" dot={false} strokeWidth={2} />
                </LineChart>
            </ResponsiveContainer>
        </div>
    );
};

export default MetricsChart;


======= FILE: dashboard/src/components/LiveLog.jsx =======

import React from 'react';

const LiveLog = ({ tasks }) => {
    return (
        <div style={{ flex: 1, overflowY: 'auto', minHeight: '0' }}>
            <table style={{ width: '100%', borderCollapse: 'collapse', fontSize: '0.875rem' }}>
                <thead style={{ position: 'sticky', top: 0, backgroundColor: '#1e293b', zIndex: 10 }}>
                    <tr style={{ textAlign: 'left', color: '#94a3b8' }}>
                        <th style={{ padding: '0.75rem' }}>Task ID</th>
                        <th style={{ padding: '0.75rem' }}>Placement</th>
                        <th style={{ padding: '0.75rem' }}>Est. Time</th>
                        <th style={{ padding: '0.75rem' }}>Reward</th>
                        <th style={{ padding: '0.75rem' }}>Status</th>
                    </tr>
                </thead>
                <tbody>
                    {tasks.map((task, i) => (
                        <tr key={i} style={{ borderBottom: '1px solid #334155' }}>
                            <td style={{ padding: '0.75rem', fontFamily: 'monospace' }}>#{task.task_id}</td>
                            <td style={{ padding: '0.75rem' }}>
                                {task.gpu_fraction > 0.5 ? (
                                    <span style={{ color: '#38bdf8' }}>GPU {task.gpu_id}</span>
                                ) : (
                                    <span style={{ color: '#a8a29e' }}>CPU</span>
                                )}
                            </td>
                            <td style={{ padding: '0.75rem' }}>{task.estimated_time.toFixed(2)}s</td>
                            <td style={{ padding: '0.75rem', color: task.reward > -1 ? '#22c55e' : '#ef4444' }}>
                                {task.reward ? task.reward.toFixed(2) : '-'}
                            </td>
                            <td style={{ padding: '0.75rem' }}>
                                <span style={{
                                    padding: '0.25rem 0.5rem',
                                    borderRadius: '9999px',
                                    backgroundColor: 'rgba(34, 197, 94, 0.1)',
                                    color: '#22c55e',
                                    fontSize: '0.75rem'
                                }}>
                                    Scheduled
                                </span>
                            </td>
                        </tr>
                    ))}
                    {tasks.length === 0 && (
                        <tr>
                            <td colSpan="5" style={{ padding: '2rem', textAlign: 'center', color: '#64748b' }}>
                                Waiting for tasks...
                            </td>
                        </tr>
                    )}
                </tbody>
            </table>
        </div>
    );
};

export default LiveLog;


======= FILE: dashboard/src/components/WinLossMatrix.jsx =======

import React from 'react';

const WinLossMatrix = ({ matrix, schedulers }) => {
    // matrix: { scheduler_a: { scheduler_b: 0.65 (win rate) } }

    if (!matrix || Object.keys(matrix).length === 0) {
        return (
            <div className="flex items-center justify-center h-32 text-gray-500 text-sm">
                Insufficient data for comparison
            </div>
        );
    }

    const schedulerIds = Object.keys(matrix);
    const cellSize = 60;
    const padding = 80;
    const size = schedulerIds.length * cellSize + padding;

    const getColor = (winRate) => {
        if (winRate === null || winRate === undefined) return '#1f2937';

        // Red (0%) -> Yellow (50%) -> Green (100%)
        if (winRate < 0.5) {
            const t = winRate / 0.5;
            // Red to Yellow
            return `rgb(${Math.round(239 + (251 - 239) * t)}, ${Math.round(68 + (191 - 68) * t)}, 68)`;
        } else {
            const t = (winRate - 0.5) / 0.5;
            // Yellow to Green
            return `rgb(${Math.round(251 - (251 - 16) * t)}, ${Math.round(191 - (191 - 185) * t)}, ${Math.round(68 + (129 - 68) * t)})`;
        }
    };

    return (
        <div className="overflow-auto">
            <svg width={size} height={size}>
                {/* Grid */}
                {schedulerIds.map((rowId, ri) => (
                    schedulerIds.map((colId, ci) => {
                        const winRate = rowId === colId ? null : matrix[rowId]?.[colId];
                        const scheduler = schedulers?.find(s => s.id === rowId);

                        return (
                            <g key={`${rowId}-${colId}`}>
                                <rect
                                    x={padding + ci * cellSize}
                                    y={padding + ri * cellSize}
                                    width={cellSize - 1}
                                    height={cellSize - 1}
                                    fill={rowId === colId ? '#27272a' : getColor(winRate)}
                                    stroke="#000"
                                    strokeWidth={0.5}
                                />
                                {rowId !== colId && winRate !== null && winRate !== undefined && (
                                    <text
                                        x={padding + ci * cellSize + cellSize / 2}
                                        y={padding + ri * cellSize + cellSize / 2 + 5}
                                        textAnchor="middle"
                                        style={{ fontSize: 12, fill: '#fff', fontWeight: 700 }}
                                    >
                                        {(winRate * 100).toFixed(0)}%
                                    </text>
                                )}
                                {rowId === colId && (
                                    <text
                                        x={padding + ci * cellSize + cellSize / 2}
                                        y={padding + ri * cellSize + cellSize / 2 + 3}
                                        textAnchor="middle"
                                        style={{ fontSize: 16, fill: '#4b5563' }}
                                    >
                                        â€”
                                    </text>
                                )}
                            </g>
                        );
                    })
                ))}

                {/* Row labels */}
                {schedulerIds.map((id, i) => {
                    const scheduler = schedulers?.find(s => s.id === id);
                    return (
                        <text
                            key={`row-${id}`}
                            x={padding - 10}
                            y={padding + i * cellSize + cellSize / 2 + 4}
                            textAnchor="end"
                            style={{ fontSize: 11, fill: scheduler?.color || '#9ca3af', fontWeight: 600 }}
                        >
                            {scheduler?.name || id}
                        </text>
                    );
                })}

                {/* Column labels */}
                {schedulerIds.map((id, i) => {
                    const scheduler = schedulers?.find(s => s.id === id);
                    return (
                        <text
                            key={`col-${id}`}
                            x={padding + i * cellSize + cellSize / 2}
                            y={padding - 10}
                            textAnchor="middle"
                            style={{ fontSize: 11, fill: scheduler?.color || '#9ca3af', fontWeight: 600 }}
                        >
                            {scheduler?.name || id}
                        </text>
                    );
                })}

                {/* Labels */}
                <text
                    x={padding / 2}
                    y={size / 2}
                    textAnchor="middle"
                    transform={`rotate(-90, ${padding / 2}, ${size / 2})`}
                    style={{ fontSize: 12, fill: '#6b7280', fontWeight: 700 }}
                >
                    Wins Against â†’
                </text>

                <text
                    x={size / 2}
                    y={padding / 2}
                    textAnchor="middle"
                    style={{ fontSize: 12, fill: '#6b7280', fontWeight: 700 }}
                >
                    â† Opponent
                </text>
            </svg>
        </div>
    );
};

export default WinLossMatrix;


======= FILE: dashboard/src/components/GaugeChart.jsx =======

import React from 'react';

const GaugeChart = ({ value, max = 100, label, color = '#06b6d4', size = 120 }) => {
    const percentage = Math.min(100, (value / max) * 100);
    const angle = (percentage / 100) * 180; // 0-180 degrees for semi-circle

    // Calculate needle position
    const needleLength = size * 0.35;
    const centerX = size / 2;
    const centerY = size / 2;
    const angleRad = ((angle - 90) * Math.PI) / 180;
    const needleX = centerX + needleLength * Math.cos(angleRad);
    const needleY = centerY + needleLength * Math.sin(angleRad);

    return (
        <div className="flex flex-col items-center">
            <svg width={size} height={size * 0.8} viewBox={`0 0 ${size} ${size * 0.8}`}>
                {/* Background arc */}
                <path
                    d={`M ${size * 0.1} ${size / 2} A ${size * 0.4} ${size * 0.4} 0 0 1 ${size * 0.9} ${size / 2}`}
                    fill="none"
                    stroke="#374151"
                    strokeWidth={size * 0.08}
                    strokeLinecap="round"
                />

                {/* Progress arc */}
                <path
                    d={`M ${size * 0.1} ${size / 2} A ${size * 0.4} ${size * 0.4} 0 0 1 ${size * 0.9} ${size / 2}`}
                    fill="none"
                    stroke={color}
                    strokeWidth={size * 0.08}
                    strokeLinecap="round"
                    strokeDasharray={`${(percentage / 100) * Math.PI * size * 0.4} ${Math.PI * size * 0.4}`}
                    style={{ filter: `drop-shadow(0 0 ${size * 0.05}px ${color})` }}
                />

                {/* Center circle */}
                <circle cx={centerX} cy={centerY} r={size * 0.04} fill="#18181b" stroke={color} strokeWidth={2} />

                {/* Needle */}
                <line
                    x1={centerX}
                    y1={centerY}
                    x2={needleX}
                    y2={needleY}
                    stroke={color}
                    strokeWidth={size * 0.015}
                    strokeLinecap="round"
                    style={{ filter: `drop-shadow(0 0 ${size * 0.02}px ${color})` }}
                />

                {/* Value text */}
                <text
                    x={centerX}
                    y={centerY + size * 0.15}
                    textAnchor="middle"
                    className="font-bold"
                    style={{ fontSize: size * 0.15, fill: '#fff' }}
                >
                    {value.toFixed(1)}
                </text>

                {/* Min/Max labels */}
                <text x={size * 0.05} y={size * 0.55} style={{ fontSize: size * 0.08, fill: '#6b7280' }}>0</text>
                <text x={size * 0.88} y={size * 0.55} textAnchor="end" style={{ fontSize: size * 0.08, fill: '#6b7280' }}>{max}</text>
            </svg>

            {label && (
                <div className="text-xs text-gray-400 mt-1 font-semibold text-center">{label}</div>
            )}
        </div>
    );
};

export default GaugeChart;


======= FILE: scripts/run_heavy_simulation.py =======


import sys
import os
from pathlib import Path

# Add project root to path
sys.path.append(str(Path(__file__).parent.parent))

from loguru import logger
import yaml
import pandas as pd
import asyncio
import numpy as np

from src.pipeline import (
    run_profiling_phase,
    run_data_generation_phase,
    run_offline_training_phase,
    run_online_scheduling_phase,
    run_rl_training_phase
)
from src.simulator import VirtualMultiGPU
from src.workload_generator import WorkloadGenerator
from src.visualization import plot_comparison, plot_cost_analysis, plot_latency_distribution
from src.reporting import generate_enhanced_report
from backend.services.simulation_data_service import SimulationDataService
from backend.core.database import init_db, close_db

def load_config(config_path: str) -> dict:
    with open(config_path) as f:
        return yaml.safe_load(f)

async def main():
    logger.add("logs/heavy_simulation.log", rotation="500 MB")
    
    logger.info("="*80)
    logger.info("HEAVY WORKLOAD SIMULATION (10,000 TASKS)")
    logger.info("="*80)
    
    # 0. Initialize Database
    logger.info("Initializing Database...")
    await init_db()
    
    config_path = "config_heavy.yaml"
    if not Path(config_path).exists():
        logger.error(f"{config_path} not found!")
        return
        
    config = load_config(config_path)
    
    # Phase 1: Profiling (Standard)
    run_profiling_phase(config)
    
    # Phase 2: Data Gen (Standard)
    wg, _ = run_data_generation_phase(config)
    
    # Phase 3: Offline Training (Standard)
    trainer, _ = run_offline_training_phase(config, wg)
    
    # Phase 4.5: RL Training (Standard)
    rl_scheduler = run_rl_training_phase(config, trainer)
    
    # Phase 5: Heavy Evaluation & DB Population
    logger.info("\n" + "="*80)
    logger.info("PHASE 5: HEAVY WORKLOAD EVALUATION & DB POPULATION")
    logger.info("="*80)
    
    num_eval_tasks = config['workload_generation']['evaluation_tasks']
    logger.info(f"Generating {num_eval_tasks} evaluation tasks...")
    
    test_wg = WorkloadGenerator(seed=999)
    test_tasks = test_wg.generate_workload(
        num_tasks=num_eval_tasks,
        task_size_range=tuple(config['workload_generation']['task_size_range']),
        compute_intensity_range=tuple(config['workload_generation']['compute_intensity_range']),
        memory_range=tuple(config['workload_generation']['memory_range'])
    )
    
    # Save training data to DB (Simulating that we saw this 'historical' data)
    # We'll batch save the task definitions first
    # Actually, SimulationDataService.save_scheduler_results handles task creation if missing.
    
    simulator = VirtualMultiGPU(num_gpus=config['hardware']['num_virtual_gpus'])
    
    baselines = {}
    latencies = {} # For CDF/Boxplot
    
    # 1. Run Baselines
    strategies = config['evaluation']['baseline_strategies']
    for strategy in strategies:
        logger.info(f"Running baseline: {strategy}...")
        results = simulator.simulate_workload(test_tasks, strategy)
        
        # Calculate Metrics
        times = [r['actual_time'] for r in results]
        costs = [r['cost'] for r in results]
        total_time = sum(times)
        total_cost = sum(costs)
        
        baselines[strategy] = {
            'makespan': total_time,
            'avg_time': np.mean(times),
            'p95_time': np.percentile(times, 95),
            'p99_time': np.percentile(times, 99),
            'throughput': len(test_tasks) / total_time * config['hardware']['num_virtual_gpus'], # approx
            'total_cost': total_cost,
            'cost_efficiency': len(test_tasks) / (total_cost + 1e-6)
        }
        latencies[strategy] = times
        
        # Save to DB (Background or Batch)
        # To avoid slowing down too much, we'll save in chunks
        logger.info(f"Saving {len(results)} results for {strategy} to DB...")
        batch_data = []
        for i, task in enumerate(test_tasks):
            batch_data.append((task, {strategy: results[i]}))
            
            if len(batch_data) >= 500:
                await SimulationDataService.save_scheduler_results_batch(batch_data)
                batch_data = []
        
        if batch_data:
            await SimulationDataService.save_scheduler_results_batch(batch_data)

    # 2. Run Hybrid ML
    logger.info("Running Hybrid ML Scheduler...")
    our_results = []
    our_times = []
    our_costs = []
    
    for task in test_tasks:
         # Predict
        features = pd.DataFrame([{
            'size': task.size,
            'compute_intensity': task.compute_intensity,
            'memory_required': task.memory_required,
            'memory_per_size': task.memory_required / (task.size + 1),
            'compute_to_memory': task.compute_intensity / (task.memory_required + 1),
        }])
        gpu_fraction = trainer.model.predict(features)[0]
        gpu_fraction = max(0.0, min(1.0, gpu_fraction))
        
        # Simulate
        result = simulator.simulate_task_execution(task, gpu_fraction)
        our_results.append(result)
        our_times.append(result['actual_time'])
        our_costs.append(result['cost'])

    baselines['hybrid_ml'] = {
        'makespan': sum(our_times),
        'avg_time': np.mean(our_times),
        'p95_time': np.percentile(our_times, 95),
        'p99_time': np.percentile(our_times, 99),
        'throughput': len(test_tasks) / sum(our_times) * config['hardware']['num_virtual_gpus'],
        'total_cost': sum(our_costs),
        'cost_efficiency': len(test_tasks) / (sum(our_costs) + 1e-6)
    }
    latencies['hybrid_ml'] = our_times
    
    # Save Hybrid ML Results to DB
    logger.info("Saving Hybrid ML results to DB...")
    batch_data = []
    for i, task in enumerate(test_tasks):
        batch_data.append((task, {'hybrid_ml': our_results[i]}))
        if len(batch_data) >= 500:
            await SimulationDataService.save_scheduler_results_batch(batch_data)
            batch_data = []
    if batch_data:
        await SimulationDataService.save_scheduler_results_batch(batch_data)

    # 3. Run RL Agent
    logger.info("Running RL Agent (Evaluated)...")
    rl_scheduler.epsilon = 0.0 # Greedy evaluation
    rl_results = []
    rl_times = []
    rl_costs = []
    
    for task in test_tasks:
        rl_scheduler.reset_state()
        # Note: In real sim we'd loop, but here getting decision is enough for 'simulated' execution
        decision = rl_scheduler.schedule_task(task)
        
        # The RL scheduler returns a decision, we need to run it in the simulator to get actuals
        # The decision dict contains 'action' (0=CPU, 1=GPU, etc if discrete, or fraction)
        # RL uses discrete actions usually mapped to gpu_fraction
        
        # Map action to fraction
        # Action 0: CPU (0.0), Action 1: GPU (1.0)
        action = decision['action']
        gpu_fraction = decision['gpu_fraction'] 
        
        result = simulator.simulate_task_execution(task, gpu_fraction)
        rl_results.append(result)
        rl_times.append(result['actual_time'])
        rl_costs.append(result['cost'])
        
    baselines['rl_agent'] = {
        'makespan': sum(rl_times),
        'avg_time': np.mean(rl_times),
        'p95_time': np.percentile(rl_times, 95),
        'p99_time': np.percentile(rl_times, 99),
        'throughput': len(test_tasks) / sum(rl_times) * config['hardware']['num_virtual_gpus'],
        'total_cost': sum(rl_costs),
        'cost_efficiency': len(test_tasks) / (sum(rl_costs) + 1e-6)
    }
    latencies['rl_agent'] = rl_times

    # Save RL Results to DB
    logger.info("Saving RL Agent results to DB...")
    batch_data = []
    for i, task in enumerate(test_tasks):
        batch_data.append((task, {'rl_agent': rl_results[i]}))
        if len(batch_data) >= 500:
            await SimulationDataService.save_scheduler_results_batch(batch_data)
            batch_data = []
    if batch_data:
        await SimulationDataService.save_scheduler_results_batch(batch_data)

    # Generate Plots
    logger.info("Generating plots...")
    plot_comparison(baselines, output_dir=config['output']['plots_dir'])
    plot_cost_analysis(baselines, output_dir=config['output']['plots_dir'])
    plot_latency_distribution(latencies, output_dir=config['output']['plots_dir'])
    
    # Generate Report
    logger.info("Generating PDF Report...")
    report_path = "data/results/Heavy_Workload_Report.pdf"
    generate_enhanced_report(report_path, baselines, config['output']['plots_dir'])
    
    logger.info(f"SUCCESS! Report generated at {report_path}")
    
    await close_db()

if __name__ == "__main__":
    asyncio.run(main())


======= FILE: scripts/generate_long_term_report.py =======


import asyncio
import os
import sys
from pathlib import Path

# Add project root to path
sys.path.append(str(Path(__file__).parent.parent))

import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine, text
from loguru import logger

from backend.core.config import settings
from src.visualization import (
    plot_comparison, 
    plot_cost_analysis, 
    plot_latency_distribution,
    plot_workload_characteristics
)
from src.reporting import generate_enhanced_report

async def main():
    logger.info("Starting Report Generation...")
    
    # 1. Fetch Data
    # We use the sync URL for pandas
    db_url = settings.database_url
    
    # Check for psycopg2/asyncpg
    try:
        # Try connecting with SQLAlchemy (requires psycopg2-binary usually)
        engine = create_engine(db_url)
        with engine.connect() as conn:
            logger.info("Connected to database via SQLAlchemy.")
            
            # Fetch Scheduler Results
            query = "SELECT * FROM scheduler_results"
            df_results = pd.read_sql(query, conn)
            logger.info(f"Fetched {len(df_results)} scheduler results.")
            
            # Fetch Tasks for workload characteristics
            query_tasks = "SELECT * FROM tasks"
            df_tasks = pd.read_sql(query_tasks, conn)
            logger.info(f"Fetched {len(df_tasks)} tasks.")
            
    except Exception as e:
        logger.warning(f"SQLAlchemy connection failed: {e}. Attempting fallback to asyncpg...")
        try:
            import asyncpg
            # Clean url for asyncpg (remove +asyncpg if present, though settings.async_database_url handles it differently)
            # settings.async_database_url is postgresql+asyncpg://...
            # asyncpg.connect expects postgresql://...
            url = settings.async_database_url.replace("postgresql+asyncpg://", "postgresql://")
            
            conn = await asyncpg.connect(url)
            
            # Fetch Results
            rows = await conn.fetch("SELECT * FROM scheduler_results")
            df_results = pd.DataFrame([dict(r) for r in rows])
            
            # Fetch Tasks
            rows_tasks = await conn.fetch("SELECT * FROM tasks")
            df_tasks = pd.DataFrame([dict(r) for r in rows_tasks])
            
            await conn.close()
            logger.info(f"Fetched {len(df_results)} results via asyncpg.")
        except Exception as e2:
            logger.critical(f"Could not access database via asyncpg either: {e2}")
            # Try to degrade gracefully if csv exists?
            # Creating empty DF to prevent crash
            df_results = pd.DataFrame()
            df_tasks = pd.DataFrame()

    if df_results.empty:
        logger.warning("No results found in database! (DataFrame is empty)")
        # If empty, we can't do comparative analysis.
        pass

    # 2. Process Data for Visualization/Reporting
    plots_dir = Path("data/results/report_plots")
    plots_dir.mkdir(parents=True, exist_ok=True)
    
    # Prepare baselines dict
    baselines = {}
    latencies_dict = {}
    
    if not df_results.empty:
        # Ensure numeric types
        cols_to_numeric = ['actual_time', 'execution_cost']
        for col in cols_to_numeric:
            if col in df_results.columns:
                df_results[col] = pd.to_numeric(df_results[col], errors='coerce')

        # Group by scheduler
        for name, group in df_results.groupby("scheduler_name"):
            total_time = group["actual_time"].sum()
            avg_time = group["actual_time"].mean()
            cost = group["execution_cost"].sum() if "execution_cost" in group.columns else 0
            
            baselines[name] = {
                "makespan": total_time,
                "avg_time": avg_time,
                "p95_time": group["actual_time"].quantile(0.95),
                "p99_time": group["actual_time"].quantile(0.99),
                "throughput": len(group) / (total_time + 1e-6),
                "total_cost": cost,
                "cost_efficiency": len(group) / (cost + 1e-6)
            }
            latencies_dict[name] = group["actual_time"].dropna().tolist()
            
    else:
        logger.info("Using Dummy data for report proof-of-concept if DB is empty.")
        # Create minimal dummy data so report doesn't crash
        dummy_schedulers = ['round_robin', 'hybrid_ml', 'rl_agent']
        import numpy as np
        for s in dummy_schedulers:
            data = np.random.exponential(1.0, 100)
            baselines[s] = {
                "makespan": data.sum(),
                "avg_time": data.mean(),
                "p95_time": np.percentile(data, 95),
                "p99_time": np.percentile(data, 99),
                "throughput": 100 / data.sum(),
                "total_cost": data.sum() * 0.1,
                "cost_efficiency": 10.0
            }
            latencies_dict[s] = data.tolist()

    # 3. Generate Plots
    logger.info("Generating plots...")
    if baselines:
        plot_comparison(baselines, output_dir=str(plots_dir))
        plot_cost_analysis(baselines, output_dir=str(plots_dir))
    
    if latencies_dict:
        plot_latency_distribution(latencies_dict, output_dir=str(plots_dir))
        
    if not df_tasks.empty:
        plot_workload_characteristics(df_tasks, output_dir=str(plots_dir))

    # 4. Generate PDF
    output_pdf = "PROJECT_LONG_TERM_REPORT.pdf"
    logger.info(f"Generating PDF: {output_pdf}")
    generate_enhanced_report(output_pdf, baselines, plots_dir)
    logger.info("Done!")

if __name__ == "__main__":
    asyncio.run(main())


======= FILE: scripts/verify_rl.py =======

import asyncio
import numpy as np
import pandas as pd
from loguru import logger
import sys
import os

# Add project root to path
sys.path.append(os.getcwd())

from src.workload_generator import WorkloadGenerator
from src.dqn_scheduler import DQNScheduler
from src.simulator import VirtualMultiGPU
from src.profiler import HardwareProfiler

def calculate_reward_metrics(result):
    # Match simulation_engine logic
    time = result['actual_time']
    gpu_frac = result['gpu_fraction']
    power = (gpu_frac * 50.0) + ((1.0 - gpu_frac) * 30.0)
    energy_joules = power * time
    return {'time': time, 'energy': energy_joules}

async def run_verification():
    logger.info("Starting RL Verification Simulation...")
    
    # 1. Setup
    wg = WorkloadGenerator(seed=42)
    # Increase epsilon decay to ensure it doesn't stop exploring too fast in this short run
    # default was 0.9995. Let's make it 0.99 for faster convergence in this short test, 
    # OR slower (0.9999) if we want to be safe. 
    # Actually, for 1000 tasks, 0.999^1000 = 0.36. 0.99^1000 is tiny.
    # Let's use 0.995 to decays reasonably fast.
    scheduler = DQNScheduler(
        num_gpus=4, 
        epsilon_start=1.0, 
        epsilon_end=0.05, 
        epsilon_decay=0.995,
        batch_size=32 # Smaller batch for faster updates
    )
    simulator = VirtualMultiGPU(num_gpus=4)
    
    tasks = wg.generate_workload(num_tasks=1000)
    
    regrets = []
    rewards = []
    
    print(f"{'Task':<6} | {'Oracle':<8} | {'RL Agent':<8} | {'Regret':<8} | {'Epsilon':<8} | {'Action':<6}")
    print("-" * 65)

    window_size = 50
    
    for i, task in enumerate(tasks):
        # 2. Oracle (Best Possible)
        best_time = float('inf')
        for frac in np.linspace(0, 1, 11):
            res = simulator.simulate_task_execution(task, frac)
            if res['actual_time'] < best_time:
                best_time = res['actual_time']
                
        # 3. RL Agent
        action_dict = scheduler.get_action(task)
        rl_res = simulator.simulate_task_execution(task, action_dict['gpu_fraction'])
        
        # 4. Feedback
        metrics = calculate_reward_metrics(rl_res)
        scheduler.observe(task, action_dict['action'], metrics)
        
        # 5. Track stats
        regret = rl_res['actual_time'] - best_time
        regrets.append(regret)
        
        # Print every 50
        if (i + 1) % 50 == 0:
            avg_regret = np.mean(regrets[-50:])
            print(f"{i+1:<6} | {best_time:<8.4f} | {rl_res['actual_time']:<8.4f} | {avg_regret:<8.4f} | {scheduler.epsilon:<8.4f} | {action_dict['action']:<6}")
            
    avg_total_regret = np.mean(regrets)
    logger.info(f"Final Average Regret: {avg_total_regret:.4f}")
    
    # Check if it learned (compare first 100 vs last 100)
    first_100 = np.mean(regrets[:100])
    last_100 = np.mean(regrets[-100:])
    logger.info(f"Regret Improvement: {first_100:.4f} -> {last_100:.4f}")
    
    if last_100 < first_100:
        logger.success("âœ… RL Agent is learning! Regret decreased.")
    else:
        logger.warning("âš ï¸ RL Agent did not improve. Tuning might be needed.")

if __name__ == "__main__":
    asyncio.run(run_verification())


======= FILE: scripts/export_for_notebooklm.py =======

import os

def export_for_notebooklm():
    """
    Crawls the entire repository and concatenates all actual source code files
    into a single text file named notebookllm_source.txt.
    """
    # Exclude directories that are not source code or contain heavy binaries/dependencies
    IGNORED_DIRS = {
        '.git', '__pycache__', 'venv', '.venv', 'node_modules', 
        '.idea', '.vscode', 'archive', 'data', 'logs', 'dist', 'build', '.pytest_cache'
    }
    
    # Extensions to include
    ALLOWED_EXTS = {
        '.py', '.js', '.jsx', '.ts', '.tsx', '.md', '.json', '.yml', '.yaml', 
        '.sh', '.ini', '.txt', '.css', '.html'
    }
    
    # Base directory is the root of the project (parent of 'scripts')
    base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    output_file = os.path.join(base_dir, 'notebookllm_source.txt')
    
    print(f"Starting directory traversal from: {base_dir}")
    print(f"Outputting to: {output_file}")
    
    with open(output_file, 'w', encoding='utf-8') as outfile:
        for root, dirs, files in os.walk(base_dir):
            # Modify dirs in-place to skip ignored directories
            dirs[:] = [d for d in dirs if d not in IGNORED_DIRS]
            
            for file in files:
                # Check extension
                _, ext = os.path.splitext(file)
                if ext.lower() not in ALLOWED_EXTS:
                    continue
                
                # Exclude output file itself or large PDF documentation
                if file == 'notebookllm_source.txt' or file.endswith('.pdf'):
                    continue
                    
                file_path = os.path.join(root, file)
                # Ensure the path printed is relative to the base directory
                rel_path = os.path.relpath(file_path, base_dir)
                
                try:
                    with open(file_path, 'r', encoding='utf-8') as infile:
                        content = infile.read()
                        
                    # Write separator and file path inside
                    outfile.write(f"\n\n======= FILE: {rel_path} =======\n\n")
                    outfile.write(content)
                    
                except Exception as e:
                    print(f"Warning: Could not read {rel_path} - {e}")
                    
    print(f"Export complete. The file was saved at: {output_file}")

if __name__ == "__main__":
    export_for_notebooklm()


======= FILE: scripts/verify_deployment.py =======

"""
Verification script for Hybrid ML Scheduler deployment.
Checks all API endpoints and verifies system health.
"""
import requests
import sys
import time
import json
from termcolor import colored

BASE_URL = "http://localhost:8000"

def print_status(message, status="INFO"):
    if status == "INFO":
        print(colored(f"[INFO] {message}", "blue"))
    elif status == "SUCCESS":
        print(colored(f"[SUCCESS] {message}", "green"))
    elif status == "ERROR":
        print(colored(f"[ERROR] {message}", "red"))
    elif status == "WARNING":
        print(colored(f"[WARNING] {message}", "yellow"))

def check_endpoint(path, method="GET", expected_status=200, description=""):
    url = f"{BASE_URL}{path}"
    try:
        start_time = time.time()
        if method == "GET":
            response = requests.get(url)
        elif method == "POST":
            response = requests.post(url)
        duration = (time.time() - start_time) * 1000
        
        if response.status_code == expected_status:
            print_status(f"âœ“ {method} {path} - {response.status_code} ({duration:.2f}ms) - {description}", "SUCCESS")
            return True, response
        else:
            print_status(f"âœ— {method} {path} - Expected {expected_status}, got {response.status_code}", "ERROR")
            print(f"Response: {response.text}")
            return False, response
    except Exception as e:
        print_status(f"âœ— {method} {path} - Connection failed: {e}", "ERROR")
        return False, None

def verify_deployment():
    print_status("Starting deployment verification...", "INFO")
    
    # Wait for server to be ready
    max_retries = 5
    for i in range(max_retries):
        try:
            requests.get(f"{BASE_URL}/health")
            break
        except requests.ConnectionError:
            if i == max_retries - 1:
                print_status("Server is not reachable after 5 retries. Is it running?", "ERROR")
                sys.exit(1)
            print_status(f"Waiting for server... ({i+1}/{max_retries})", "WARNING")
            time.sleep(2)

    success_count = 0
    total_checks = 0

    # 1. Health Checks
    print("\n--- Health Checks ---")
    checks = [
        ("/health", "GET", 200, "Basic health check"),
        ("/health/ready", "GET", 200, "Readiness check (DB+Redis)"),
        ("/health/live", "GET", 200, "Liveness check"),
        ("/health/info", "GET", 200, "System info & version"),
    ]
    
    for path, method, status, desc in checks:
        total_checks += 1
        ok, _ = check_endpoint(path, method, status, desc)
        if ok: success_count += 1

    # 2. Metrics & Observability
    print("\n--- Metrics & Observability ---")
    checks = [
        ("/metrics/prometheus", "GET", 200, "Prometheus metrics"),
        ("/metrics/cache", "GET", 200, "Cache statistics"),
        ("/observability/health-check", "GET", 200, "Observability health"),
        ("/observability/logs/tail?lines=5", "GET", 200, "Log viewing"),
    ]
    
    for path, method, status, desc in checks:
        total_checks += 1
        ok, _ = check_endpoint(path, method, status, desc)
        if ok: success_count += 1

    # 3. Simulation Control
    print("\n--- Simulation Control ---")
    # Check status first
    total_checks += 1
    ok, resp = check_endpoint("/api/status", "GET", 200, "Simulation status")
    if ok:
        success_count += 1
        data = resp.json()
        print(f"   Status: Running={data.get('is_running')}, Paused={data.get('is_paused')}")
        print(f"   Tasks Processed: {data.get('tasks_processed')}")

    # 4. Security Headers
    print("\n--- Security Headers ---")
    total_checks += 1
    ok, resp = check_endpoint("/health", "GET", 200, "Checking security headers")
    if ok:
        headers = resp.headers
        security_headers = [
            "Strict-Transport-Security",
            "X-Content-Type-Options",
            "X-Frame-Options",
            "X-XSS-Protection",
            "Content-Security-Policy"
        ]
        
        all_headers_present = True
        for h in security_headers:
            if h in headers:
                print_status(f"âœ“ Header present: {h}", "SUCCESS")
            else:
                print_status(f"âœ— Missing header: {h}", "ERROR")
                all_headers_present = False
        
        if all_headers_present:
            success_count += 1

    # 5. Rate Limiting
    print("\n--- Rate Limiting ---")
    print_status("Testing rate limiting (sending 5 requests)...", "INFO")
    rate_limit_ok = True
    for i in range(5):
        ok, resp = check_endpoint("/health", "GET", 200, f"Request {i+1}")
        if not ok: rate_limit_ok = False
        if "X-RateLimit-Remaining" in resp.headers:
            print(f"   Remaining: {resp.headers['X-RateLimit-Remaining']}")
    
    if rate_limit_ok:
        success_count += 1
        total_checks += 1

    # Summary
    print("\n--- Verification Summary ---")
    if success_count == total_checks:
        print_status(f"ALL CHECKS PASSED ({success_count}/{total_checks})", "SUCCESS")
        sys.exit(0)
    else:
        print_status(f"SOME CHECKS FAILED ({success_count}/{total_checks})", "ERROR")
        sys.exit(1)

if __name__ == "__main__":
    verify_deployment()


======= FILE: scripts/generate_pdf.py =======


import markdown
from xhtml2pdf import pisa
import os

def convert_md_to_pdf(source_md, output_pdf):
    # Read Markdown
    with open(source_md, 'r', encoding='utf-8') as f:
        text = f.read()

    # Convert to HTML
    # Enable extensions for tables and fenced code blocks
    html = markdown.markdown(text, extensions=['tables', 'fenced_code'])

    # Add some basic CSS for styling
    css = """
    <style>
        body { font-family: Helvetica, sans-serif; font-size: 10pt; }
        h1 { color: #2c3e50; font-size: 24pt; border-bottom: 2px solid #2c3e50; padding-bottom: 10px; }
        h2 { color: #34495e; font-size: 18pt; margin-top: 20px; }
        h3 { color: #7f8c8d; font-size: 14pt; margin-top: 15px; }
        code { background-color: #f4f4f4; padding: 2px; font-family: Courier; }
        pre { background-color: #f8f9fa; padding: 10px; border: 1px solid #e9ecef; white-space: pre-wrap; }
        table { width: 100%; border-collapse: collapse; margin-top: 10px; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
        img { max-width: 100%; height: auto; margin: 20px 0; }
    </style>
    """
    
    full_html = f"<html><head>{css}</head><body>{html}</body></html>"

    # Write HTML for debug (optional)
    # with open("debug.html", "w") as f:
    #     f.write(full_html)

    # Convert to PDF
    # Helper to fix image paths for xhtml2pdf
    from urllib.parse import unquote
    def link_callback(uri, rel):
        # Decode URI (e.g. %20 -> space)
        uri = unquote(uri)
        
        if not uri.startswith('http'):
            # Convert relative path to absolute
            abs_path = os.path.abspath(uri)
            if os.path.exists(abs_path):
                return abs_path
        return uri

    with open(output_pdf, "wb") as result_file:
        pisa_status = pisa.CreatePDF(
            full_html,
            dest=result_file,
            encoding='utf-8',
            link_callback=link_callback
        )

    if pisa_status.err:
        print(f"Error converting to PDF: {pisa_status.err}")
        return False
    
    print(f"Successfully created {output_pdf}")
    return True

if __name__ == "__main__":
    source = "PROJECT_WIKI_DOCUMENTATION.md"
    output = "PROJECT_WIKI_DOCUMENTATION.pdf"
    
    if os.path.exists(source):
        convert_md_to_pdf(source, output)
    else:
        print(f"Source file {source} not found!")


======= FILE: scripts/init_db.py =======

"""
Database initialization and setup script.

Creates the database, tables, and initial data.
"""
import asyncio
import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from sqlalchemy import create_engine, text
from loguru import logger

from backend.core.config import settings
from backend.core.database import init_db, engine, Base


async def create_database():
    """Create the database if it doesn't exist."""
    import psycopg2
    from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT
    
    try:
        # Connect to postgres database to create our database
        conn = psycopg2.connect(
            host=settings.postgres_host,
            port=settings.postgres_port,
            user=settings.postgres_user,
            password=settings.postgres_password,
            database="postgres"
        )
        conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)
        cursor = conn.cursor()
        
        # Check if database exists
        cursor.execute(
            "SELECT 1 FROM pg_database WHERE datname = %s",
            (settings.postgres_db,)
        )
        exists = cursor.fetchone()
        
        if not exists:
            # Create database
            cursor.execute(f"CREATE DATABASE {settings.postgres_db}")
            logger.info(f"Database '{settings.postgres_db}' created successfully")
        else:
            logger.info(f"Database '{settings.postgres_db}' already exists")
        
        cursor.close()
        conn.close()
        
    except Exception as e:
        logger.error(f"Error creating database: {e}")
        raise


async def create_tables():
    """Create all database tables."""
    try:
        async with engine.begin() as conn:
            # Import models to register them
            from backend.models.domain import Task, SchedulerResult, Metric, TrainingData, SimulationState
            
            # Create all tables
            await conn.run_sync(Base.metadata.create_all)
            logger.info("All tables created successfully")
    except Exception as e:
        logger.error(f"Error creating tables: {e}")
        raise


async def verify_setup():
    """Verify database setup."""
    try:
        async with engine.begin() as conn:
            result = await conn.execute(
                text("SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'")
            )
            tables = [row[0] for row in result]
            
            logger.info(f"Found {len(tables)} tables:")
            for table in tables:
                logger.info(f"  - {table}")
            
            return len(tables) > 0
    except Exception as e:
        logger.error(f"Error verifying setup: {e}")
        return False


async def main():
    """Main setup function."""
    logger.info("=" * 80)
    logger.info("DATABASE INITIALIZATION")
    logger.info("=" * 80)
    logger.info(f"Database: {settings.postgres_db}")
    logger.info(f"Host: {settings.postgres_host}:{settings.postgres_port}")
    logger.info(f"User: {settings.postgres_user}")
    logger.info("=" * 80)
    
    # Step 1: Create database
    logger.info("\n[1/3] Creating database...")
    await create_database()
    
    # Step 2: Create tables
    logger.info("\n[2/3] Creating tables...")
    await create_tables()
    
    # Step 3: Verify setup
    logger.info("\n[3/3] Verifying setup...")
    success = await verify_setup()
    
    if success:
        logger.info("\nâœ… Database initialization complete!")
    else:
        logger.error("\nâŒ Database initialization failed!")
        sys.exit(1)
    
    # Close connections
    from backend.core.database import close_db
    await close_db()


if __name__ == "__main__":
    asyncio.run(main())


======= FILE: .agent/workflows/backend_refactoring_plan.md =======

---
description: Complete Backend Refactoring Implementation Plan
---

# Backend Refactoring Implementation Plan

## Overview
This plan outlines the comprehensive refactoring of the backend to implement enterprise-grade features including database integration, caching, improved architecture, observability, and security.

## Phase 1: Foundation & Infrastructure Setup âš™ï¸

### 1.1 Database Setup
- [x] Create PostgreSQL database `hybrid_scheduler_db`
- [ ] Create database schema (tasks, metrics, scheduler_results)
- [ ] Create migration scripts
- [ ] Add database connection pooling
- [ ] Implement async database client (asyncpg)

### 1.2 Redis Setup
- [ ] Install Redis locally or via Docker
- [ ] Configure Redis connection
- [ ] Implement caching utilities

### 1.3 Project Structure Refactoring
- [ ] Create layered architecture:
  ```
  backend/
  â”œâ”€â”€ api/
  â”‚   â”œâ”€â”€ routes/          # API endpoints
  â”‚   â”œâ”€â”€ dependencies.py  # FastAPI dependencies
  â”‚   â””â”€â”€ middleware.py    # Custom middleware
  â”œâ”€â”€ core/
  â”‚   â”œâ”€â”€ config.py        # Configuration management
  â”‚   â”œâ”€â”€ database.py      # Database setup
  â”‚   â”œâ”€â”€ redis.py         # Redis setup
  â”‚   â””â”€â”€ security.py      # Auth utilities
  â”œâ”€â”€ models/
  â”‚   â”œâ”€â”€ domain.py        # Domain models
  â”‚   â””â”€â”€ schemas.py       # Pydantic schemas
  â”œâ”€â”€ repositories/        # Data access layer
  â”œâ”€â”€ services/            # Business logic
  â””â”€â”€ utils/               # Helper functions
  ```

## Phase 2: Core Refactoring ğŸ—ï¸

### 2.1 Database Integration
- [ ] Create SQLAlchemy models
- [ ] Implement repository pattern
- [ ] Migrate CSV data to PostgreSQL
- [ ] Update simulation engine to use DB

### 2.2 API Architecture
- [ ] Split dashboard_server.py into routes
- [ ] Implement service layer
- [ ] Add Pydantic models for validation
- [ ] Implement API versioning (/api/v1/)

### 2.3 Configuration Management
- [ ] Create Pydantic Settings
- [ ] Add .env support
- [ ] Environment-specific configs

## Phase 3: Performance & Scalability âš¡

### 3.1 Redis Caching
- [ ] Cache recent metrics
- [ ] Cache scheduler leaderboard
- [ ] Implement cache invalidation
- [ ] Add TTL policies

### 3.2 Async Optimization
- [ ] Use async database drivers
- [ ] Implement connection pooling
- [ ] Add background task queue (Celery)
- [ ] Optimize WebSocket broadcasts

### 3.3 WebSocket Enhancements
- [ ] Add message compression
- [ ] Implement heartbeat/ping-pong
- [ ] Add subscription topics
- [ ] Implement message buffering

## Phase 4: Observability & Monitoring ğŸ“Š

### 4.1 Metrics & Logging
- [ ] Add Prometheus metrics
- [ ] Implement structured logging
- [ ] Add correlation IDs
- [ ] Create custom metrics for schedulers

### 4.2 Tracing
- [ ] Add OpenTelemetry
- [ ] Implement distributed tracing
- [ ] Add performance profiling

### 4.3 Health Checks
- [ ] /health endpoint
- [ ] /ready endpoint
- [ ] Database health check
- [ ] Redis health check

## Phase 5: Resilience & Error Handling ğŸ›¡ï¸

### 5.1 Error Handling
- [ ] Implement circuit breaker pattern
- [ ] Add exponential backoff
- [ ] Graceful degradation
- [ ] Custom exception handling

### 5.2 Retry Logic
- [ ] Database retry logic
- [ ] External service retries
- [ ] WebSocket reconnection

## Phase 6: Security & Testing ğŸ”’

### 6.1 Security
- [ ] JWT authentication
- [ ] Rate limiting (per client)
- [ ] CORS whitelist
- [ ] Input sanitization
- [ ] Request size limits

### 6.2 Testing
- [ ] Integration tests
- [ ] Load tests (Locust)
- [ ] Contract tests
- [ ] WebSocket tests

## Phase 7: Deployment & Documentation ğŸ“¦

### 7.1 Containerization
- [ ] Update Dockerfile
- [ ] Docker Compose with all services
- [ ] Environment variables

### 7.2 Documentation
- [ ] API documentation (Swagger/OpenAPI)
- [ ] Architecture diagrams
- [ ] Deployment guide
- [ ] Developer guide

## Implementation Order

**Day 1: Foundation**
1. Database setup and schema
2. Project restructuring
3. Configuration management

**Day 2: Core Refactoring**
4. Database integration
5. API architecture refactoring
6. Repository pattern

**Day 3: Performance**
7. Redis caching
8. Async optimization
9. WebSocket improvements

**Day 4: Observability**
10. Metrics and logging
11. Health checks
12. Tracing

**Day 5: Polish**
13. Error handling
14. Security
15. Testing
16. Documentation

## Success Criteria
- âœ… All tests passing
- âœ… API response time < 100ms (p95)
- âœ… WebSocket latency < 50ms
- âœ… Database query time < 50ms (p95)
- âœ… 99.9% uptime in load tests
- âœ… Zero data loss during failures


======= FILE: src/online_scheduler.py =======

"""
Online Scheduler - Real-time task scheduling using ML models
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple
from loguru import logger
from dataclasses import dataclass
import time

from .ml_models import PerformancePredictor
from .workload_generator import Task
from .profiler import HardwareProfiler


@dataclass
class ResourceState:
    """Current state of a virtual GPU"""
    gpu_id: int
    total_memory: int
    available_memory: int
    current_load: float
    tasks_running: int


class OnlineScheduler:
    """Uses trained ML models to dynamically schedule tasks at runtime"""
    
    def __init__(self, model: PerformancePredictor, num_gpus: int = 4, energy_weight: float = 0.5, monitor_callback=None):
        self.model = model
        self.num_gpus = num_gpus
        self.energy_weight = energy_weight
        self.monitor_callback = monitor_callback
        self.gpu_states = [
            ResourceState(
                gpu_id=i,
                total_memory=8000,
                available_memory=8000,
                current_load=0.0,
                tasks_running=0
            )
            for i in range(num_gpus)
        ]
        
        self.task_queue = []
        self.scheduled_tasks = []
        self.task_timeline = []
        
        logger.info(f"OnlineScheduler initialized with {num_gpus} virtual GPUs")
    
    def submit_task(self, task: Task):
        """Submit a task for scheduling"""
        self.task_queue.append(task)
        logger.debug(f"Task {task.task_id} submitted")
    
    def _predict_placement(self, task: Task) -> Dict:
        """Use ML model to predict best placement"""
        # Prepare features for model
        features = pd.DataFrame([{
            'size': task.size,
            'compute_intensity': task.compute_intensity,
            'memory_required': task.memory_required,
            'memory_per_size': task.memory_required / (task.size + 1),
            'compute_to_memory': task.compute_intensity / (task.memory_required + 1),
        }])
        
        # Get model prediction
        gpu_fraction_pred = self.model.predict(features)[0]
        
        # Clamp prediction
        gpu_fraction_pred = np.clip(gpu_fraction_pred, 0.0, 1.0)
        
        return {
            'gpu_fraction': gpu_fraction_pred,
            'cpu_fraction': 1.0 - gpu_fraction_pred,
        }
    
    def _find_best_gpu(self, task: Task, gpu_fraction: float) -> int:
        """Find best GPU to assign task to using Energy-Aware Cost"""
        best_gpu = -1
        min_cost = float('inf')
        
        for i in range(self.num_gpus):
            gpu_state = self.gpu_states[i]
            
            # Estimate execution time (simple model: load increases time)
            base_time = task.duration_estimate
            load_factor = 1.0 + gpu_state.current_load
            estimated_time = base_time * load_factor
            
            # Estimate energy
            # If gpu_fraction > 0.5, we treat it as GPU-heavy
            is_gpu_task = gpu_fraction > 0.5
            estimated_energy = HardwareProfiler.estimate_energy(estimated_time, is_gpu_task)
            
            # Weighted Cost
            # Normalize time and energy to roughly 0-1 range for fair weighting
            # (Assuming max time ~10s, max energy ~500J)
            norm_time = estimated_time / 10.0
            norm_energy = estimated_energy / 500.0
            
            cost = (1.0 - self.energy_weight) * norm_time + self.energy_weight * norm_energy
            
            if cost < min_cost:
                min_cost = cost
                best_gpu = i
                
        return best_gpu
    
    def schedule_task(self, task: Task) -> Dict:
        """Schedule a single task"""
        # Get placement prediction
        placement = self._predict_placement(task)
        
        # Find best GPU
        gpu_id = self._find_best_gpu(task, placement['gpu_fraction'])
        
        # Allocate resources
        gpu_state = self.gpu_states[gpu_id]
        
        gpu_state.available_memory -= task.memory_required
        gpu_state.tasks_running += 1
        gpu_state.current_load = 1.0 - (gpu_state.available_memory / gpu_state.total_memory)
        
        decision = {
            'task_id': task.task_id,
            'gpu_id': gpu_id,
            'gpu_fraction': placement['gpu_fraction'],
            'cpu_fraction': placement['cpu_fraction'],
            'estimated_time': task.duration_estimate,
            'scheduled_time': time.time(),
        }
        
        self.scheduled_tasks.append(decision)
        logger.debug(f"Task {task.task_id} scheduled on GPU {gpu_id}")
        
        if self.monitor_callback:
            self.monitor_callback({
                'type': 'decision',
                'data': decision,
                'utilization': self.get_utilization()
            })
        
        return decision
    
    def process_queue(self) -> List[Dict]:
        """Process all tasks in queue respecting dependencies"""
        logger.info(f"Processing queue with {len(self.task_queue)} tasks")
        
        # Separate tasks into ready (no deps) and waiting
        ready_queue = []
        waiting_queue = []
        completed_tasks = set()
        
        # Initial sort
        for task in self.task_queue:
            if not task.dependencies:
                ready_queue.append(task)
            else:
                waiting_queue.append(task)
        
        decisions = []
        
        # Process loop
        while ready_queue:
            # Get next ready task
            task = ready_queue.pop(0)
            
            # Schedule it
            decision = self.schedule_task(task)
            decisions.append(decision)
            completed_tasks.add(task.task_id)
            
            # Check waiting queue for newly ready tasks
            # Iterate backwards to allow safe removal
            for i in range(len(waiting_queue) - 1, -1, -1):
                waiting_task = waiting_queue[i]
                # Check if all dependencies are met
                if all(dep_id in completed_tasks for dep_id in waiting_task.dependencies):
                    ready_queue.append(waiting_task)
                    waiting_queue.pop(i)
        
        if waiting_queue:
            logger.warning(f"Could not schedule {len(waiting_queue)} tasks due to missing dependencies/cycles")
            
        self.task_queue.clear()
        logger.info(f"Processed {len(decisions)} tasks")
        
        return decisions
    
    def get_utilization(self) -> Dict:
        """Get current resource utilization"""
        utilizations = {}
        for gpu_state in self.gpu_states:
            utilizations[f'gpu_{gpu_state.gpu_id}'] = {
                'utilization': float(gpu_state.current_load),
                'tasks_running': gpu_state.tasks_running,
                'available_memory': gpu_state.available_memory,
            }
        
        avg_util = np.mean([s.current_load for s in self.gpu_states])
        utilizations['average_utilization'] = float(avg_util)
        
        return utilizations
    
    def reset_state(self):
        """Reset scheduler state"""
        for gpu_state in self.gpu_states:
            gpu_state.available_memory = gpu_state.total_memory
            gpu_state.current_load = 0.0
            gpu_state.tasks_running = 0
        
        self.task_queue.clear()
        self.scheduled_tasks.clear()
        
        logger.info("Scheduler state reset")
        
    def randomize_resources(self):
        """Randomize GPU states to simulate dynamic environment"""
        import random
        for gpu_state in self.gpu_states:
            # Random load between 0.0 and 0.9
            gpu_state.current_load = random.uniform(0.0, 0.9)
            # Random memory availability
            gpu_state.available_memory = random.uniform(500, gpu_state.total_memory)
            # Random tasks count
            gpu_state.tasks_running = random.randint(0, 10)


======= FILE: src/__init__.py =======

"""
Hybrid Offline-Online ML Scheduler for Parallel Computing
Package initialization
"""

__version__ = "1.0.0"
__author__ = "Your Name"

from .profiler import HardwareProfiler
from .workload_generator import WorkloadGenerator
from .ml_models import PerformancePredictor, RandomForestPredictor, XGBoostPredictor
from .offline_trainer import OfflineTrainer
from .online_scheduler import OnlineScheduler
from .simulator import VirtualMultiGPU

__all__ = [
    "HardwareProfiler",
    "WorkloadGenerator",
    "PerformancePredictor",
    "RandomForestPredictor",
    "XGBoostPredictor",
    "OfflineTrainer",
    "OnlineScheduler",
    "VirtualMultiGPU",
]


======= FILE: src/dqn_scheduler.py =======

"""
Deep Q-Network (DQN) Scheduler
Uses Deep Reinforcement Learning to optimize task scheduling
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque
from typing import Dict, List, Tuple, Any
from loguru import logger

from .workload_generator import Task
from .profiler import HardwareProfiler
from .online_scheduler import OnlineScheduler, ResourceState

# Define the Neural Network
class DQN(nn.Module):
    def __init__(self, input_dim: int, output_dim: int):
        super(DQN, self).__init__()
        
        # Enhanced Architecture with Batch Norm and Dropout
        self.fc1 = nn.Linear(input_dim, 256)
        self.bn1 = nn.BatchNorm1d(256)
        self.fc2 = nn.Linear(256, 256)
        self.bn2 = nn.BatchNorm1d(256)
        
        # Dueling DQN: Value and Advantage streams
        self.value_stream = nn.Linear(256, 1)
        self.advantage_stream = nn.Linear(256, output_dim)
        
        self.relu = nn.ReLU()

    def forward(self, x):
        # Handle single sample (batch norm requires > 1 sample usually, but we can manage)
        if x.dim() == 1:
            x = x.unsqueeze(0)
            
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        
        val = self.value_stream(x)
        adv = self.advantage_stream(x)
        
        # Combine: Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))
        return val + (adv - adv.mean(dim=1, keepdim=True))

# Experience Replay Buffer
class ReplayBuffer:
    def __init__(self, capacity: int):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size: int):
        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))
        return np.array(state), action, reward, np.array(next_state), done

    def __len__(self):
        return len(self.buffer)

class DQNScheduler(OnlineScheduler):
    """
    Scheduler that uses Deep Q-Learning (DQN) to make decisions.
    Now supports interactive learning loop.
    """
    
    def __init__(self, num_gpus: int = 4, energy_weight: float = 0.2, 
                 learning_rate: float = 0.0001, gamma: float = 0.99, 
                 epsilon_start: float = 0.5, epsilon_end: float = 0.05, 
                 epsilon_decay: float = 0.9995, buffer_size: int = 50000,
                 batch_size: int = 64, target_update: int = 50,
                 monitor_callback=None):
        
        super().__init__(model=None, num_gpus=num_gpus, energy_weight=energy_weight, monitor_callback=monitor_callback)
        self.gpu_states = [
            ResourceState(
                gpu_id=i,
                total_memory=8000,
                available_memory=8000,
                current_load=0.0,
                tasks_running=0
            )
            for i in range(num_gpus)
        ]
        
        # RL Parameters
        self.gamma = gamma
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        self.batch_size = batch_size
        self.target_update = target_update
        self.update_counter = 0
        self.energy_weight = energy_weight
        
        # State Dimension: 
        # Task Features (3): Size, Intensity, Memory
        # Total = 3
        self.state_dim = 3
        
        
        # Action Dimension: 
        # We use DISCRETE actions to represent CONTINUOUS fractions.
        # Actions: 0=0.0, 1=0.2, 2=0.4, 3=0.6, 4=0.8, 5=1.0
        self.action_bins = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
        self.action_dim = len(self.action_bins)
        
        # Networks
        self.device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
        self.policy_net = DQN(self.state_dim, self.action_dim).to(self.device)
        self.target_net = DQN(self.state_dim, self.action_dim).to(self.device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()
        
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)
        self.memory = ReplayBuffer(buffer_size)
        
        logger.info(f"DQNScheduler initialized on {self.device} with Fractional Actions")

    def _get_state_vector(self, task: Task) -> np.ndarray:
        """
        Construct continuous state vector [Size, Intensity, Memory]
        """
        task_features = [
            task.size / 10000.0, # Normalize
            task.compute_intensity,
            task.memory_required / 5000.0 # Normalize
        ]
        return np.array(task_features, dtype=np.float32)

    def get_action(self, task: Task) -> Dict:
        """
        Select an action for the given task.
        Returns detailed action info for simulation.
        """
        state = self._get_state_vector(task)
        
        # Epsilon-Greedy
        if random.random() < self.epsilon:
            action = random.randrange(self.action_dim)
        else:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).to(self.device)
                q_values = self.policy_net(state_tensor)
                action = q_values.argmax().item()
                
        # Decode Action (Discrete Bin -> Continuous Fraction)
        gpu_fraction = self.action_bins[action]
        
        # GPU ID Selection (Load Balancing Heuristic)
        # The RL agent decides "How much offload", the scheduler decides "Where".
        # We use a simple heuristic: pick gpu_id based on a hash if we don't have load info,
        # or -1 if Fraction is 0.0.
        if gpu_fraction == 0.0:
            gpu_id = -1
        else:
            # Round Robin-ish based on task ID for stability
            gpu_id = task.task_id % self.num_gpus
            
        return {
            'action': action,
            'gpu_fraction': gpu_fraction,
            'gpu_id': gpu_id,
            'state_vector': state # Keep for observation
        }

    def observe(self, task: Task, action: int, reward_metrics: Dict):
        """
        Receive feedback from the environment (Simulator) and learn.
        """
        # 1. Calculate Reward
        # GOAL: Balance Time and Energy.
        # Reward Function: R = -Cost
        # Cost = (1-w) * Normalized_Time + w * Normalized_Energy
        #
        # Note: We do NOT include financial cost ($) in the reward function here,
        # so the agent does not care about the price of GPU vs CPU.
        
        time_taken = reward_metrics['time']
        energy_used = reward_metrics['energy']
        
        # Normalize for numerical stability (DQN learns better with rewards close to 0-1 range)
        # Using approximated max values (10s and 500J) based on workload config.
        norm_time = time_taken / 10.0 
        norm_energy = energy_used / 500.0
        
        cost = (1.0 - self.energy_weight) * norm_time + self.energy_weight * norm_energy
        reward = -cost
        
        # 2. Store in Memory
        current_state = self._get_state_vector(task)
        
        # Since tasks are independent (IID), the next state is effectively a new random task.
        # We treat this as a terminal state for the scheduling "episode" of one task.
        next_state = np.zeros_like(current_state) # Dummy next state
        done = True
        

        
        self.memory.push(current_state, action, reward, next_state, done)
        
        # 3. Train
        loss = self.train_step()
        
        # 4. Decay Epsilon
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)
        
        # 5. Log/Monitor (Optional)
        # if self.update_counter % 100 == 0:
        #    logger.debug(f"RL Train: Eps={self.epsilon:.4f}, Reward={reward:.4f}, Loss={loss}")

    def train_step(self) -> float:
        """Perform one step of optimization"""
        if len(self.memory) < self.batch_size:
            return 0.0
        
        state, action, reward, next_state, done = self.memory.sample(self.batch_size)
        
        state = torch.FloatTensor(state).to(self.device)
        action = torch.LongTensor(action).unsqueeze(1).to(self.device)
        reward = torch.FloatTensor(reward).unsqueeze(1).to(self.device)
        next_state = torch.FloatTensor(next_state).to(self.device)
        done = torch.FloatTensor(done).unsqueeze(1).to(self.device)
        
        # Compute Q(s, a)
        q_values = self.policy_net(state).gather(1, action)
        
        # Compute Target Q
        with torch.no_grad():
            # Double DQN
            next_actions = self.policy_net(next_state).argmax(1).unsqueeze(1)
            next_q_values = self.target_net(next_state).gather(1, next_actions)
            expected_q_values = reward + (self.gamma * next_q_values * (1 - done))
            
        # Loss
        loss = nn.SmoothL1Loss()(q_values, expected_q_values)
        
        # Optimize
        self.optimizer.zero_grad()
        loss.backward()
        # Clip gradients
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)
        self.optimizer.step()
        
        # Update target network
        self.update_counter += 1
        if self.update_counter % self.target_update == 0:
            self.target_net.load_state_dict(self.policy_net.state_dict())
            
        return loss.item()

    def save_model(self, filepath: str):
        torch.save(self.policy_net.state_dict(), filepath)

    def load_model(self, filepath: str):
        self.policy_net.load_state_dict(torch.load(filepath))
        self.policy_net.eval()
        
    # Legacy method wrapper if needed, but we use get_action now
    def schedule_task(self, task: Task) -> Dict:
        # This is for the OnlineScheduler interface if used directly
        action_dict = self.get_action(task)
        # We can't observe() here because we don't have the result yet.
        # This compatibility method might need adjustment if used elsewhere.
        return {
            'task_id': task.task_id,
            'gpu_id': action_dict['gpu_id'],
            'gpu_fraction': action_dict['gpu_fraction'],
            'estimated_time': task.duration_estimate, # Placeholder
            'scheduled_time': 0,
            'action': action_dict['action']
        }

    def pretrain(self, tasks: List[Task], epochs: int = 10):
        """
        Pre-train the agent using a simple heuristic to warm-start the policy.
        Heuristic: High Compute Intensity (> 0.5) -> GPU, else CPU.
        """
        logger.info(f"Pre-training DQN on {len(tasks)} tasks for {epochs} epochs...")
        
        # Fill Replay Buffer with Heuristic Demonstrations
        for task in tasks:
            state = self._get_state_vector(task)
            
            # Heuristic Labeling (Improved for Time)
            # We want to match the Oracle's behavior approximately.
            # Small Task -> 0.0
            # Big Task + High Intensity -> 1.0
            # Medium -> 0.4 or 0.6
            
            if task.size < 500:
                target_frac = 0.0
            elif task.compute_intensity > 0.8:
                target_frac = 1.0
            elif task.compute_intensity > 0.5:
                # Linear mapping for medium intensity
                target_frac = 0.6
            else:
                target_frac = 0.0
            
            # Find closest bin index
            action = min(range(len(self.action_bins)), key=lambda i: abs(self.action_bins[i]-target_frac))
                
            # Fake Reward (Positive for following heuristic)
            reward = 1.0 
            
            next_state = np.zeros_like(state)
            done = True
            
            self.memory.push(state, action, reward, next_state, done)
            
        # Training Loop
        initial_epsilon = self.epsilon
        self.epsilon = 0.5 # Allow some exploration of the buffer? No, off-policy.
        
        for epoch in range(epochs):
            total_loss = 0
            # Train for a few batches per epoch
            steps = len(tasks) // self.batch_size
            if steps == 0 and len(self.memory) > self.batch_size:
                steps = len(self.memory) // self.batch_size
            
            if steps == 0:
                 logger.warning("Not enough data to train!")
                 break

            for _ in range(steps):
                loss = self.train_step()
                total_loss += loss
            
            if epoch % 10 == 0:
                logger.debug(f"Pretrain Epoch {epoch}: Avg Loss = {total_loss/steps:.4f}")
                
        self.epsilon = initial_epsilon
        logger.info("Pre-training complete.")


======= FILE: src/workload_generator.py =======

"""
Workload Generator
Creates synthetic parallel workloads for training and evaluation
"""

import numpy as np
import pandas as pd
from typing import List, Dict, Tuple
from dataclasses import dataclass
from loguru import logger


@dataclass
class Task:
    """Represents a parallel task"""
    task_id: int
    size: int  # Problem size
    compute_intensity: float  # 0-1, higher = more GPU-friendly
    memory_required: int  # MB
    arrival_time: float
    duration_estimate: float = None
    dependencies: List[int] = None  # List of parent task IDs
    
    def __post_init__(self):
        if self.dependencies is None:
            self.dependencies = []

    def to_dict(self) -> Dict:
        return {
            'task_id': self.task_id,
            'size': self.size,
            'compute_intensity': self.compute_intensity,
            'memory_required': self.memory_required,
            'arrival_time': self.arrival_time,
            'duration_estimate': self.duration_estimate,
            'dependencies': str(self.dependencies) # Store as string for CSV simplicity
        }


class WorkloadGenerator:
    """Generates synthetic but realistic workloads"""
    
    def __init__(self, seed: int = 42):
        np.random.seed(seed)
        self.tasks = []
        logger.info(f"WorkloadGenerator initialized with seed {seed}")
    
    def generate_workload_stream(
        self,
        num_tasks: int = 1000,
        task_size_range: Tuple[int, int] = (100, 5000),
        compute_intensity_range: Tuple[float, float] = (0.1, 1.0),
        memory_range: Tuple[int, int] = (10, 500),
        arrival_rate: float = 100.0,
        duration_model: str = "size_based",
        dependency_prob: float = 0.2,
        max_dependencies: int = 3
    ):
        """
        Generator that yields tasks one by one for memory efficiency.
        
        This method uses Python's `yield` keyword to create a generator. This is crucial for 
        large-scale simulations where creating millions of Task objects at once would consume 
        too much RAM. Instead, tasks are created on-the-fly as they are requested.
        
        Args:
            num_tasks: Number of tasks to generate
            task_size_range: (min_size, max_size) for the problem size
            compute_intensity_range: (min_intensity, max_intensity) where higher means more GPU-bound
            memory_range: (min_memory_MB, max_memory_MB) required by the task
            arrival_rate: Average number of tasks arriving per second (Poisson process parameter)
            duration_model: Model to estimate task duration ('size_based' or 'random')
            dependency_prob: Probability that a task depends on previous tasks
            max_dependencies: Maximum number of dependencies per task
            
            
        Yields:
            Task object: A single generated task
        """
        current_time = 0.0
        
        for i in range(num_tasks):
            # Generate inter-arrival time using Exponential distribution
            # This models a Poisson arrival process, which is standard for queuing systems
            inter_arrival = np.random.exponential(1.0 / arrival_rate)
            current_time += inter_arrival
            
            # 1. Task Size: Use Pareto distribution (Heavy-tailed)
            # Many small tasks, few very large ones (realistic for HPC/Cloud)
            shape = 1.5  # Pareto shape parameter (1.16 is roughly 80/20 rule)
            size = (np.random.pareto(shape) + 1) * task_size_range[0]
            size = int(min(size, task_size_range[1])) # Clip to max size
            
            # 2. Compute Intensity: Use Bimodal distribution
            # Most tasks are either CPU-bound (low intensity) or GPU-bound (high intensity)
            if np.random.random() < 0.3:
                # CPU-bound peak
                compute_intensity = np.random.normal(0.2, 0.1)
            else:
                # GPU-bound peak
                compute_intensity = np.random.normal(0.8, 0.1)
            compute_intensity = np.clip(compute_intensity, 0.05, 1.0) # Keep within bounds

            # 3. Memory: Correlated with size (larger tasks usually need more memory)
            # Add some noise so it's not a perfect correlation
            base_memory = (size / task_size_range[1]) * memory_range[1]
            noise = np.random.normal(0, memory_range[1] * 0.1)
            memory = int(np.clip(base_memory + noise, memory_range[0], memory_range[1]))
            
            # Generate dependencies
            dependencies = []
            if i > 0 and np.random.random() < dependency_prob:
                # Can only depend on previous tasks
                # Look back window to avoid long chains
                window_size = min(i, 50) 
                potential_parents = list(range(max(0, i - window_size), i))
                num_deps = np.random.randint(1, min(len(potential_parents), max_dependencies) + 1)
                dependencies = sorted(np.random.choice(potential_parents, num_deps, replace=False).tolist())
            
            
            # Estimate duration based on size and intensity
            if duration_model == "size_based":
                # Heuristic: Duration grows super-linearly with size (O(N^1.5))
                # and is inversely proportional to compute intensity (more intense = harder)
                # Note: This is a synthetic model for demonstration
                base_duration = (size / 1000) ** 1.5
                duration = base_duration / (compute_intensity + 0.5)
            else:
                # Fallback to simple random duration
                duration = np.random.exponential(0.1)
            
            # Create the Task object
            task = Task(
                task_id=i,
                size=size,
                compute_intensity=compute_intensity,
                memory_required=memory,
                arrival_time=current_time,
                duration_estimate=duration,
                dependencies=dependencies
            )
            
            # Yield the task to the consumer immediately
            # This pauses execution here until the next task is requested
            yield task

    def generate_workload(
        self,
        num_tasks: int = 1000,
        task_size_range: Tuple[int, int] = (100, 5000),
        compute_intensity_range: Tuple[float, float] = (0.1, 1.0),
        memory_range: Tuple[int, int] = (10, 500),
        arrival_rate: float = 100.0,
        duration_model: str = "size_based",
        dependency_prob: float = 0.2,
        max_dependencies: int = 3
    ) -> List[Task]:
        """
        Generate synthetic workload (returns full list).
        
        This method is a wrapper around `generate_workload_stream` for backward compatibility.
        It consumes the entire stream and returns a list of all tasks.
        
        Args:
            num_tasks: Number of tasks to generate
            task_size_range: (min_size, max_size)
            compute_intensity_range: (min_intensity, max_intensity)
            memory_range: (min_memory_MB, max_memory_MB)
            arrival_rate: Tasks per second
            duration_model: How to estimate task duration
            
        Returns:
            List of Task objects
        """
        logger.info(f"Generating {num_tasks} tasks...")
        
        # Consume the generator to create a list
        # WARNING: This may use a lot of memory for very large num_tasks
        self.tasks = list(self.generate_workload_stream(
            num_tasks=num_tasks,
            task_size_range=task_size_range,
            compute_intensity_range=compute_intensity_range,
            memory_range=memory_range,
            arrival_rate=arrival_rate,
            duration_model=duration_model,
            dependency_prob=dependency_prob,
            max_dependencies=max_dependencies
        ))
        
        logger.info(f"Generated {len(self.tasks)} tasks")
        return self.tasks
    
    def to_dataframe(self) -> pd.DataFrame:
        """Convert tasks to pandas DataFrame"""
        data = [task.to_dict() for task in self.tasks]
        df = pd.DataFrame(data)
        logger.debug(f"Converted {len(df)} tasks to DataFrame")
        return df
    
    def save_workload(self, filepath: str):
        """Save workload to CSV"""
        df = self.to_dataframe()
        df.to_csv(filepath, index=False)
        logger.info(f"Workload saved to {filepath}")
    
    @staticmethod
    def load_workload(filepath: str) -> 'WorkloadGenerator':
        """Load workload from CSV"""
        df = pd.read_csv(filepath)
        generator = WorkloadGenerator()
        
        for _, row in df.iterrows():
            task = Task(
                task_id=int(row['task_id']),
                size=int(row['size']),
                compute_intensity=float(row['compute_intensity']),
                memory_required=int(row['memory_required']),
                arrival_time=float(row['arrival_time']),
                duration_estimate=float(row['duration_estimate']),
                dependencies=eval(str(row.get('dependencies', '[]'))) # Safe eval for list string
            )
            generator.tasks.append(task)
        
        logger.info(f"Loaded {len(generator.tasks)} tasks from {filepath}")
        return generator
    
    def get_statistics(self) -> Dict:
        """Get workload statistics"""
        if not self.tasks:
            return {}
        
        sizes = [t.size for t in self.tasks]
        intensities = [t.compute_intensity for t in self.tasks]
        
        stats = {
            'num_tasks': len(self.tasks),
            'total_duration': self.tasks[-1].arrival_time,
            'avg_size': np.mean(sizes),
            'avg_intensity': np.mean(intensities),
            'size_std': np.std(sizes),
            'intensity_std': np.std(intensities),
        }
        
        logger.debug(f"Workload stats: {stats}")
        return stats


======= FILE: src/visualization.py =======

"""
Visualization Module
Generates plots and charts for analyzing scheduling performance
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List
from loguru import logger

def plot_comparison(results: Dict[str, Dict[str, float]], 
                   output_dir: str = "data/results/plots"):
    """
    Plot comparison of different scheduling strategies
    
    Args:
        results: Dictionary mapping strategy names to metrics
        output_dir: Directory to save plots
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    strategies = list(results.keys())
    metrics = list(results[strategies[0]].keys())
    
    # 1. Makespan Comparison
    plt.figure(figsize=(10, 6))
    makespans = [results[s]['makespan'] for s in strategies]
    # Use distinct colors from cmap
    colors = plt.cm.Set2(np.linspace(0, 1, len(strategies)))
    bars = plt.bar(strategies, makespans, color=colors)
    
    plt.title('Total Execution Time (Makespan) by Strategy', fontsize=14)
    plt.ylabel('Time (seconds)', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    
    # Add margin for labels
    if makespans:
        plt.ylim(0, max(makespans) * 1.15)
        
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    
    # Add value labels
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}s',
                ha='center', va='bottom')
                
    plt.tight_layout()
    plt.savefig(output_path / 'makespan_comparison.png', dpi=300)
    plt.close()
    
    # 2. Average Task Duration Comparison
    plt.figure(figsize=(10, 6))
    avg_times = [results[s]['avg_time'] for s in strategies]
    bars = plt.bar(strategies, avg_times, color=colors)
    
    plt.title('Average Task Duration by Strategy', fontsize=14)
    plt.ylabel('Time (seconds)', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    
    if avg_times:
        plt.ylim(0, max(avg_times) * 1.15)
        
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}s',
                ha='center', va='bottom')
                
    plt.tight_layout()
    plt.savefig(output_path / 'avg_duration_comparison.png', dpi=300)
    plt.close()
    
    # 3. Speedup relative to Baseline (First strategy)
    baseline_makespan = makespans[0]
    speedups = [baseline_makespan / (m + 1e-6) for m in makespans]
    
    plt.figure(figsize=(10, 6))
    bars = plt.bar(strategies, speedups, color=colors)
    
    plt.title(f'Speedup Factor (relative to {strategies[0]})', fontsize=14)
    plt.ylabel('Speedup Factor (Higher is Better)', fontsize=12)
    plt.axhline(y=1.0, color='k', linestyle='-', alpha=0.3)
    plt.xticks(rotation=45, ha='right')
    
    if speedups:
        plt.ylim(0, max(speedups) * 1.15)
        
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.2f}x',
                ha='center', va='bottom')
                
    plt.tight_layout()
    plt.savefig(output_path / 'speedup_comparison.png', dpi=300)
    plt.close()
    
    logger.info(f"Plots saved to {output_path}")

def plot_workload_characteristics(tasks: List[Dict], output_dir: str = "data/results/plots"):
    """
    Plot characteristics of the generated workload
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Convert to DataFrame if needed
    if not isinstance(tasks, pd.DataFrame):
        if hasattr(tasks[0], 'to_dict'):
            data = [t.to_dict() for t in tasks]
        else:
            data = tasks
        df = pd.DataFrame(data)
    else:
        df = tasks
        
    # 1. Task Size Distribution
    plt.figure(figsize=(10, 6))
    plt.hist(df['size'], bins=50, color='#3498db', edgecolor='black', alpha=0.7)
    plt.title('Task Size Distribution', fontsize=14)
    plt.xlabel('Task Size', fontsize=12)
    plt.ylabel('Count', fontsize=12)
    plt.grid(axis='y', linestyle='--', alpha=0.5)
    plt.savefig(output_path / 'task_size_dist.png', dpi=300)
    plt.close()
    
    # 2. Compute Intensity vs Memory
    plt.figure(figsize=(10, 6))
    plt.scatter(df['memory_required'], df['compute_intensity'], 
               c=df['size'], cmap='viridis', alpha=0.6)
    plt.colorbar(label='Task Size')
    plt.title('Compute Intensity vs Memory Requirement', fontsize=14)
    plt.xlabel('Memory Required (MB)', fontsize=12)
    plt.ylabel('Compute Intensity (0-1)', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.savefig(output_path / 'compute_vs_memory.png', dpi=300)
    plt.close()
    
    # 3. Arrival Rate (Tasks per second)
    plt.figure(figsize=(12, 6))
    # Bin arrival times into 1-second intervals
    max_time = int(df['arrival_time'].max()) + 1
    bins = range(0, max_time + 1)
    plt.hist(df['arrival_time'], bins=bins, color='#2ecc71', alpha=0.7)
    plt.title('Task Arrival Rate Over Time', fontsize=14)
    plt.xlabel('Time (seconds)', fontsize=12)
    plt.ylabel('Tasks per Second', fontsize=12)
    plt.grid(axis='y', linestyle='--', alpha=0.5)
    plt.savefig(output_path / 'arrival_rate.png', dpi=300)
    plt.close()
    
    # 4. Dependency Graph (First 50 tasks)
    # Only if dependencies exist
    if 'dependencies' in df.columns:
        try:
            import networkx as nx
            
            # Parse dependencies if they are strings
            if isinstance(df['dependencies'].iloc[0], str):
                df['dependencies'] = df['dependencies'].apply(eval)
                
            subset = df.head(50)
            G = nx.DiGraph()
            
            for _, task in subset.iterrows():
                G.add_node(task['task_id'], size=task['size'])
                for dep in task['dependencies']:
                    if dep in subset['task_id'].values:
                        G.add_edge(dep, task['task_id'])
            
            plt.figure(figsize=(12, 8))
            pos = nx.spring_layout(G, seed=42)
            nx.draw(G, pos, with_labels=True, node_color='#9b59b6', 
                   node_size=500, font_color='white', font_size=8,
                   edge_color='gray', arrows=True)
            plt.title('Task Dependency Graph (First 50 Tasks)', fontsize=14)
            plt.savefig(output_path / 'dependency_graph.png', dpi=300)
            plt.close()
        except ImportError:
            logger.warning("NetworkX not installed, skipping dependency graph")
        except Exception as e:
            logger.warning(f"Failed to plot dependency graph: {e}")

    logger.info(f"Workload plots saved to {output_path}")

def plot_cost_analysis(results: Dict[str, Dict[str, float]], output_dir: str = "data/results/plots"):
    """
    Plot total cost comparison.
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    strategies = list(results.keys())
    costs = [results[s].get('total_cost', 0) for s in strategies]
    
    plt.figure(figsize=(10, 6))
    colors = plt.cm.Set2(np.linspace(0, 1, len(strategies)))
    bars = plt.bar(strategies, costs, color=colors)
    
    plt.title('Total Operational Cost by Strategy', fontsize=14)
    plt.ylabel('Cost (Abstract Units)', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    
    if costs:
        plt.ylim(0, max(costs) * 1.15)
        
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.2f}',
                ha='center', va='bottom')
                
    plt.tight_layout()
    plt.savefig(output_path / 'cost_comparison.png', dpi=300)
    plt.close()


def plot_latency_distribution(latencies_dict: Dict[str, List[float]], output_dir: str = "data/results/plots"):
    """
    Plot latency distribution (CDF and Boxplot).
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # 1. CDF Plot
    plt.figure(figsize=(10, 6))
    for strategy, latencies in latencies_dict.items():
        sorted_data = np.sort(latencies)
        yvals = np.arange(len(sorted_data)) / float(len(sorted_data) - 1)
        plt.plot(sorted_data, yvals, label=strategy, linewidth=2)
        
    plt.title('Latency CDF (Cumulative Distribution Function)', fontsize=14)
    plt.xlabel('Latency (seconds)', fontsize=12)
    plt.ylabel('Probability', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.legend()
    plt.tight_layout()
    plt.savefig(output_path / 'latency_cdf.png', dpi=300)
    plt.close()
    
    # 2. Box Plot
    plt.figure(figsize=(10, 6))
    data_to_plot = [latencies_dict[s] for s in latencies_dict.keys()]
    labels = list(latencies_dict.keys())
    
    plt.boxplot(data_to_plot, labels=labels, patch_artist=True)
    plt.title('Latency Distribution (Box Plot)', fontsize=14)
    plt.ylabel('Latency (seconds)', fontsize=12)
    plt.grid(axis='y', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(output_path / 'latency_boxplot.png', dpi=300)
    plt.close()



======= FILE: src/simulation_engine.py =======

"""
This is the main engine that runs the simulation.
It generates tasks, runs them through all the schedulers, and sends the results to the dashboard.
"""

import asyncio
import time
import random
import yaml
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List
from loguru import logger
from concurrent.futures import ThreadPoolExecutor

from src.workload_generator import WorkloadGenerator, Task
from src.simulator import VirtualMultiGPU
from src.online_scheduler import OnlineScheduler
from src.offline_trainer import OfflineTrainer
from src.dqn_scheduler import DQNScheduler

class ContinuousSimulation:
    # Controls the whole simulation loop.
    # Generates tasks, runs schedulers, saves data, and retrains the model.
    
    def __init__(self, broadcast_callback):
        """Sets up the simulation with all the schedulers and stuff."""
        self.broadcast_callback = broadcast_callback
        self.is_running = False
        self.is_paused = False
        
        # Load Configuration
        self.config = self._load_config()
        
        # Configuration
        self.num_gpus = self.config['hardware'].get('num_virtual_gpus', 4)
        self.retrain_interval = 50
        self.tasks_processed = 0
        self.history_file = Path("data/long_term_history.csv")
        self.history_file.parent.mkdir(parents=True, exist_ok=True)
        
        # Performance Optimizations
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.batch_buffer = [] # For training data
        self.results_buffer = [] # For full scheduler results
        self.batch_size = 1 # Write to disk every 1 task (Immediate updates)
        
        # Initialize Workload Generator (Persistent)
        self.workload_generator = WorkloadGenerator(seed=self.config['workload_generation'].get('seed', 42))
        
        # Create a persistent task stream so task_ids increment correctly
        # We generate a large number of tasks to keep the simulation running
        self.task_stream = self.workload_generator.generate_workload_stream(
            num_tasks=1000000, 
            arrival_rate=2.0
        )
        
        # Initialize Schedulers / Simulators
        # We need separate simulators for each strategy to maintain independent state
        self.simulators = {
            'round_robin': VirtualMultiGPU(self.num_gpus),
            'random': VirtualMultiGPU(self.num_gpus),
            'greedy': VirtualMultiGPU(self.num_gpus),
            'hybrid_ml': VirtualMultiGPU(self.num_gpus),
            'rl_agent': VirtualMultiGPU(self.num_gpus),
            'oracle': VirtualMultiGPU(self.num_gpus) # Theoretical best
        }
        
        # Initialize Models
        self.trainer = OfflineTrainer(model_type="random_forest", n_estimators=10)
        # Try to load existing model, else create new
        try:
            # We need a dummy model first
            self.trainer.create_model() 
            # In a real scenario we'd load_model, but for now we'll just init a fresh one 
            # or rely on the first retraining to make it good.
            # Let's pre-train it on a tiny bit of dummy data so it's not empty
            self._initial_pretrain()
        except Exception as e:
            logger.warning(f"Could not load initial model: {e}")
            
        self.hybrid_scheduler = OnlineScheduler(self.trainer.model, self.num_gpus)
        
        self.rl_scheduler = DQNScheduler(self.num_gpus)
        # Load RL model if exists? For now start fresh or use epsilon decay
        
        # --- FIX: Pre-train RL Agent to avoid high initial latency ---
        try:
            logger.info("Pre-training RL Agent to fix initial performance...")
            # Generate 1000 tasks for "Heuristic Pre-training"
            # This teaches the agent basic rules (Big Math -> GPU, Tiny script -> CPU)
            pretrain_tasks = self.workload_generator.generate_workload(num_tasks=1000)
            self.rl_scheduler.pretrain(pretrain_tasks, epochs=5)
            logger.info("RL Agent pre-training complete.")
        except Exception as e:
            logger.error(f"Failed to pre-train RL agent: {e}")
        
        # Metrics Storage
        self.metrics = {k: {'total_time': 0.0, 'tasks': 0} for k in self.simulators.keys()}

    def _load_config(self) -> Dict:
        """Loads the config from the yaml file."""
        try:
            with open("config.yaml", "r") as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.warning(f"Failed to load config.yaml, using defaults: {e}")
            return {'hardware': {}, 'workload_generation': {}}

    def _initial_pretrain(self):
        """
        Trains the model on some dummy data just so it's not empty when we start.
        """
        wg = WorkloadGenerator()
        tasks = wg.generate_workload(num_tasks=10)
        self.trainer.prepare_data(wg)
        # Mock optimal fractions for dummy data
        self.trainer.training_data['optimal_gpu_fraction'] = [0.5] * 10
        
        X = self.trainer.training_data[['size', 'compute_intensity', 'memory_required', 'memory_per_size', 'compute_to_memory']]
        y = self.trainer.training_data['optimal_gpu_fraction']
        self.trainer.train(X, y)

    async def start(self):
        """
        Starts the main loop.
        Generates tasks, runs them, saves data, and updates the dashboard.
        """
        self.is_running = True
        logger.info("Simulation Engine Started")
        
        # Initialize CSV if not exists
        if not self.history_file.exists():
            pd.DataFrame(columns=[
                'task_id', 'size', 'compute_intensity', 'memory_required', 
                'optimal_gpu_fraction', 'optimal_time'
            ]).to_csv(self.history_file, index=False)
            
        loop = asyncio.get_running_loop()

        while self.is_running:
            if self.is_paused:
                await asyncio.sleep(0.5)
                continue
                
            # 1. Generate Task
            task = self._generate_task()
            
            # 2. Run on All Schedulers (Offload to ThreadPool)
            # This prevents blocking the async loop during heavy computation
            results = await loop.run_in_executor(self.executor, self._run_all_schedulers, task)
            
            # 3. Update Metrics & Broadcast
            self._update_metrics(results)
            await self._broadcast_state(task, results)
            
            # 4. Persist Data (Oracle Result) - Buffered
            self._persist_data(task, results['oracle'], results)
            
            # 5. Retrain if needed
            self.tasks_processed += 1
            if self.tasks_processed % self.retrain_interval == 0:
                # Flush buffer before retraining to ensure latest data is available
                self._flush_batch()
                await self._retrain_model()
            
            # Delay for visual pacing
            await asyncio.sleep(1.5) # Slow enough to read

    def stop(self):
        """Stop the simulation loop gracefully."""
        self.is_running = False
        self._flush_batch() # Ensure pending data is saved
        self.executor.shutdown(wait=False)

    def pause(self):
        """Pause the simulation temporarily without stopping it."""
        self.is_paused = True

    def resume(self):
        """Resume a paused simulation."""
        self.is_paused = False

    def _generate_task(self) -> Task:
        """Gets the next task from our generator."""
        try:
            # Get next task from the persistent stream
            return next(self.task_stream)
        except StopIteration:
            # If we run out of tasks, restart the stream
            logger.info("Task stream exhausted, restarting...")
            self.task_stream = self.workload_generator.generate_workload_stream(
                num_tasks=1000000, 
                arrival_rate=2.0
            )
            return next(self.task_stream)

    def _calculate_metrics(self, result: Dict) -> Dict:
        """
        Calculates energy and cost based on how long the task took and how much GPU it used.
        """
        time = result['actual_time']
        gpu_frac = result['gpu_fraction']
        
        # Power Model: GPU=50W, CPU=30W
        power = (gpu_frac * 50.0) + ((1.0 - gpu_frac) * 30.0)
        energy_joules = power * time
        
        # Cost Model: $0.15 per kWh
        kwh = energy_joules / 3_600_000
        cost = kwh * 0.15
        
        return {
            'time': time,
            'energy': energy_joules,
            'cost': cost
        }

    def _run_all_schedulers(self, task: Task) -> Dict:
        """
        Runs the task on all the different schedulers (Round Robin, Random, ML, etc.)
        so we can compare them.
        """
        results = {}
        
        # --- 1. Round Robin ---
        rr_gpu_frac = 0.5 if task.task_id % 2 == 0 else 0.0
        res = self.simulators['round_robin'].simulate_task_execution(task, rr_gpu_frac)
        results['round_robin'] = {**res, **self._calculate_metrics(res)}
        
        # --- 2. Random ---
        rand_gpu_frac = random.random()
        res = self.simulators['random'].simulate_task_execution(task, rand_gpu_frac)
        results['random'] = {**res, **self._calculate_metrics(res)}
        
        # --- 3. Greedy ---
        greedy_frac = task.compute_intensity
        res = self.simulators['greedy'].simulate_task_execution(task, greedy_frac)
        results['greedy'] = {**res, **self._calculate_metrics(res)}
        
        # --- 4. Hybrid ML ---
        features = pd.DataFrame([{
            'size': task.size,
            'compute_intensity': task.compute_intensity,
            'memory_required': task.memory_required,
            'memory_per_size': task.memory_required / (task.size + 1),
            'compute_to_memory': task.compute_intensity / (task.memory_required + 1),
        }])
        ml_frac = self.hybrid_scheduler.model.predict(features)[0]
        ml_frac = max(0.0, min(1.0, ml_frac))
        res = self.simulators['hybrid_ml'].simulate_task_execution(task, ml_frac)
        results['hybrid_ml'] = {**res, **self._calculate_metrics(res)}
        
        # --- 5. RL Agent ---
        # Get action from RL Agent (Exploration vs Exploitation handled inside)
        rl_action_dict = self.rl_scheduler.get_action(task)
        rl_frac = rl_action_dict['gpu_fraction']
        
        res = self.simulators['rl_agent'].simulate_task_execution(task, rl_frac)
        
        # Calculate Reward based on actual execution
        # We need the metrics to calculate reward (negative cost)
        metrics = self._calculate_metrics(res)
        
        # Feedback to RL Agent (Observe and Learn)
        # We pass the reward back 
        # Reward = -(w*Time + (1-w)*Energy) (or similar logic inside scheduler)
        self.rl_scheduler.observe(
            task=task,
            action=rl_action_dict['action'],
            reward_metrics=metrics # Scheduler will calculate scalar reward
        )
        
        results['rl_agent'] = {**res, **metrics}
        
        # --- 6. Oracle (Brute Force) ---
        best_time = float('inf')
        best_frac = 0.0
        best_res = None
        for frac in np.linspace(0, 1, 11):
            r = self.simulators['oracle'].simulate_task_execution(task, frac)
            if r['actual_time'] < best_time:
                best_time = r['actual_time']
                best_frac = frac
                best_res = r
        
        results['oracle'] = {**best_res, **self._calculate_metrics(best_res)}
        
        return results

    def _update_metrics(self, results):
        """Updates the total time and task count for each scheduler."""
        for name, res in results.items():
            self.metrics[name]['total_time'] += res['time']
            # We can track total energy/cost too if needed
            self.metrics[name]['tasks'] += 1

    def _persist_data(self, task: Task, oracle_result: Dict, all_results: Dict = None):
        """
        Saves the task data and the best result (Oracle) to our buffer.
        We write to disk/DB in batches.
        """
        row = {
            'task_id': task.task_id,
            'size': task.size,
            'compute_intensity': task.compute_intensity,
            'memory_required': task.memory_required,
            'optimal_gpu_fraction': oracle_result['gpu_fraction'],
            'optimal_time': oracle_result['actual_time']
        }
        self.batch_buffer.append(row)
        
        if all_results:
            self.results_buffer.append((task, all_results))
        
        if len(self.batch_buffer) >= self.batch_size:
            self._flush_batch()

    def _flush_batch(self):
        """Writes the buffered data to the DB and CSV."""
        if not self.batch_buffer:
            return
            
        buffer_copy = self.batch_buffer.copy()
        self.batch_buffer = []
        
        # 1. Save to database (async, non-blocking)
        try:
            from backend.services import SimulationDataService
            # Create async task to save to database
            asyncio.create_task(
                SimulationDataService.save_training_data_batch(buffer_copy)
            )
            
            # Also save full scheduler results if we have them
            if self.results_buffer:
                results_copy = self.results_buffer.copy()
                self.results_buffer = []
                asyncio.create_task(
                    SimulationDataService.save_scheduler_results_batch(results_copy)
                )
                
            logger.debug(f"Queued {len(buffer_copy)} records for database save")
        except Exception as e:
            logger.error(f"Failed to queue database save: {e}")
        
        # 2. Save to CSV as backup
        try:
            df = pd.DataFrame(buffer_copy)
            df.to_csv(self.history_file, mode='a', header=False, index=False)
            logger.debug(f"Flushed {len(buffer_copy)} records to CSV backup")
        except Exception as e:
            logger.error(f"Failed to flush CSV batch: {e}")


    async def _retrain_model(self):
        """
        Retrains the ML model using the latest data.
        Uses a sliding window so it doesn't get too slow.
        """
        logger.info("Retraining Hybrid ML Model...")
        try:
            df = None
            
            # Try database first
            try:
                from backend.services import SimulationDataService
                data = await SimulationDataService.get_latest_training_data(limit=1000)
                if data:
                    df = pd.DataFrame(data)
                    logger.debug(f"Loaded {len(df)} records from database for retraining")
            except Exception as e:
                logger.warning(f"Failed to load from database: {e}, falling back to CSV")
            
            # Fallback to CSV if database fails or is empty  
            if df is None or len(df) == 0:
                if self.history_file.exists():
                    df = pd.read_csv(self.history_file)
                    logger.debug(f"Loaded {len(df)} records from CSV for retraining")
                    
                    # Keep only last 1000 samples
                    if len(df) > 1000:
                        df = df.tail(1000)
                else:
                    logger.warning("No training data available (neither database nor CSV)")
                    return
            
            if len(df) < 50:
                logger.info(f"Not enough data for retraining ({len(df)} samples, need 50)")
                return
            
            # Prepare features (if not already computed from database)
            if 'memory_per_size' not in df.columns:
                df['memory_per_size'] = df['memory_required'] / (df['size'] + 1)
            if 'compute_to_memory' not in df.columns:
                df['compute_to_memory'] = df['compute_intensity'] / (df['memory_required'] + 1)
            
            X = df[['size', 'compute_intensity', 'memory_required', 'memory_per_size', 'compute_to_memory']]
            y = df['optimal_gpu_fraction']
            
            # Train (in executor to avoid blocking)
            loop = asyncio.get_running_loop()
            await loop.run_in_executor(self.executor, self.trainer.train, X, y)
            
            # Update scheduler's model reference
            self.hybrid_scheduler.model = self.trainer.model
            
            logger.info(f"Retraining Complete. Samples: {len(df)}")
            
            # Notify frontend
            await self.broadcast_callback({
                'type': 'notification',
                'message': f'Hybrid Model Retrained! (Samples: {len(df)})'
            })
            
        except Exception as e:
            logger.error(f"Retraining failed: {e}")

    async def _broadcast_state(self, task: Task, results: Dict):
        """Sends the current state to the frontend via WebSocket."""
        # Calculate averages for chart
        comparison = []
        for name, metrics in self.metrics.items():
            avg_time = metrics['total_time'] / max(1, metrics['tasks'])
            comparison.append({'name': name, 'avg_time': avg_time})
            
        # Current Task Info
        task_info = {
            'id': task.task_id,
            'size': task.size,
            'intensity': task.compute_intensity,
            'memory': task.memory_required
        }
        
        # Hybrid ML Decision (for the visualizer)
        hybrid_res = results['hybrid_ml']
        
        # Construct message
        msg = {
            'type': 'simulation_update',
            'task': task_info,
            'comparison': comparison,
            'latest_results': results, # Now contains time, energy, cost
            'utilization': {
                'average_utilization': random.uniform(0.4, 0.9),
                'gpu_0': {'utilization': random.uniform(0, 1)},
                'gpu_1': {'utilization': random.uniform(0, 1)},
                'gpu_2': {'utilization': random.uniform(0, 1)},
                'gpu_3': {'utilization': random.uniform(0, 1)},
            },
            'data': {
                'task_id': task.task_id,
                'gpu_id': random.randint(0, 3),
                'gpu_fraction': hybrid_res['gpu_fraction'],
                'scheduled_time': time.time()
            }
        }
        
        await self.broadcast_callback(msg)


======= FILE: src/pipeline.py =======

"""
Pipeline Execution Phases
Extracted from main.py for modularity.
"""

from pathlib import Path
from loguru import logger
import pandas as pd
import time
import json

from src.profiler import HardwareProfiler
from src.workload_generator import WorkloadGenerator
from src.offline_trainer import OfflineTrainer
from src.online_scheduler import OnlineScheduler
from src.dqn_scheduler import DQNScheduler
from src.simulator import VirtualMultiGPU
from src.visualization import plot_comparison, plot_workload_characteristics


def run_profiling_phase(config: dict):
    """Phase 1: Hardware Profiling"""
    logger.info("\n" + "="*80)
    logger.info("PHASE 1: HARDWARE PROFILING")
    logger.info("="*80)
    
    profiler = HardwareProfiler(device_type=config['hardware']['device'])
    profile = profiler.profile_range(sizes=config['profiling']['matrix_sizes'])
    
    if config['profiling']['save_profiles']:
        profiler.save_profile(config['profiling']['profile_output'])
    
    return profiler, profile


def run_data_generation_phase(config: dict):
    """Phase 2: Generate Workload Data"""
    logger.info("\n" + "="*80)
    logger.info("PHASE 2: WORKLOAD DATA GENERATION")
    logger.info("="*80)
    
    wg = WorkloadGenerator(seed=config['workload_generation']['seed'])
    tasks = wg.generate_workload(
        num_tasks=config['workload_generation']['num_tasks'],
        task_size_range=tuple(config['workload_generation']['task_size_range']),
        compute_intensity_range=tuple(config['workload_generation']['compute_intensity_range']),
        memory_range=tuple(config['workload_generation']['memory_range']),
        arrival_rate=config['workload_generation']['arrival_rate']
    )
    
    # Save workload
    output_dir = Path("data/workload_traces")
    output_dir.mkdir(parents=True, exist_ok=True)
    wg.save_workload(str(output_dir / "workload_train.csv"))
    
    stats = wg.get_statistics()
    logger.info(f"Workload stats: {stats}")
    
    # Visualize workload
    plot_workload_characteristics(tasks, output_dir=config['output']['plots_dir'])
    
    return wg, tasks


def run_offline_training_phase(config: dict, wg: WorkloadGenerator):
    """Phase 3: Offline ML Model Training"""
    logger.info("\n" + "="*80)
    logger.info("PHASE 3: OFFLINE ML MODEL TRAINING")
    logger.info("="*80)
    
    trainer = OfflineTrainer(
        model_type=config['ml_models']['model_type'],
        **config['ml_models'][config['ml_models']['model_type']]
    )
    
    # Run training pipeline
    model_output = Path("models")
    model_output.mkdir(parents=True, exist_ok=True)
    
    results = trainer.run_full_pipeline(
        wg,
        model_output_path=str(model_output / "scheduler_model.pkl")
    )
    
    logger.info(f"Training complete. Model saved.")
    logger.info(f"Feature importances: {results['feature_importances']}")
    
    return trainer, results


def run_online_scheduling_phase(config: dict, trainer: OfflineTrainer):
    """Phase 4: Online Scheduling Simulation"""
    logger.info("\n" + "="*80)
    logger.info("PHASE 4: ONLINE SCHEDULING SIMULATION")
    logger.info("="*80)
    
    # Kafka Setup
    producer = None
    try:
        from kafka import KafkaProducer
        
        producer = KafkaProducer(
            bootstrap_servers=['localhost:9092'],
            value_serializer=lambda x: json.dumps(x).encode('utf-8')
        )
        logger.info("Kafka Producer initialized successfully")
    except Exception as e:
        logger.warning(f"Failed to initialize Kafka Producer: {e}")

    def kafka_callback(data):
        if producer:
            try:
                # Add timestamp if missing
                if 'timestamp' not in data:
                    data['timestamp'] = time.time()
                
                producer.send('scheduler_events', value=data)
                # Small delay to simulate real-time execution for visualization
                time.sleep(0.1) 
            except Exception as e:
                logger.error(f"Failed to send to Kafka: {e}")

    # Create scheduler with trained model
    scheduler = OnlineScheduler(
        model=trainer.model,
        num_gpus=config['hardware']['num_virtual_gpus'],
        monitor_callback=kafka_callback
    )
    
    # Generate test workload
    test_wg = WorkloadGenerator(seed=42)
    test_tasks = test_wg.generate_workload(
        num_tasks=config['workload_generation']['simulation_tasks'],
        arrival_rate=config['workload_generation']['arrival_rate']
    )
    
    # Submit and schedule tasks
    for task in test_tasks:
        scheduler.submit_task(task)
    
    decisions = scheduler.process_queue()
    
    if producer:
        producer.flush()
        producer.close()
    
    logger.info(f"Scheduled {len(decisions)} tasks")
    logger.info(f"Utilization: {scheduler.get_utilization()}")
    
    return scheduler, decisions


def run_rl_training_phase(config: dict, trainer: OfflineTrainer = None):
    """Phase 4.5: RL Scheduler Training"""
    logger.info("\n" + "="*80)
    logger.info("PHASE 4.5: RL SCHEDULER TRAINING")
    logger.info("="*80)
    
    # Initialize RL Scheduler
    rl_scheduler = DQNScheduler(
        num_gpus=config['hardware']['num_virtual_gpus'],
        energy_weight=0.0 # Pure Performance
    )
    
    # Generate Imitation Learning Data (Teacher Student)
    logger.info("Generating pre-training data (Imitation Learning)...")
    pretrain_wg = WorkloadGenerator(seed=42)
    pretrain_tasks = pretrain_wg.generate_workload(
        num_tasks=10000,
        arrival_rate=config['workload_generation']['arrival_rate'],
        task_size_range=tuple(config['workload_generation']['task_size_range']),
        compute_intensity_range=tuple(config['workload_generation']['compute_intensity_range']),
        memory_range=tuple(config['workload_generation']['memory_range'])
    )
    
    # Teacher Labeling
    if trainer and hasattr(trainer, 'model'):
        logger.info("Teacher (Hybrid ML) found! Generating optimal labels...")
        count = 0
        for task in pretrain_tasks:
            # 1. Teacher Prediction
            features = pd.DataFrame([{
                'size': task.size,
                'compute_intensity': task.compute_intensity,
                'memory_required': task.memory_required,
                'memory_per_size': task.memory_required / (task.size + 1),
                'compute_to_memory': task.compute_intensity / (task.memory_required + 1),
            }])
            teacher_fraction = trainer.model.predict(features)[0]
            teacher_fraction = max(0.0, min(1.0, teacher_fraction))
            
            # 2. Map to Action Bin
            action_bins = rl_scheduler.action_bins
            action = min(range(len(action_bins)), key=lambda i: abs(action_bins[i]-teacher_fraction))
            
            # 3. Push to Buffer
            state = rl_scheduler._get_state_vector(task)
            reward = 1.0 # Strong positive reinforcement for copying teacher
            next_state = list(state) # Dummy
            done = True
            
            rl_scheduler.memory.push(state, action, reward, next_state, done)
            count += 1
        logger.info(f"Filled buffer with {count} teacher demonstrations.")
        
        # Train on this buffer
        rl_scheduler.pretrain([], epochs=100)
        
    else:
        logger.info("No Teacher found. Using Heuristic.")
        rl_scheduler.pretrain(pretrain_tasks, epochs=100)
    
    
    # Generate training workload for RL (Online)
    # Needs many tasks to learn
    train_wg = WorkloadGenerator(seed=101)
    train_tasks = train_wg.generate_workload(
        num_tasks=config['workload_generation']['rl_training_tasks'],
        arrival_rate=config['workload_generation']['arrival_rate'],
        task_size_range=tuple(config['workload_generation']['task_size_range']),
        compute_intensity_range=tuple(config['workload_generation']['compute_intensity_range']),
        memory_range=tuple(config['workload_generation']['memory_range'])
    )
    
    # Lower epsilon to preserve pre-trained knowledge
    rl_scheduler.epsilon = 0.1
    logger.info(f"Epsilon set to {rl_scheduler.epsilon} for fine-tuning")

    # Train (Online Learning)
    logger.info("Training RL agent (Online)...")
    train_simulator = VirtualMultiGPU(num_gpus=config['hardware']['num_virtual_gpus'])
    
    for task in train_tasks:
        rl_scheduler.randomize_resources() # Noise
        
        # 1. Get Action
        decision = rl_scheduler.get_action(task)
        action = decision['action']
        gpu_fraction = decision['gpu_fraction']
        
        # 2. Simulate to get Feedback
        result = train_simulator.simulate_task_execution(task, gpu_fraction)
        
        # 3. Observe and Learn
        reward_metrics = {
            'time': result['actual_time'],
            'energy': result['energy']
        }
        rl_scheduler.observe(task, action, reward_metrics)
        
    logger.info(f"RL Training complete. Final Epsilon: {rl_scheduler.epsilon:.4f}")
    
    # Save model
    model_output = Path(config['output']['model_dir'])
    rl_scheduler.save_model(str(model_output / "rl_dqn_model.pth"))
    
    return rl_scheduler


def run_evaluation_phase(config: dict, scheduler, decisions, rl_scheduler=None):
    """Phase 5: Evaluation & Analysis"""
    logger.info("\n" + "="*80)
    logger.info("PHASE 5: EVALUATION & ANALYSIS")
    logger.info("="*80)
    
    # Create simulator for baseline comparison
    simulator = VirtualMultiGPU(num_gpus=config['hardware']['num_virtual_gpus'])
    
    # Generate test workload for baselines
    # Generate test workload for baselines (Distribution Shift: CPU favored)
    test_wg = WorkloadGenerator(seed=99)
    test_tasks = test_wg.generate_workload(
        num_tasks=config['workload_generation']['evaluation_tasks'],
        task_size_range=(10, 100), # Small tasks (CPU favored due to transfer overhead)
        compute_intensity_range=(0.0, 0.1), # Low intensity
        memory_range=(2000, 5000) # High memory
    )
    
    # Evaluate baselines
    baselines = simulator.evaluate_baseline_schedulers(test_tasks)
    
    # Evaluate our scheduler
    our_times = []
    for i, task in enumerate(test_tasks):
        # Re-predict using our model for fair comparison on same workload
        
        # 1. Predict placement
        features = pd.DataFrame([{
            'size': task.size,
            'compute_intensity': task.compute_intensity,
            'memory_required': task.memory_required,
            'memory_per_size': task.memory_required / (task.size + 1),
            'compute_to_memory': task.compute_intensity / (task.memory_required + 1),
        }])
        gpu_fraction = scheduler.model.predict(features)[0]
        gpu_fraction = max(0.0, min(1.0, gpu_fraction))
        
        # 2. Simulate execution
        result = simulator.simulate_task_execution(task, gpu_fraction)
        our_times.append(result['actual_time'])
        
    baselines['hybrid_ml'] = {
        'makespan': sum(our_times),
        'avg_time': sum(our_times) / len(our_times),
        'max_time': max(our_times),
    }

    # Evaluate RL Scheduler
    if rl_scheduler:
        rl_times = []
        # Disable exploration for evaluation (Greedy Policy)
        rl_scheduler.epsilon = 0.0
        
        for task in test_tasks:
            pass 
            
        # Re-run properly
        for task in test_tasks:
            rl_scheduler.reset_state()
            decision = rl_scheduler.schedule_task(task)
            rl_times.append(decision['estimated_time'])
            
        baselines['rl_agent'] = {
            'makespan': sum(rl_times),
            'avg_time': sum(rl_times) / len(rl_times),
            'max_time': max(rl_times),
        }

    logger.info("Baseline Performance:")
    for strategy, metrics in baselines.items():
        logger.info(f"  {strategy}: {metrics}")
    
    # Generate Plots
    plot_comparison(baselines, output_dir=config['output']['plots_dir'])
    
    # Save results
    results_dir = Path(config['output']['results_dir'])
    results_dir.mkdir(parents=True, exist_ok=True)
    
    results_df = pd.DataFrame(decisions)
    results_df.to_csv(str(results_dir / "scheduling_results.csv"), index=False)
    
    logger.info(f"Results saved to {results_dir}")
    
    return baselines, results_df


======= FILE: src/simulator.py =======

"""
Virtual Multi-GPU Simulator for baseline comparisons
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple
from loguru import logger
import time

from .workload_generator import Task, WorkloadGenerator


class VirtualMultiGPU:
    """Simulates multiple GPUs"""
    
    def __init__(self, num_gpus: int = 4, memory_per_gpu: int = 8000):
        self.num_gpus = num_gpus
        self.memory_per_gpu = memory_per_gpu
        self.task_timeline = []
        self.execution_log = []
        
        logger.info(f"VirtualMultiGPU initialized with {num_gpus} GPUs")
    
    def simulate_task_execution(self, 
                               task: Task,
                               gpu_fraction: float,
                               duration_multiplier: float = 1.0) -> Dict:
        """
        Simulate the execution of a single task on the virtual hardware.
        
        This method models:
        1. Speedup provided by GPU based on task compute intensity.
        2. Data transfer overhead (PCIE bandwidth) for GPU usage.
        3. Energy consumption based on power profiles of CPU vs GPU.
        4. Financial cost based on cloud pricing models (e.g., AWS spot rates).
        
        Args:
            task: The task object to execute.
            gpu_fraction: Fraction of the task offloaded to GPU (0.0=CPU only, 1.0=GPU only).
            duration_multiplier: Optional scaler for execution time (e.g., to simulate interference).
            
        Returns:
            Dict containing execution metrics (time, energy, cost).
        """
        # 1. Simulate GPU speedup based on compute intensity
        # Tasks with higher compute intensity benefit more from GPU parallelism.
        cpu_time = task.duration_estimate
        
        # Max speedup is 4x for perfectly parallel tasks (intensity=1.0)
        speedup = 1.0 + (3.0 * task.compute_intensity)
        gpu_execution_time = cpu_time / speedup
        
        # 2. Data Transfer Overhead 
        # Simulating PCIe Bandwidth ~16GB/s.
        # This penalizes small, memory-heavy tasks on GPU.
        transfer_time = 0.0
        if gpu_fraction > 0:
            transfer_time = task.memory_required / 16000.0
            
        # 3. Calculate Combined Execution Time
        # The task is split: `gpu_fraction` runs on GPU, rest on CPU.
        gpu_part = (gpu_execution_time + transfer_time) * gpu_fraction
        cpu_part = cpu_time * (1.0 - gpu_fraction)
        
        total_time = (gpu_part + cpu_part) * duration_multiplier
        
        # 4. Calculate Energy (Joules)
        # Power Model: GPU ~200W, CPU ~100W under load.
        # This linear interpolation estimates system power.
        power_rate = gpu_fraction * 200.0 + (1.0 - gpu_fraction) * 100.0
        energy = power_rate * total_time
        
        # 5. Calculate Financial Cost ($)
        # Pricing Model (approx AWS): GPU instance ~$0.36/hr ($0.0001/s), CPU ~$0.07/hr ($0.00002/s).
        # Note: GPU is ~5x more expensive than CPU per second.
        cost_rate = gpu_fraction * 0.0001 + (1.0 - gpu_fraction) * 0.00002
        cost = cost_rate * total_time
        
        result = {
            'task_id': task.task_id,
            'gpu_fraction': gpu_fraction,
            'predicted_time': task.duration_estimate,
            'actual_time': total_time,
            'speedup': task.duration_estimate / (total_time + 1e-9),
            'energy': energy,
            'cost': cost
        }
        
        self.execution_log.append(result)
        return result
    
    def simulate_workload(self, workload: List[Task], strategy: str) -> List[Dict]:
        """Simulate execution of a full workload with a specific strategy"""
        results = []
        for i, task in enumerate(workload):
            gpu_frac = 0.0
            
            if strategy == "round_robin":
                # Strict Alternation: 50% GPU, 30% GPU (Just a simple heuristic pattern)
                gpu_frac = 0.5 if i % 2 == 0 else 0.3
                    
            elif strategy == "random":
                # Uniform Random: Assign anywhere between 20% and 80% to GPU
                gpu_frac = np.random.uniform(0.2, 0.8)
                
            elif strategy == "greedy":
                # Heuristic: Assign to GPU proportional to compute intensity.
                # Compute intensive tasks get more GPU.
                gpu_frac = task.compute_intensity
                
            elif strategy == "cpu_only":
                # Baseline 1: Pure CPU Execution
                gpu_frac = 0.0
                
            elif strategy == "gpu_only":
                # Baseline 2: Pure GPU Execution
                gpu_frac = 1.0
                
            elif strategy == "oracle":
                 # Theoretical Best: Search for optimal fraction by "simulating" all options
                best_time = float('inf')
                best_frac = 0.0
                for frac in np.linspace(0, 1, 11): 
                    t_res = self.simulate_task_execution(task, frac)
                    if t_res['actual_time'] < best_time:
                        best_time = t_res['actual_time']
                        best_frac = frac
                gpu_frac = best_frac
            
            # Execute
            res = self.simulate_task_execution(task, gpu_frac)
            results.append(res)
            
        return results

    def evaluate_baseline_schedulers(self,
                                     workload: List[Task]) -> Dict:
        """Evaluate baseline scheduling strategies (Legacy wrapper)"""
        baselines = {}
        
        for strategy in ['round_robin', 'random', 'greedy']:
            results = self.simulate_workload(workload, strategy)
            times = [r['actual_time'] for r in results]
            baselines[strategy] = {
                'makespan': np.sum(times),
                'avg_time': np.mean(times),
                'max_time': np.max(times),
            }
        
        logger.info(f"Baseline evaluation complete")
        return baselines


======= FILE: src/dashboard_server_v2.py =======

"""
Modernized Dashboard Server with Modular API Architecture.

This is the new main entry point that uses the refactored API routes.
"""
import asyncio
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from loguru import logger

from backend.core.config import settings
from backend.core.database import init_db, close_db
from backend.core.redis import redis_client
from backend.api.routes import health, websocket, simulation as simulation_routes, metrics, observability
from backend.__version__ import __version__
from backend.middleware.observability import ObservabilityMiddleware, ErrorTrackingMiddleware
from backend.middleware.rate_limit import RateLimitMiddleware
from backend.middleware.security import SecurityHeadersMiddleware
from backend.services.logging_service import setup_logging
from src.simulation_engine import ContinuousSimulation


# Global simulation engine instance
simulation_engine_instance = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan manager."""
    # Startup
    logger.info("ğŸš€ Starting Hybrid ML Scheduler API...")
    
    # Setup structured logging
    setup_logging(environment=settings.environment)
    logger.info(f"âœ… Logging configured for {settings.environment} environment")
    
    # Initialize database
    try:
        await init_db()
        logger.info("âœ… Database initialized")
    except Exception as e:
        logger.error(f"âŒ Database initialization failed: {e}")
    
    # Connect to Redis
    try:
        await redis_client.connect()
        logger.info("âœ… Redis connected")
    except Exception as e:
        logger.warning(f"âš ï¸  Redis connection failed (degraded mode): {e}")
    
    # Initialize simulation engine
    global simulation_engine_instance
    sim_engine = ContinuousSimulation(broadcast_callback=websocket.manager.broadcast)
    
    # Set engine in the router
    simulation_routes.set_simulation_engine(sim_engine)
    
    simulation_engine_instance = sim_engine
    logger.info("âœ… Simulation engine initialized")
    
    # Start simulation by default
    asyncio.create_task(sim_engine.start())
    logger.info("âœ… Simulation started")
    
    yield
    
    # Shutdown
    logger.info("ğŸ›‘ Shutting down...")
    
    if simulation_engine_instance:
        simulation_engine_instance.stop()
        logger.info("âœ… Simulation stopped")
    
    await close_db()
    await redis_client.disconnect()
    logger.info("âœ… Cleanup complete")


# Create FastAPI app
app = FastAPI(
    title="Hybrid ML Scheduler API",
    description="Real-time GPU scheduling simulation with ML optimization",
    version=__version__,
    lifespan=lifespan
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://127.0.0.1:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Security middleware (applied first)
app.add_middleware(SecurityHeadersMiddleware)
app.add_middleware(RateLimitMiddleware, requests_per_minute=100)

# Observability middleware
app.add_middleware(ObservabilityMiddleware)
app.add_middleware(ErrorTrackingMiddleware)

# Include routers
app.include_router(health.router)
app.include_router(websocket.router)
app.include_router(simulation_routes.router)
app.include_router(metrics.router)
app.include_router(observability.router)


@app.get("/")
async def root():
    """Root endpoint."""
    return {
        "message": "Hybrid ML Scheduler API",
        "version": __version__,
        "docs": "/docs",
        "health": "/health"
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "dashboard_server_v2:app",
        host="0.0.0.0",
        port=8000,
        reload=settings.debug,
        log_level="info"
    )


======= FILE: src/profiler.py =======

"""
Hardware Performance Profiler for M4 Max
Measures CPU vs GPU performance characteristics
"""

import time
import numpy as np
import torch
import json
from pathlib import Path
from typing import Dict, List, Tuple
from loguru import logger


class HardwareProfiler:
    """Profiles CPU and GPU performance"""
    
    def __init__(self, device_type: str = "mps"):
        self.device_type = device_type
        self.device = torch.device(device_type if torch.cuda.is_available() or torch.backends.mps.is_available() else "cpu")
        self.cpu_times = []
        self.gpu_times = []
        self.problem_sizes = []
        self.cpu_gpu_ratio = None
        
        logger.info(f"Hardware Profiler initialized on device: {self.device}")
    
    def benchmark_cpu(self, matrix_size: int, iterations: int = 3) -> float:
        """
        Benchmark CPU matrix multiplication
        
        Args:
            matrix_size: Size of square matrix (matrix_size x matrix_size)
            iterations: Number of iterations for averaging
            
        Returns:
            Average execution time in seconds
        """
        times = []
        
        for _ in range(iterations):
            A = np.random.randn(matrix_size, matrix_size).astype(np.float32)
            B = np.random.randn(matrix_size, matrix_size).astype(np.float32)
            
            start = time.perf_counter()
            C = np.matmul(A, B)
            end = time.perf_counter()
            
            times.append(end - start)
        
        avg_time = np.mean(times)
        logger.debug(f"CPU benchmark {matrix_size}x{matrix_size}: {avg_time:.4f}s")
        return avg_time
    
    def benchmark_gpu(self, matrix_size: int, iterations: int = 3) -> float:
        """
        Benchmark GPU matrix multiplication using MPS
        
        Args:
            matrix_size: Size of square matrix
            iterations: Number of iterations
            
        Returns:
            Average execution time or None if GPU unavailable
        """
        if self.device.type == "cpu":
            logger.warning("GPU not available, returning None")
            return None
        
        times = []
        
        try:
            for _ in range(iterations):
                A = torch.randn(matrix_size, matrix_size, device=self.device, dtype=torch.float32)
                B = torch.randn(matrix_size, matrix_size, device=self.device, dtype=torch.float32)
                
                # Synchronize before timing
                if self.device.type == "mps":
                    torch.mps.synchronize()
                else:
                    torch.cuda.synchronize()
                
                start = time.perf_counter()
                C = torch.matmul(A, B)
                
                # Synchronize after computation
                if self.device.type == "mps":
                    torch.mps.synchronize()
                else:
                    torch.cuda.synchronize()
                end = time.perf_counter()
                
                times.append(end - start)
        
        except Exception as e:
            logger.error(f"GPU benchmarking failed: {e}")
            return None
        
        avg_time = np.mean(times)
        logger.debug(f"GPU benchmark {matrix_size}x{matrix_size}: {avg_time:.4f}s")
        return avg_time
    
    def profile_range(self, sizes: List[int] = None) -> Dict:
        """
        Profile hardware across multiple problem sizes
        
        Args:
            sizes: List of matrix sizes to test
            
        Returns:
            Performance profile dictionary
        """
        if sizes is None:
            sizes = [256, 512, 1024, 2048]
        
        logger.info(f"Starting hardware profiling with sizes: {sizes}")
        
        for size in sizes:
            logger.info(f"Profiling size {size}x{size}...")
            
            cpu_time = self.benchmark_cpu(size)
            gpu_time = self.benchmark_gpu(size)
            
            self.problem_sizes.append(size)
            self.cpu_times.append(cpu_time)
            self.gpu_times.append(gpu_time if gpu_time else cpu_time)
        
        return self.get_performance_model()
    
    def get_performance_model(self) -> Dict:
        """
        Generate performance model from profiling data
        
        Returns:
            Performance model with key metrics
        """
        if not self.cpu_times or not self.gpu_times:
            logger.warning("No profiling data available")
            return {'cpu_only': True}
        
        # Filter out None GPU times
        valid_ratios = [
            c/g for c, g in zip(self.cpu_times, self.gpu_times) 
            if g is not None and g > 0
        ]
        
        if not valid_ratios:
            logger.warning("No valid GPU benchmarks")
            return {'cpu_only': True}
        
        # Calculate average ratio
        self.cpu_gpu_ratio = np.mean(valid_ratios)
        
        model = {
            'cpu_only': False,
            'cpu_gpu_ratio': float(self.cpu_gpu_ratio),
            'gpu_fraction': float(self.cpu_gpu_ratio / (self.cpu_gpu_ratio + 1)),
            'cpu_fraction': float(1 / (self.cpu_gpu_ratio + 1)),
            'profiled_sizes': self.problem_sizes,
            'cpu_times': [float(t) for t in self.cpu_times],
            'gpu_times': [float(t) if t else None for t in self.gpu_times]
        }
        
        logger.info(f"Performance Model: CPU/GPU Ratio = {self.cpu_gpu_ratio:.2f}")
        logger.info(f"GPU should receive {model['gpu_fraction']:.1%} of work")
        
        return model
    
    def save_profile(self, filepath: str):
        """Save performance profile to JSON"""
        profile = self.get_performance_model()
        
        Path(filepath).parent.mkdir(parents=True, exist_ok=True)
        
        with open(filepath, 'w') as f:
            json.dump(profile, f, indent=2)
        
        logger.info(f"Profile saved to {filepath}")
    
    @staticmethod
    def load_profile(filepath: str) -> Dict:
        """Load performance profile from JSON"""
        with open(filepath, 'r') as f:
            profile = json.load(f)
        
        logger.info(f"Profile loaded from {filepath}")
    @staticmethod
    def estimate_energy(time_seconds: float, is_gpu: bool) -> float:
        """
        Estimate energy consumption in Joules
        
        Simple Power Model for M4 Max:
        - GPU Power: ~50W under load
        - CPU Power: ~30W under load
        - Idle/Overhead: ~5W
        """
        power_watts = 50.0 if is_gpu else 30.0
        return power_watts * time_seconds


======= FILE: src/ml_models.py =======

"""
Machine Learning Models for Performance Prediction
Uses Random Forest and XGBoost for predicting execution times
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import joblib
from pathlib import Path
from typing import Tuple, Dict
from loguru import logger


class PerformancePredictor:
    """Base class for performance prediction models"""

    def __init__(self, model_type: str = "random_forest"):
        self.model_type = model_type
        self.model = None
        self.scaler = StandardScaler()
        self.feature_names = None
        self.is_fitted = False

        logger.info(f"PerformancePredictor initialized with {model_type}")

    def fit(self, X: pd.DataFrame, y: pd.Series, **kwargs):
        """Fit the model"""
        raise NotImplementedError

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """Make predictions"""
        raise NotImplementedError

    def evaluate(self, X_test: pd.DataFrame, y_test: pd.Series) -> Dict:
        """Evaluate model performance"""
        y_pred = self.predict(X_test)

        mse = mean_squared_error(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        metrics = {
            'mse': float(mse),
            'mae': float(mae),
            'rmse': float(np.sqrt(mse)),
            'r2': float(r2)
        }

        logger.info(f"Model Evaluation - MAE: {mae:.4f}, R2: {r2:.4f}")
        return metrics

    def save(self, filepath: str):
        """Save model to disk"""
        Path(filepath).parent.mkdir(parents=True, exist_ok=True)
        joblib.dump(self.model, filepath)
        logger.info(f"Model saved to {filepath}")

    @staticmethod
    def load(filepath: str):
        """Load model from disk"""
        model = joblib.load(filepath)
        logger.info(f"Model loaded from {filepath}")
        return model


class RandomForestPredictor(PerformancePredictor):
    """Random Forest based performance predictor"""

    def __init__(self, n_estimators: int = 100, max_depth: int = 15, **kwargs):
        super().__init__(model_type="random_forest")
        # FIXED: Don't pass n_jobs=-1 here, let it come from kwargs
        self.model = RandomForestRegressor(
            n_estimators=n_estimators,
            max_depth=max_depth,
            random_state=42,
            **kwargs
        )

    def fit(self, X: pd.DataFrame, y: pd.Series, test_size: float = 0.2):
        """
        Fit Random Forest model

        Args:
            X: Feature matrix
            y: Target values
            test_size: Fraction for test split
        """
        self.feature_names = X.columns.tolist()

        # Scale features
        X_scaled = self.scaler.fit_transform(X)

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=test_size, random_state=42
        )

        # Train model
        logger.info("Training Random Forest model...")
        self.model.fit(X_train, y_train)

        # Evaluate
        train_score = self.model.score(X_train, y_train)
        test_score = self.model.score(X_test, y_test)

        logger.info(f"Train R2: {train_score:.4f}, Test R2: {test_score:.4f}")

        self.is_fitted = True

        # Cross-validation
        cv_scores = cross_val_score(self.model, X_scaled, y, cv=5)
        logger.info(f"CV R2: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")

        return {'train_r2': train_score, 'test_r2': test_score}

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """Make predictions"""
        if not self.is_fitted:
            raise ValueError("Model not fitted yet")

        X_scaled = self.scaler.transform(X)
        return self.model.predict(X_scaled)

    def feature_importance(self) -> Dict:
        """Get feature importances"""
        if not self.is_fitted:
            raise ValueError("Model not fitted yet")

        importance_dict = {}
        for name, importance in zip(self.feature_names, self.model.feature_importances_):
            importance_dict[name] = float(importance)

        # Sort by importance
        return dict(sorted(importance_dict.items(), key=lambda x: x[1], reverse=True))


class XGBoostPredictor(PerformancePredictor):
    """XGBoost based performance predictor"""

    def __init__(self, n_estimators: int = 100, max_depth: int = 7,
                 learning_rate: float = 0.1, **kwargs):
        super().__init__(model_type="xgboost")
        # FIXED: Remove any conflicting kwargs before passing
        kwargs.pop('n_jobs', None)  # Remove n_jobs if present
        self.model = xgb.XGBRegressor(
            n_estimators=n_estimators,
            max_depth=max_depth,
            learning_rate=learning_rate,
            random_state=42,
            tree_method='hist',
            **kwargs
        )
    
    def fit(self, X: pd.DataFrame, y: pd.Series, test_size: float = 0.2):
        """
        Fit XGBoost model
        
        Args:
            X: Feature matrix
            y: Target values
            test_size: Fraction for test split
        """
        self.feature_names = X.columns.tolist()
        
        # Scale features
        X_scaled = self.scaler.fit_transform(X)
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=test_size, random_state=42
        )
        
        # Train model
        logger.info("Training XGBoost model...")
        self.model.fit(
            X_train, y_train,
            eval_set=[(X_test, y_test)],
            verbose=False
        )
        
        # Evaluate
        train_score = self.model.score(X_train, y_train)
        test_score = self.model.score(X_test, y_test)
        
        logger.info(f"Train R2: {train_score:.4f}, Test R2: {test_score:.4f}")
        
        self.is_fitted = True
        
        return {'train_r2': train_score, 'test_r2': test_score}
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """Make predictions"""
        if not self.is_fitted:
            raise ValueError("Model not fitted yet")
        
        X_scaled = self.scaler.transform(X)
        return self.model.predict(X_scaled)

    def feature_importance(self) -> Dict:
        """Get feature importances"""
        if not self.is_fitted:
            raise ValueError("Model not fitted yet")
        
        importance_dict = {}
        for name, importance in zip(self.feature_names, self.model.feature_importances_):
            importance_dict[name] = float(importance)
        
        # Sort by importance
        return dict(sorted(importance_dict.items(), key=lambda x: x[1], reverse=True))


======= FILE: src/reporting.py =======

import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import matplotlib.image as mpimg
from pathlib import Path
import datetime
import pandas as pd
import numpy as np
from loguru import logger

def create_text_page(pdf, title, content, fontsize=10):
    """Creates a text-only page in the PDF."""
    fig = plt.figure(figsize=(8.5, 11))
    plt.axis('off')
    
    # Title
    plt.text(0.5, 0.95, title, ha='center', va='top', fontsize=16, weight='bold')
    
    # Content
    y_pos = 0.85
    for line in content.split('\n'):
        # Simple word wrap
        words = line.split()
        current_line = ""
        for word in words:
            if len(current_line + " " + word) > 90:
                plt.text(0.1, y_pos, current_line, ha='left', va='top', fontsize=fontsize, fontfamily='monospace')
                y_pos -= 0.015
                current_line = word
            else:
                current_line += " " + word if current_line else word
        
        if current_line:
            plt.text(0.1, y_pos, current_line, ha='left', va='top', fontsize=fontsize, fontfamily='monospace')
            y_pos -= 0.015
        
        y_pos -= 0.005 # Paragraph spacing
        
        if y_pos < 0.05:
            pdf.savefig(fig)
            plt.close()
            fig = plt.figure(figsize=(8.5, 11))
            plt.axis('off')
            y_pos = 0.95

    pdf.savefig(fig)
    plt.close()

def add_image_page(pdf, image_path, title):
    """Adds an image to a page in the PDF."""
    path = Path(image_path)
    if not path.exists():
        logger.warning(f"Warning: {image_path} not found")
        return

    fig = plt.figure(figsize=(8.5, 11))
    plt.axis('off')
    plt.text(0.5, 0.95, title, ha='center', va='top', fontsize=14, weight='bold')
    
    try:
        img = mpimg.imread(str(path))
        # Maintain aspect ratio, fit to page
        plt.imshow(img)
        plt.tight_layout(rect=[0.05, 0.05, 0.95, 0.90])
        pdf.savefig(fig)
    except Exception as e:
        logger.error(f"Failed to add image {image_path}: {e}")
    finally:
        plt.close()

def generate_enhanced_report(output_path, baselines, plots_dir):
    """
    Generates a PDF report with detailed metrics and plots.
    
    Args:
        output_path: Path to save the PDF.
        baselines: Dictionary of results (e.g., {'strategy': {'makespan': ..., 'latencies': ...}}).
        plots_dir: Directory containing generated plots.
    """
    output_path = Path(output_path)
    plots_dir = Path(plots_dir)
    
    logger.info(f"Generating report at {output_path}")
    
    with PdfPages(output_path) as pdf:
        # Page 1: Executive Summary
        title = "Heavy Workload Simulation Report"
        
        # Calculate summary metrics
        strategies = list(baselines.keys())
        best_strategy = min(baselines, key=lambda k: baselines[k].get('makespan', float('inf')))
        worst_strategy = max(baselines, key=lambda k: baselines[k].get('makespan', float('-inf')))
        
        summary_text = f"""
Date: {datetime.datetime.now().strftime("%Y-%m-%d %H:%M")}

1. EXECUTIVE SUMMARY
--------------------
This report analyzes the performance of scheduling strategies under a HEAVY workload regime.
Workload Characteristics:
- Total Tasks: ~10,000 per strategy
- Compute Bias: High (0.6 - 1.0 intensity)
- Size Range: 100 - 5000 units
- Memory Range: 100 - 10000 MB

Key Findings:
- Best Performing Strategy: {best_strategy.upper()}
- Worst Performing Strategy: {worst_strategy.upper()}
- Speedup: {best_strategy.upper()} was {baselines[worst_strategy]['makespan'] / baselines[best_strategy]['makespan']:.2f}x faster than {worst_strategy.upper()}.

2. DETAILED METRICS
-------------------
"""
        
        # Add detailed table
        headers = ["Strategy", "Makespan(s)", "Avg Latency(s)", "P95 Latency(s)", "P99 Latency(s)", "Throughput(T/s)"]
        summary_text += f"{headers[0]:<20} | {headers[1]:<12} | {headers[2]:<14} | {headers[3]:<14} | {headers[4]:<14} | {headers[5]:<14}\n"
        summary_text += "-"*100 + "\n"
        
        for name, metrics in baselines.items():
            makespan = metrics.get('makespan', 0)
            avg_lat = metrics.get('avg_time', 0)
            p95 = metrics.get('p95_time', 0)
            p99 = metrics.get('p99_time', 0)
            throughput = metrics.get('throughput', 0)
            
            summary_text += f"{name:<20} | {makespan:<12.2f} | {avg_lat:<14.4f} | {p95:<14.4f} | {p99:<14.4f} | {throughput:<14.2f}\n"

        summary_text += """
        
3. COST ANALYSIS
----------------
(Abstract Cost units based on compute time and resource usage)
"""
        summary_text += f"{'Strategy':<20} | {'Total Cost':<12} | {'Cost Efficiency':<15}\n"
        summary_text += "-"*60 + "\n"
        
        for name, metrics in baselines.items():
            cost = metrics.get('total_cost', 0)
            efficiency = metrics.get('cost_efficiency', 0)
            summary_text += f"{name:<20} | {cost:<12.2f} | {efficiency:<15.2f}\n"

        create_text_page(pdf, title, summary_text)
        
        # Plots
        add_image_page(pdf, plots_dir / "makespan_comparison.png", "Makespan Comparison")
        add_image_page(pdf, plots_dir / "latency_cdf.png", "Latency CDF (Cumulative Distribution)")
        add_image_page(pdf, plots_dir / "latency_boxplot.png", "Latency Distribution (Box Plot)")
        add_image_page(pdf, plots_dir / "cost_comparison.png", "Total Operational Cost")
        add_image_page(pdf, plots_dir / "speedup_comparison.png", "Speedup vs Baseline")
        add_image_page(pdf, plots_dir / "compute_vs_memory.png", "Workload Characteristics")


======= FILE: src/offline_trainer.py =======

"""
Offline Training Pipeline
Processes workload data and trains ML models
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, Tuple
from loguru import logger
from .simulator import VirtualMultiGPU


from .ml_models import RandomForestPredictor, XGBoostPredictor
from .workload_generator import WorkloadGenerator


class OfflineTrainer:
    """Orchestrates offline training pipeline"""
    
    def __init__(self, model_type: str = "random_forest", **model_kwargs):
        self.model_type = model_type
        self.model_kwargs = model_kwargs
        self.model = None
        self.training_data = None
        self.features = None
        self.target = None
        
        logger.info(f"OfflineTrainer initialized with {model_type}")
    
    def prepare_data(self, workload_generator: WorkloadGenerator) -> pd.DataFrame:
        """
        Prepare training data from workload
        
        Args:
            workload_generator: WorkloadGenerator instance with tasks
            
        Returns:
            DataFrame with features for training
        """
        logger.info("Preparing training data...")
        
        tasks = workload_generator.tasks
        
        # Extract features from tasks
        features_list = []
        for task in tasks:
            features_list.append({
                'size': task.size,
                'compute_intensity': task.compute_intensity,
                'memory_required': task.memory_required,
                'duration_estimate': task.duration_estimate,
                # Derived features
                'memory_per_size': task.memory_required / (task.size + 1),
                'compute_to_memory': task.compute_intensity / (task.memory_required + 1),
            })
        
        df = pd.DataFrame(features_list)
        
        # Create target: optimal resource allocation
        # Simple heuristic: GPU good for high compute_intensity tasks
        simulator = VirtualMultiGPU(num_gpus=1)
        optimal_fractions = []
        optimal_times = []

        for i, task in enumerate(tasks):
            best_frac = 0.0
            best_time = float('inf')
            # Try gpu_fraction from 0.0 up to 1.0 (inclusive, fine steps)
            for frac in np.linspace(0, 1, 11):
                sim_result = simulator.simulate_task_execution(task, gpu_fraction=frac)
                total_time = sim_result['actual_time']
                if total_time < best_time:
                    best_time = total_time
                    best_frac = frac
            optimal_fractions.append(best_frac)
            optimal_times.append(best_time)

        df['optimal_gpu_fraction'] = optimal_fractions
        df['optimal_total_time'] = optimal_times

        self.training_data = df
        logger.info(f"Prepared {len(df)} training samples")
        
        return df
    
    def create_model(self):
        """Create model instance based on type"""
        if self.model_type == "random_forest":
            self.model = RandomForestPredictor(**self.model_kwargs)
        elif self.model_type == "xgboost":
            self.model = XGBoostPredictor(**self.model_kwargs)
        else:
            raise ValueError(f"Unknown model type: {self.model_type}")
        
        logger.info(f"Created {self.model_type} model")
        return self.model
    
    def train(self, 
              X: pd.DataFrame, 
              y: pd.Series,
              test_size: float = 0.2) -> Dict:
        """
        Train the model
        
        Args:
            X: Feature matrix
            y: Target values
            test_size: Test set fraction
            
        Returns:
            Training results dictionary
        """
        if self.model is None:
            self.create_model()
        
        logger.info(f"Training model on {len(X)} samples...")
        
        results = self.model.fit(X, y, test_size=test_size)
        
        # Get feature importances
        importances = self.model.feature_importance()
        logger.info(f"Top 3 important features: {dict(list(importances.items())[:3])}")
        
        return results
    
    def evaluate(self, X_test: pd.DataFrame, y_test: pd.Series) -> Dict:
        """Evaluate model on test set"""
        if self.model is None:
            raise ValueError("Model not trained yet")
        
        metrics = self.model.evaluate(X_test, y_test)
        return metrics
    
    def save_model(self, filepath: str):
        """Save trained model"""
        if self.model is None:
            raise ValueError("No model to save")
        
        self.model.save(filepath)
        logger.info(f"Model saved to {filepath}")
    
    def run_full_pipeline(self, 
                         workload_generator: WorkloadGenerator,
                         model_output_path: str = "models/scheduler_model.pkl") -> Dict:
        """
        Run complete offline training pipeline
        
        Args:
            workload_generator: WorkloadGenerator with tasks
            model_output_path: Where to save the model
            
        Returns:
            Dictionary with pipeline results
        """
        logger.info("="*60)
        logger.info("Starting Offline Training Pipeline")
        logger.info("="*60)
        
        # Step 1: Prepare data
        df = self.prepare_data(workload_generator)
        
        # Step 2: Extract features and target
        feature_cols = ['size', 'compute_intensity', 'memory_required', 
                       'memory_per_size', 'compute_to_memory']
        X = df[feature_cols]
        y = df['optimal_gpu_fraction']
        
        # Step 3: Create and train model
        self.create_model()
        train_results = self.train(X, y, test_size=0.2)
        
        # Step 4: Save model
        self.save_model(model_output_path)
        
        # Step 5: Generate feature importances
        importances = self.model.feature_importance()
        
        pipeline_results = {
            'training_results': train_results,
            'feature_importances': importances,
            'model_path': model_output_path,
            'data_stats': {
                'num_samples': len(X),
                'num_features': len(feature_cols),
            }
        }
        
        logger.info("="*60)
        logger.info("Offline Training Pipeline Complete")
        logger.info("="*60)
        
        return pipeline_results

    def empirical_best_split(task, simulator, split_steps=11):
        """
        Find the gpu_fraction in [0, 1] that minimizes simulated runtime for the given task.
        Args:
            task: Task object (from WorkloadGenerator)
            simulator: VirtualMultiGPU or equivalent (should expose simulate_task_execution)
            split_steps: Granularity (default: test 0.0, 0.1, ..., 1.0)
        Returns:
            (best_fraction, best_time)
        """
        best_fraction = 0.0
        best_time = float('inf')
        # Try gpu_fraction from 0.0 up to 1.0 (inclusive)
        for frac in np.linspace(0, 1, split_steps):
            sim_result = simulator.simulate_task_execution(task, gpu_fraction=frac)
            total_time = sim_result['actual_time']
            if total_time < best_time:
                best_time = total_time
                best_fraction = frac
        return best_fraction, best_time

